{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# co-teaching code: https://github.com/bhanML/Co-teaching\n",
    "# paper: https://arxiv.org/abs/1804.06872 (2018)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Iq4YfuLbcKgh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845764709,
     "user_tz": -120,
     "elapsed": 188,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "id": "Iq4YfuLbcKgh"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qj65Ip_Ae4_A",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845781377,
     "user_tz": -120,
     "elapsed": 15347,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    },
    "outputId": "8d88ddbf-a2e0-44a4-b460-cc51d486817b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "qj65Ip_Ae4_A"
  },
  {
   "cell_type": "code",
   "source": [
    "cd /content/drive/MyDrive"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpRwnAnWcLaD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845809885,
     "user_tz": -120,
     "elapsed": 183,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    },
    "outputId": "4372c49d-398b-405d-d212-b483ea20bbce"
   },
   "id": "LpRwnAnWcLaD",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26Lxf-iDcL17",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845811556,
     "user_tz": -120,
     "elapsed": 195,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    },
    "outputId": "d2b8cdb5-008f-41ba-b7db-4315117d0c16"
   },
   "id": "26Lxf-iDcL17",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "coteaching_BiLSTM.ipynb  \u001B[0m\u001B[01;34mdata\u001B[0m/  \u001B[01;34mparams\u001B[0m/  \u001B[01;34m__pycache__\u001B[0m/  week10ma.ipynb\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {
    "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845815907,
     "user_tz": -120,
     "elapsed": 2564,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {
    "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845819021,
     "user_tz": -120,
     "elapsed": 194,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {
    "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845821843,
     "user_tz": -120,
     "elapsed": 199,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r') as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:] # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "# number of trainable parameters in model\n",
    "def get_total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {
    "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845824821,
     "user_tz": -120,
     "elapsed": 187,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "class CleavageBatch:\n",
    "    def __init__(self, batch: List[Tuple[str, str]]):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def collate_wrapper(batch):\n",
    "    return CleavageBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {
    "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845827530,
     "user_tz": -120,
     "elapsed": 180,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.dropout(self.embedding(seq))\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled = torch.mean(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, 2*hidden_size)\n",
    "        out = self.dropout(F.relu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size)\n",
    "        out = self.fc2(out).squeeze()\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "def loss_coteaching(y_1, y_2, t, forget_rate):\n",
    "    criterion = nn.BCEWithLogitsLoss(reduce=False)\n",
    "    \n",
    "    loss_1 = criterion(y_1, t)\n",
    "    ind_1_sorted = np.argsort(loss_1.data.cpu())\n",
    "    loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "    loss_2 = criterion(y_2, t)\n",
    "    ind_2_sorted = np.argsort(loss_2.data.cpu())\n",
    "    loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "    remember_rate = 1 - forget_rate\n",
    "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "    ind_1_update = ind_1_sorted[:num_remember]\n",
    "    ind_2_update = ind_2_sorted[:num_remember]\n",
    "    \n",
    "    # exchange\n",
    "\n",
    "    loss_1_update = criterion(y_1[ind_2_update], t[ind_2_update])\n",
    "    loss_2_update = criterion(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "    #if torch.isnan(loss_1_update):\n",
    "        #print(loss_1_update, y_1[ind_2_update], t[ind_2_update], len(loss_1_sorted), remember_rate)\n",
    "\n",
    "    return torch.sum(loss_1_update)/num_remember, torch.sum(loss_2_update)/num_remember"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kaUEQh_bcKgk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845997137,
     "user_tz": -120,
     "elapsed": 194,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "id": "kaUEQh_bcKgk"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {
    "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845836033,
     "user_tz": -120,
     "elapsed": 201,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "def process(model1, model2, loader, forget_rate, optim=None):\n",
    "    epoch_loss1, num_correct1, total = 0, 0, 0\n",
    "    epoch_loss2, num_correct2 = 0, 0\n",
    "\n",
    "\n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores1 = model1(seq)\n",
    "        scores2 = model2(seq)\n",
    "        loss1, loss2= loss_coteaching(scores1, scores2, lbl, forget_rate)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss1.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            loss2.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_loss1 += loss1.item()\n",
    "        epoch_loss2 += loss2.item()\n",
    "        num_correct1 += ((scores1 > 0) == lbl).sum()\n",
    "        num_correct2 += ((scores2 > 0) == lbl).sum()\n",
    "        total += len(seq)\n",
    "    return epoch_loss1 / total, epoch_loss2 / total, num_correct1 / total, num_correct2 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {
    "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845842854,
     "user_tz": -120,
     "elapsed": 4521,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('data/n_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('data/n_val.csv')\n",
    "\n",
    "# create vocab from train seqs\n",
    "vocab = build_vocab_from_iterator(train_seqs, specials=['<UNK>'])\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "encode_text = lambda x: vocab(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661845852162,
     "user_tz": -120,
     "elapsed": 7418,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    },
    "outputId": "ec922cbc-8403-40b8-ee37-88a1bf974f16"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total trainable model1 parameters: 2,648,373\n",
      "Total trainable model2 parameters: 2,648,373\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "RNN_SIZE = 512\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "NUM_GRADUAL = 10 # how many epochs for linear drop rate, can be 5, 10, 15. This parameter is equal to Tk for R(T) in Co-teaching paper.\n",
    "NOISY_RATE = 0.2\n",
    "FORGET_RATE = 0.3\n",
    "EXPONENT = 1\n",
    "\n",
    "# define drop rate schedule\n",
    "rate_schedule = np.ones(NUM_EPOCHS)*FORGET_RATE\n",
    "rate_schedule[:NUM_GRADUAL] = np.linspace(0, FORGET_RATE**EXPONENT, NUM_GRADUAL)\n",
    "\n",
    "\n",
    "model1 = BiLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model2 = BiLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer2 = optim.Adam(model1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "print(f\"Total trainable model1 parameters: {get_total_model_params(model1):,}\")\n",
    "print(f\"Total trainable model2 parameters: {get_total_model_params(model1):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1661847291915,
     "user_tz": -120,
     "elapsed": 1290681,
     "user": {
      "displayName": "B M",
      "userId": "00963395871157218456"
     }
    },
    "outputId": "03727d32-7760-4df1-ddab-500245876cc4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Training.\n",
      "Train: 100%|██████████| 2236/2236 [02:02<00:00, 18.21batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.79batches/s]\n",
      "Model1 Training:   [Epoch  1, Loss:   0.0012, Acc: 0.6289]\n",
      "Model1 Evaluation: [Epoch  1, Loss:   0.0011, Acc: 0.6834]\n",
      "Model2 Training:   [Epoch  1, Loss:   0.0014, Acc: 0.5650]\n",
      "Model2 Evaluation: [Epoch  1, Loss:   0.0014, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:04<00:00, 17.95batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.12batches/s]\n",
      "Model1 Training:   [Epoch  2, Loss:   0.0011, Acc: 0.6811]\n",
      "Model1 Evaluation: [Epoch  2, Loss:   0.0011, Acc: 0.6884]\n",
      "Model2 Training:   [Epoch  2, Loss:   0.0013, Acc: 0.5654]\n",
      "Model2 Evaluation: [Epoch  2, Loss:   0.0014, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:04<00:00, 17.97batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.48batches/s]\n",
      "Model1 Training:   [Epoch  3, Loss:   0.0011, Acc: 0.6828]\n",
      "Model1 Evaluation: [Epoch  3, Loss:   0.0011, Acc: 0.6897]\n",
      "Model2 Training:   [Epoch  3, Loss:   0.0013, Acc: 0.5652]\n",
      "Model2 Evaluation: [Epoch  3, Loss:   0.0014, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:04<00:00, 18.02batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.39batches/s]\n",
      "Model1 Training:   [Epoch  4, Loss:   0.0011, Acc: 0.6801]\n",
      "Model1 Evaluation: [Epoch  4, Loss:   0.0010, Acc: 0.6855]\n",
      "Model2 Training:   [Epoch  4, Loss:   0.0013, Acc: 0.5648]\n",
      "Model2 Evaluation: [Epoch  4, Loss:   0.0014, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:03<00:00, 18.05batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.11batches/s]\n",
      "Model1 Training:   [Epoch  5, Loss:   0.0010, Acc: 0.6740]\n",
      "Model1 Evaluation: [Epoch  5, Loss:   0.0010, Acc: 0.6762]\n",
      "Model2 Training:   [Epoch  5, Loss:   0.0013, Acc: 0.5650]\n",
      "Model2 Evaluation: [Epoch  5, Loss:   0.0014, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:03<00:00, 18.09batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.31batches/s]\n",
      "Model1 Training:   [Epoch  6, Loss:   0.0010, Acc: 0.6662]\n",
      "Model1 Evaluation: [Epoch  6, Loss:   0.0010, Acc: 0.6632]\n",
      "Model2 Training:   [Epoch  6, Loss:   0.0013, Acc: 0.5647]\n",
      "Model2 Evaluation: [Epoch  6, Loss:   0.0013, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:03<00:00, 18.13batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.23batches/s]\n",
      "Model1 Training:   [Epoch  7, Loss:   0.0010, Acc: 0.6564]\n",
      "Model1 Evaluation: [Epoch  7, Loss:   0.0009, Acc: 0.6633]\n",
      "Model2 Training:   [Epoch  7, Loss:   0.0013, Acc: 0.5647]\n",
      "Model2 Evaluation: [Epoch  7, Loss:   0.0013, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:02<00:00, 18.24batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.60batches/s]\n",
      "Model1 Training:   [Epoch  8, Loss:   0.0009, Acc: 0.6449]\n",
      "Model1 Evaluation: [Epoch  8, Loss:   0.0009, Acc: 0.6435]\n",
      "Model2 Training:   [Epoch  8, Loss:   0.0013, Acc: 0.5648]\n",
      "Model2 Evaluation: [Epoch  8, Loss:   0.0013, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:02<00:00, 18.23batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.51batches/s]\n",
      "Model1 Training:   [Epoch  9, Loss:   0.0009, Acc: 0.6329]\n",
      "Model1 Evaluation: [Epoch  9, Loss:   0.0008, Acc: 0.6477]\n",
      "Model2 Training:   [Epoch  9, Loss:   0.0013, Acc: 0.5653]\n",
      "Model2 Evaluation: [Epoch  9, Loss:   0.0013, Acc: 0.5795]\n",
      "Train: 100%|██████████| 2236/2236 [02:01<00:00, 18.34batches/s]\n",
      "Eval: 100%|██████████| 280/280 [00:05<00:00, 50.03batches/s]\n",
      "Model1 Training:   [Epoch 10, Loss:   0.0008, Acc: 0.6213]\n",
      "Model1 Evaluation: [Epoch 10, Loss:   0.0007, Acc: 0.6270]\n",
      "Model2 Training:   [Epoch 10, Loss:   0.0013, Acc: 0.5649]\n",
      "Model2 Evaluation: [Epoch 10, Loss:   0.0013, Acc: 0.5795]\n",
      "Finished Training.\n",
      "Training took 21.510587426026664 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_acc1, highest_val_acc2 = 0,0\n",
    "train_losses1, train_accuracies1= [], []\n",
    "train_losses2, train_accuracies2= [], []\n",
    "val_losses1, val_accuracies1 = [], []\n",
    "val_losses2, val_accuracies2 = [], []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss1, train_loss2, train_acc1, train_acc2 = process(model1, model2, train_loader, rate_schedule[epoch-1], optimizer)\n",
    "    \n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss1, val_loss2, val_acc1, val_acc2 = process(model1, model2, dev_loader, rate_schedule[epoch-1])\n",
    "        \n",
    "    # save current acc, loss model1\n",
    "    train_losses1.append((epoch, train_loss1))\n",
    "    train_accuracies1.append((epoch, train_acc1))\n",
    "    val_losses1.append((epoch, val_loss1))\n",
    "    val_accuracies1.append((epoch, val_acc1))\n",
    "    \n",
    "    if val_acc1 > highest_val_acc1:\n",
    "        highest_val_acc1 = val_acc1\n",
    "        path = f\"params/model1_acc{val_acc1:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model1.state_dict(), path)\n",
    "\n",
    "\n",
    "    print(f\"Model1 Training:   [Epoch {epoch:2d}, Loss: {train_loss1:8.4f}, Acc: {train_acc1:.4f}]\")\n",
    "    print(f\"Model1 Evaluation: [Epoch {epoch:2d}, Loss: {val_loss1:8.4f}, Acc: {val_acc1:.4f}]\")\n",
    "\n",
    "\n",
    "    # save current acc, loss model2\n",
    "    train_losses2.append((epoch, train_loss2))\n",
    "    train_accuracies2.append((epoch, train_acc2))\n",
    "    val_losses2.append((epoch, val_loss2))\n",
    "    val_accuracies2.append((epoch, val_acc2))\n",
    "\n",
    "    if val_acc2 > highest_val_acc2:\n",
    "        highest_val_acc2 = val_acc2\n",
    "        path = f\"params/model2_acc{val_acc2:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model2.state_dict(), path)\n",
    "\n",
    "    print(f\"Model2 Training:   [Epoch {epoch:2d}, Loss: {train_loss2:8.4f}, Acc: {train_acc2:.4f}]\")\n",
    "    print(f\"Model2 Evaluation: [Epoch {epoch:2d}, Loss: {val_loss2:8.4f}, Acc: {val_acc2:.4f}]\")\n",
    "    \n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {
    "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22"
   },
   "outputs": [],
   "source": [
    "# save training stats\n",
    "lsts = [train_losses, train_accuracies, val_losses, val_accuracies, train_time]\n",
    "names = [\n",
    "    \"train_losses\",\n",
    "    \"train_accuracies\",\n",
    "    \"val_losses\",\n",
    "    \"val_accuracies\",\n",
    "    \"train_time\",\n",
    "]\n",
    "to_save = dict()\n",
    "for name, lst in zip(names, lsts):\n",
    "    to_save[name] = lst\n",
    "\n",
    "with open(f\"../params/n_term/quadBiLSTM/metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(to_save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Finished Saving Details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7",
   "metadata": {
    "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "colab": {
   "name": "coteaching_BiLSTM.ipynb",
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}