{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9145fcc-a3e1-46c0-a876-09fdfe1d4c7e",
   "metadata": {},
   "source": [
    "# Note: The n-term dataset used here is not the final n-term dataset. Results are not directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b376cff-a0bb-42ab-8e39-4a964c417dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../denoise/')\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from divide_mix.data_handling import CleavageLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228ec4a9-8484-45d5-8af5-b8459f876d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size1, rnn_size2, hidden_size, dropout1, dropout2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        self.dropout1=nn.Dropout(dropout1)\n",
    "        self.dropout2=nn.Dropout(dropout2)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2 * rnn_size1,\n",
    "            hidden_size=rnn_size2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = spectral_norm(nn.Linear(rnn_size2 * 2, hidden_size))\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    \n",
    "    def no_embed_fw(self, embedded):\n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm1(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*rnn_size1)\n",
    "        out, _ = self.lstm2(out)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, 2*hidden_size)\n",
    "        out = self.dropout1(gelu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size, 2)\n",
    "        return self.fc2(out)\n",
    "    \n",
    "    def forward(self, seq, seq2=None, lam=None, interpolate=False):\n",
    "        if interpolate:\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            embedded1 = self.embedding(seq)\n",
    "            embedded2 = self.embedding(seq2)\n",
    "            embedded_mixed = lam * embedded1 + (1 - lam) * embedded2\n",
    "            return self.no_embed_fw(self.dropout2(embedded_mixed))\n",
    "        else:\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            embedded = self.dropout1(self.embedding(seq))\n",
    "            return self.no_embed_fw(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7db4b6d-7860-4995-b239-dab77c596fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model1, model2, optimizer, labeled_loader, unlabeled_loader, named_model):\n",
    "    epoch_loss, total = 0, 0\n",
    "    \n",
    "    # fix one model while the other one trains\n",
    "    model1.train()\n",
    "    model2.eval()\n",
    "    \n",
    "    unlabeled_train_iter = iter(unlabeled_loader)\n",
    "    num_iter = (len(labeled_loader.dataset) // BATCH_SIZE) + 1\n",
    "    \n",
    "    for batch_idx, batch in enumerate(\n",
    "        tqdm(\n",
    "        labeled_loader,\n",
    "        desc=f\"train {named_model}: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "        )\n",
    "    ):\n",
    "        in_x, in_x2, labels_x, w_x = batch.seq1, batch.seq2, batch.lbl, batch.prob\n",
    "        batch_size = in_x.shape[0]\n",
    "        \n",
    "        # re-use unlabeled_loader as long as there are batches in labeled_loader\n",
    "        try:\n",
    "            unlabeled_batch = unlabeled_train_iter.next()\n",
    "            in_u, in_u2 = unlabeled_batch.seq1, unlabeled_batch.seq2\n",
    "        except:\n",
    "            unlabeled_train_iter = iter(unlabeled_loader)\n",
    "            unlabeled_batch = unlabeled_train_iter.next()\n",
    "            in_u, in_u2 = unlabeled_batch.seq1, unlabeled_batch.seq2\n",
    "        \n",
    "        \n",
    "        w_x = w_x.view(-1, 1).to(torch.float)\n",
    "        in_x, in_x2, labels_x, w_x = (\n",
    "            in_x.to(device),\n",
    "            in_x2.to(device),\n",
    "            labels_x.to(device),\n",
    "            w_x.to(device)\n",
    "        )\n",
    "        \n",
    "        in_u, in_u2 = in_u.to(device), in_u2.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # for labeled samples: co-refinement + temperature sharpening\n",
    "            out_x = model1(in_x)\n",
    "            out_x2 = model1(in_x2)\n",
    "\n",
    "            px = (torch.softmax(out_x, dim=1) + torch.softmax(out_x2, dim=1)) / 2\n",
    "            px = w_x * labels_x + (1 - w_x) * px\n",
    "                \n",
    "            ptx = px ** (1 / TEMPERATURE)\n",
    "            targets_x = (ptx / ptx.sum(dim=1, keepdim=True)).detach()\n",
    "            \n",
    "            # for unlabeled samples: co-guessing + temperature sharpening\n",
    "            out_u11 = model1(in_u)\n",
    "            out_u12 = model1(in_u2)\n",
    "            out_u21 = model2(in_u)\n",
    "            out_u22 = model2(in_u2)\n",
    "            \n",
    "            pu = (\n",
    "                torch.softmax(out_u11, dim=1)\n",
    "                + torch.softmax(out_u12, dim=1)\n",
    "                + torch.softmax(out_u21, dim=1)\n",
    "                + torch.softmax(out_u22, dim=1)\n",
    "            ) / 4\n",
    "            \n",
    "            ptu = pu ** (1 / TEMPERATURE)\n",
    "            targets_u = (ptu / ptu.sum(dim=1, keepdim=True)).detach()\n",
    "            \n",
    "        ### MixMatch\n",
    "        # lambda interpolation factor for mixed input and targets\n",
    "        lam = beta_dist.sample()\n",
    "        lam = max(lam, 1-lam)\n",
    "        \n",
    "        all_ins = torch.cat([in_x, in_x2, in_u, in_u2], dim=0)\n",
    "        all_targets = torch.cat([targets_x, targets_x, targets_u, targets_u], dim=0)\n",
    "        \n",
    "        # shuffle all inputs to generate new pairs\n",
    "        idx = torch.randperm(all_ins.shape[0])\n",
    "\n",
    "        in_a, in_b = all_ins, all_ins[idx]\n",
    "        target_a, target_b = all_targets, all_targets[idx]\n",
    "        \n",
    "        mixed_target = lam * target_a + (1 - lam) * target_b\n",
    "        # inputs are mixed up in forward pass\n",
    "        logits = model1(in_a, in_b, lam, interpolate=True)\n",
    "        logits_x, logits_u = logits[:batch_size*2], logits[batch_size*2:]\n",
    "        \n",
    "        Lx, Lu, lambda_u = criterion(\n",
    "            logits_x,\n",
    "            mixed_target[:batch_size*2],\n",
    "            logits_u,\n",
    "            mixed_target[batch_size*2:],\n",
    "            epoch + batch_idx/num_iter,\n",
    "            NUM_WARM_UP_EPOCHS\n",
    "        )\n",
    "        \n",
    "        # regularization\n",
    "        prior = (torch.ones(NUM_CLASSES) / NUM_CLASSES).to(device)\n",
    "        pred_mean = torch.softmax(logits, dim=1).mean(0)\n",
    "        penalty = torch.sum(prior * torch.log(prior / pred_mean))\n",
    "        \n",
    "        loss = Lx + lambda_u * Lu + penalty\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total += idx.shape[0]\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b4b4cc-5674-47df-95ca-2aad842ba91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(epoch, model, optimizer, dataloader):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    preds, lbls = [], []\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(\n",
    "        tqdm(dataloader, desc=\"warmup: \", file=sys.stdout, unit=\"batches\")\n",
    "    ):\n",
    "        in_x, lbl_x = batch.seq, batch.lbl\n",
    "        in_x, lbl_x = in_x.to(device), lbl_x.to(device)\n",
    "\n",
    "        out = model(in_x)\n",
    "        loss = CEloss(out, lbl_x)\n",
    "\n",
    "        # penalty for confident predictions for asymmetric noise\n",
    "        loss += conf_penalty(out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        non_ohe_labels = lbl_x.argmax(dim=1)\n",
    "        num_correct += (\n",
    "            (out.argmax(dim=1) == non_ohe_labels).sum().item()\n",
    "        )\n",
    "        total += in_x.shape[0]\n",
    "        preds.extend(out[:, 1].detach().tolist())\n",
    "        lbls.extend(non_ohe_labels.detach().tolist())\n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5936bf1d-448b-435d-8eb7-689d11f78b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model1, model2, dataloader):\n",
    "    num_correct, total = 0, 0\n",
    "    preds, lbls, losses = [], [], []\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"evaluate: \", file=sys.stdout, unit=\"batches\"):\n",
    "            in_x, lbl_x = batch.seq, batch.lbl\n",
    "            in_x, lbl_x = in_x.to(device), lbl_x.to(device)\n",
    "            out1 = model1(in_x)\n",
    "            out2 = model2(in_x)\n",
    "            out = out1 + out2\n",
    "\n",
    "            loss = CE(out, lbl_x)\n",
    "            losses.extend(loss.detach().tolist())\n",
    "            \n",
    "            non_ohe_labels = lbl_x.argmax(dim=1)\n",
    "            num_correct += (\n",
    "                (out.argmax(dim=1) == non_ohe_labels).sum().item()\n",
    "            )\n",
    "            total += in_x.shape[0]\n",
    "            preds.extend(out[:, 1].detach().tolist())\n",
    "            lbls.extend(non_ohe_labels.detach().tolist())\n",
    "            \n",
    "    losses = np.array(losses)\n",
    "    return num_correct / total, roc_auc_score(lbls, preds), losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef77d2d2-cc29-4078-871a-fccdd5177467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gmm(model, dataloader):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(\n",
    "            dataloader, desc=\"GMM processing: \", file=sys.stdout, unit=\"batches\"\n",
    "        ):\n",
    "            in_x, lbl_x = batch.seq, batch.lbl\n",
    "            in_x, lbl_x = in_x.to(device), lbl_x.to(device)\n",
    "\n",
    "            out = model(in_x)\n",
    "            # nn.CrossEntropyLoss with reduction=None, returns per sample loss \n",
    "            loss = CE(out, lbl_x)  \n",
    "            losses.extend(loss.detach().tolist())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    # normalize losses between 0 and 1\n",
    "    norm_losses = ((losses - losses.min()) / losses.ptp())[:, np.newaxis]\n",
    "\n",
    "    # fit two component GMM to loss\n",
    "    gmm = GaussianMixture(n_components=2, max_iter=10, tol=1e-2, reg_covar=5e-4)\n",
    "    gmm.fit(norm_losses)\n",
    "    prob = gmm.predict_proba(norm_losses)\n",
    "    # get value of smaller mean dist\n",
    "    prob = prob[:, gmm.means_.argmin()]\n",
    "    # out shape: (batch_size)\n",
    "    return prob, losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14278c20-d6a2-408b-b375-30caee31bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_rampup(current_epoch, warm_up, rampup_len=15):\n",
    "    current = np.clip((current_epoch - warm_up) / rampup_len, 0.0, 1.0)\n",
    "    return LAMBDA_U * current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd552df5-4de1-438c-999d-2873f499efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiLoss:\n",
    "    def __call__(self, out_x, lbl_x, out_u, lbl_u, epoch, warm_up):\n",
    "        probs_u = torch.softmax(out_u, dim=1)\n",
    "\n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(out_x, dim=1) * lbl_x, dim=1))\n",
    "        Lu = torch.mean((probs_u - lbl_u) ** 2)\n",
    "\n",
    "        return Lx, Lu, linear_rampup(epoch, warm_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9831f33-9bfa-4890-a4c8-7e908a1eeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegEntropy:\n",
    "    def __call__(self, out):\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "        return torch.mean(torch.sum(probs.log() * probs, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c4df09f-529b-491b-8416-05e195f4df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparse params\n",
    "BATCH_SIZE = 64\n",
    "LR = 5e-5\n",
    "ALPHA = 0.5\n",
    "LAMBDA_U = 0\n",
    "P_THRESHOLD = 0.5\n",
    "TEMPERATURE = 0.5\n",
    "NUM_EPOCHS = 50\n",
    "NUM_WARM_UP_EPOCHS = 10\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d89689-7044-4e5b-9bc7-a26535f8108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN\n",
    "vocab = torch.load('../data/vocab.pt')\n",
    "beta_dist = torch.distributions.beta.Beta(ALPHA, ALPHA)\n",
    "loader = CleavageLoader(batch_size=BATCH_SIZE, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c45886-ff5f-44f0-ac21-98172540b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=200,\n",
    "    rnn_size1=256,\n",
    "    rnn_size2=512,\n",
    "    hidden_size=128,\n",
    "    dropout1=0.5,\n",
    "    dropout2=0.\n",
    ").to(device)\n",
    "\n",
    "model2 = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=20,\n",
    "    rnn_size1=256,\n",
    "    rnn_size2=512,\n",
    "    hidden_size=128,\n",
    "    dropout1=0.5,\n",
    "    dropout2=0.\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f7522c-2fa4-460b-8536-44103e250021",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SemiLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=LR)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=LR)\n",
    "CE = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "conf_penalty = NegEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59aeee47-a9fa-4c77-9058-f619df3886a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 114.46batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:18<00:00, 118.95batches/s]\n",
      "Warm-Up Model1: [Epoch  1, Loss: -0.000036, Acc: 0.6027, AUC: 0.6161]\n",
      "Warm-Up Model2: [Epoch  1, Loss: -0.000029, Acc: 0.5936, AUC: 0.6005]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.94batches/s]\n",
      "Evaluation Set: [Epoch  1, Loss: 0.590031, Acc: 0.6800, AUC: 0.7453]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 113.72batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:18<00:00, 118.85batches/s]\n",
      "Warm-Up Model1: [Epoch  2, Loss: -0.000096, Acc: 0.6749, AUC: 0.7324]\n",
      "Warm-Up Model2: [Epoch  2, Loss: -0.000071, Acc: 0.6445, AUC: 0.6962]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.40batches/s]\n",
      "Evaluation Set: [Epoch  2, Loss: 0.574175, Acc: 0.6883, AUC: 0.7595]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 112.95batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 115.07batches/s]\n",
      "Warm-Up Model1: [Epoch  3, Loss: -0.000100, Acc: 0.6785, AUC: 0.7402]\n",
      "Warm-Up Model2: [Epoch  3, Loss: -0.000080, Acc: 0.6567, AUC: 0.7125]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.40batches/s]\n",
      "Evaluation Set: [Epoch  3, Loss: 0.572023, Acc: 0.6895, AUC: 0.7617]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:20<00:00, 110.11batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 115.42batches/s]\n",
      "Warm-Up Model1: [Epoch  4, Loss: -0.000102, Acc: 0.6800, AUC: 0.7432]\n",
      "Warm-Up Model2: [Epoch  4, Loss: -0.000084, Acc: 0.6610, AUC: 0.7185]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.25batches/s]\n",
      "Evaluation Set: [Epoch  4, Loss: 0.570347, Acc: 0.6905, AUC: 0.7637]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:20<00:00, 111.79batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 117.63batches/s]\n",
      "Warm-Up Model1: [Epoch  5, Loss: -0.000103, Acc: 0.6816, AUC: 0.7455]\n",
      "Warm-Up Model2: [Epoch  5, Loss: -0.000086, Acc: 0.6634, AUC: 0.7225]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.96batches/s]\n",
      "Evaluation Set: [Epoch  5, Loss: 0.567882, Acc: 0.6925, AUC: 0.7642]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 112.45batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 112.47batches/s]\n",
      "Warm-Up Model1: [Epoch  6, Loss: -0.000104, Acc: 0.6819, AUC: 0.7472]\n",
      "Warm-Up Model2: [Epoch  6, Loss: -0.000088, Acc: 0.6649, AUC: 0.7249]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 92.08batches/s]\n",
      "Evaluation Set: [Epoch  6, Loss: 0.566567, Acc: 0.6931, AUC: 0.7655]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:20<00:00, 109.30batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:18<00:00, 118.07batches/s]\n",
      "Warm-Up Model1: [Epoch  7, Loss: -0.000105, Acc: 0.6831, AUC: 0.7483]\n",
      "Warm-Up Model2: [Epoch  7, Loss: -0.000090, Acc: 0.6665, AUC: 0.7275]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 92.15batches/s]\n",
      "Evaluation Set: [Epoch  7, Loss: 0.567463, Acc: 0.6934, AUC: 0.7660]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 111.93batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 116.70batches/s]\n",
      "Warm-Up Model1: [Epoch  8, Loss: -0.000106, Acc: 0.6833, AUC: 0.7494]\n",
      "Warm-Up Model2: [Epoch  8, Loss: -0.000091, Acc: 0.6675, AUC: 0.7289]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.86batches/s]\n",
      "Evaluation Set: [Epoch  8, Loss: 0.566395, Acc: 0.6932, AUC: 0.7665]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:20<00:00, 110.22batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 117.50batches/s]\n",
      "Warm-Up Model1: [Epoch  9, Loss: -0.000106, Acc: 0.6835, AUC: 0.7496]\n",
      "Warm-Up Model2: [Epoch  9, Loss: -0.000092, Acc: 0.6688, AUC: 0.7306]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.43batches/s]\n",
      "Evaluation Set: [Epoch  9, Loss: 0.566626, Acc: 0.6936, AUC: 0.7668]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 112.62batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 116.63batches/s]\n",
      "Warm-Up Model1: [Epoch 10, Loss: -0.000107, Acc: 0.6840, AUC: 0.7506]\n",
      "Warm-Up Model2: [Epoch 10, Loss: -0.000093, Acc: 0.6703, AUC: 0.7325]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.78batches/s]\n",
      "Evaluation Set: [Epoch 10, Loss: 0.564577, Acc: 0.6951, AUC: 0.7677]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.90batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 211.45batches/s]\n",
      "train model1: 100%|█████████████████████████████████████████████| 8442/8442 [01:32<00:00, 90.94batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 10685/10685 [01:54<00:00, 93.14batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.75batches/s]\n",
      "Training Set: [Epoch 11, Loss1: 0.595513, Loss2: 0.604008]\n",
      "DivideMix Training: [Epoch 11, Loss1: 0.001129, Loss2: 0.001350]\n",
      "Evaluation Set: [Epoch 11, Loss: 2.118285, Acc: 0.6863, AUC: 0.7547]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.22batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 208.73batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12344/12344 [02:16<00:00, 90.23batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12126/12126 [02:11<00:00, 92.32batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.32batches/s]\n",
      "Training Set: [Epoch 12, Loss1: 1.323267, Loss2: 0.992164]\n",
      "DivideMix Training: [Epoch 12, Loss1: 0.001318, Loss2: 0.001299]\n",
      "Evaluation Set: [Epoch 12, Loss: 2.455192, Acc: 0.6880, AUC: 0.7432]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.42batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 209.86batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12214/12214 [02:14<00:00, 90.71batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12343/12343 [02:13<00:00, 92.41batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.90batches/s]\n",
      "Training Set: [Epoch 13, Loss1: 1.341999, Loss2: 1.286170]\n",
      "DivideMix Training: [Epoch 13, Loss1: 0.001282, Loss2: 0.001313]\n",
      "Evaluation Set: [Epoch 13, Loss: 2.795598, Acc: 0.6879, AUC: 0.7393]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.09batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 209.46batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12396/12396 [02:15<00:00, 91.47batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12256/12256 [02:12<00:00, 92.30batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.23batches/s]\n",
      "Training Set: [Epoch 14, Loss1: 1.549562, Loss2: 1.402009]\n",
      "DivideMix Training: [Epoch 14, Loss1: 0.001294, Loss2: 0.001281]\n",
      "Evaluation Set: [Epoch 14, Loss: 2.823582, Acc: 0.6878, AUC: 0.7400]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.86batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 212.23batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12254/12254 [02:14<00:00, 90.78batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12421/12421 [02:16<00:00, 91.29batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.66batches/s]\n",
      "Training Set: [Epoch 15, Loss1: 1.402222, Loss2: 1.601415]\n",
      "DivideMix Training: [Epoch 15, Loss1: 0.001263, Loss2: 0.001309]\n",
      "Evaluation Set: [Epoch 15, Loss: 2.869078, Acc: 0.6878, AUC: 0.7357]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.40batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 209.88batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12375/12375 [02:16<00:00, 90.42batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12294/12294 [02:12<00:00, 92.56batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.89batches/s]\n",
      "Training Set: [Epoch 16, Loss1: 1.551671, Loss2: 1.453227]\n",
      "DivideMix Training: [Epoch 16, Loss1: 0.001271, Loss2: 0.001276]\n",
      "Evaluation Set: [Epoch 16, Loss: 3.037484, Acc: 0.6887, AUC: 0.7322]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.51batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 210.38batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12311/12311 [02:14<00:00, 91.58batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12400/12400 [02:13<00:00, 92.86batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 92.15batches/s]\n",
      "Training Set: [Epoch 17, Loss1: 1.545878, Loss2: 1.619897]\n",
      "DivideMix Training: [Epoch 17, Loss1: 0.001247, Loss2: 0.001288]\n",
      "Evaluation Set: [Epoch 17, Loss: 2.993674, Acc: 0.6882, AUC: 0.7387]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 206.35batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 214.98batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12415/12415 [02:15<00:00, 91.80batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12310/12310 [02:12<00:00, 92.65batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 92.57batches/s]\n",
      "Training Set: [Epoch 18, Loss1: 1.634044, Loss2: 1.506606]\n",
      "DivideMix Training: [Epoch 18, Loss1: 0.001258, Loss2: 0.001265]\n",
      "Evaluation Set: [Epoch 18, Loss: 3.265169, Acc: 0.6890, AUC: 0.7339]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 205.48batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 213.87batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12319/12319 [02:14<00:00, 91.63batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12444/12444 [02:12<00:00, 94.03batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 92.01batches/s]\n",
      "Training Set: [Epoch 19, Loss1: 1.744040, Loss2: 1.655241]\n",
      "DivideMix Training: [Epoch 19, Loss1: 0.001233, Loss2: 0.001278]\n",
      "Evaluation Set: [Epoch 19, Loss: 3.117120, Acc: 0.6882, AUC: 0.7369]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 206.77batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 214.00batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12412/12412 [02:14<00:00, 92.12batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12321/12321 [02:12<00:00, 93.19batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.86batches/s]\n",
      "Training Set: [Epoch 20, Loss1: 1.589450, Loss2: 1.647828]\n",
      "DivideMix Training: [Epoch 20, Loss1: 0.001231, Loss2: 0.001263]\n",
      "Evaluation Set: [Epoch 20, Loss: 3.231033, Acc: 0.6886, AUC: 0.7362]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 206.57batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 215.86batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12348/12348 [02:13<00:00, 92.29batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12447/12447 [02:14<00:00, 92.40batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.85batches/s]\n",
      "Training Set: [Epoch 21, Loss1: 1.669241, Loss2: 1.681071]\n",
      "DivideMix Training: [Epoch 21, Loss1: 0.001215, Loss2: 0.001280]\n",
      "Evaluation Set: [Epoch 21, Loss: 3.209130, Acc: 0.6887, AUC: 0.7334]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 206.37batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 213.51batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12443/12443 [02:16<00:00, 91.42batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12361/12361 [02:12<00:00, 93.07batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.98batches/s]\n",
      "Training Set: [Epoch 22, Loss1: 1.644458, Loss2: 1.669791]\n",
      "DivideMix Training: [Epoch 22, Loss1: 0.001226, Loss2: 0.001268]\n",
      "Evaluation Set: [Epoch 22, Loss: 3.235330, Acc: 0.6896, AUC: 0.7351]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 205.97batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 213.75batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12378/12378 [02:13<00:00, 92.39batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12462/12462 [02:13<00:00, 93.19batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 92.28batches/s]\n",
      "Training Set: [Epoch 23, Loss1: 1.659119, Loss2: 1.666032]\n",
      "DivideMix Training: [Epoch 23, Loss1: 0.001216, Loss2: 0.001278]\n",
      "Evaluation Set: [Epoch 23, Loss: 3.294198, Acc: 0.6897, AUC: 0.7309]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 208.39batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 214.86batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12434/12434 [02:15<00:00, 91.95batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12396/12396 [02:13<00:00, 93.14batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.54batches/s]\n",
      "Training Set: [Epoch 24, Loss1: 1.753470, Loss2: 1.637617]\n",
      "DivideMix Training: [Epoch 24, Loss1: 0.001215, Loss2: 0.001249]\n",
      "Evaluation Set: [Epoch 24, Loss: 3.396758, Acc: 0.6895, AUC: 0.7323]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.47batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 209.43batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12417/12417 [02:17<00:00, 90.07batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12511/12511 [02:14<00:00, 93.17batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.48batches/s]\n",
      "Training Set: [Epoch 25, Loss1: 1.727930, Loss2: 1.774944]\n",
      "DivideMix Training: [Epoch 25, Loss1: 0.001209, Loss2: 0.001285]\n",
      "Evaluation Set: [Epoch 25, Loss: 3.338281, Acc: 0.6897, AUC: 0.7332]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.84batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 209.76batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12535/12535 [02:20<00:00, 89.36batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12396/12396 [02:17<00:00, 90.27batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 88.15batches/s]\n",
      "Training Set: [Epoch 26, Loss1: 1.768121, Loss2: 1.678467]\n",
      "DivideMix Training: [Epoch 26, Loss1: 0.001234, Loss2: 0.001248]\n",
      "Evaluation Set: [Epoch 26, Loss: 3.294237, Acc: 0.6893, AUC: 0.7370]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 197.71batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 206.14batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12443/12443 [02:19<00:00, 88.95batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12501/12501 [02:18<00:00, 90.30batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 85.37batches/s]\n",
      "Training Set: [Epoch 27, Loss1: 1.663996, Loss2: 1.723875]\n",
      "DivideMix Training: [Epoch 27, Loss1: 0.001206, Loss2: 0.001266]\n",
      "Evaluation Set: [Epoch 27, Loss: 3.244228, Acc: 0.6899, AUC: 0.7311]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 193.89batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.12batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12539/12539 [02:21<00:00, 88.56batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12418/12418 [02:19<00:00, 89.03batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 85.45batches/s]\n",
      "Training Set: [Epoch 28, Loss1: 1.642652, Loss2: 1.694987]\n",
      "DivideMix Training: [Epoch 28, Loss1: 0.001222, Loss2: 0.001244]\n",
      "Evaluation Set: [Epoch 28, Loss: 3.438891, Acc: 0.6901, AUC: 0.7388]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 192.20batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.53batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12483/12483 [02:22<00:00, 87.42batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12500/12500 [02:19<00:00, 89.33batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.22batches/s]\n",
      "Training Set: [Epoch 29, Loss1: 1.765628, Loss2: 1.765968]\n",
      "DivideMix Training: [Epoch 29, Loss1: 0.001210, Loss2: 0.001265]\n",
      "Evaluation Set: [Epoch 29, Loss: 3.357678, Acc: 0.6904, AUC: 0.7359]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.01batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 199.72batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12510/12510 [02:21<00:00, 88.42batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12432/12432 [02:18<00:00, 89.71batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 85.53batches/s]\n",
      "Training Set: [Epoch 30, Loss1: 1.700894, Loss2: 1.749016]\n",
      "DivideMix Training: [Epoch 30, Loss1: 0.001220, Loss2: 0.001246]\n",
      "Evaluation Set: [Epoch 30, Loss: 3.343096, Acc: 0.6904, AUC: 0.7330]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.93batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 199.35batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12482/12482 [02:21<00:00, 88.25batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12562/12562 [02:20<00:00, 89.23batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.58batches/s]\n",
      "Training Set: [Epoch 31, Loss1: 1.738573, Loss2: 1.722221]\n",
      "DivideMix Training: [Epoch 31, Loss1: 0.001213, Loss2: 0.001272]\n",
      "Evaluation Set: [Epoch 31, Loss: 3.307831, Acc: 0.6899, AUC: 0.7348]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 189.17batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 198.74batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12618/12618 [02:22<00:00, 88.60batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12448/12448 [02:19<00:00, 89.19batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.41batches/s]\n",
      "Training Set: [Epoch 32, Loss1: 1.711690, Loss2: 1.704137]\n",
      "DivideMix Training: [Epoch 32, Loss1: 0.001224, Loss2: 0.001251]\n",
      "Evaluation Set: [Epoch 32, Loss: 3.148919, Acc: 0.6900, AUC: 0.7269]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.75batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 198.54batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12489/12489 [02:21<00:00, 88.51batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12506/12506 [02:19<00:00, 89.78batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.72batches/s]\n",
      "Training Set: [Epoch 33, Loss1: 1.549881, Loss2: 1.713401]\n",
      "DivideMix Training: [Epoch 33, Loss1: 0.001212, Loss2: 0.001250]\n",
      "Evaluation Set: [Epoch 33, Loss: 3.365460, Acc: 0.6908, AUC: 0.7372]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.24batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.56batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12586/12586 [02:21<00:00, 88.99batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12453/12453 [02:19<00:00, 89.40batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.37batches/s]\n",
      "Training Set: [Epoch 34, Loss1: 1.737878, Loss2: 1.731319]\n",
      "DivideMix Training: [Epoch 34, Loss1: 0.001230, Loss2: 0.001250]\n",
      "Evaluation Set: [Epoch 34, Loss: 3.431361, Acc: 0.6902, AUC: 0.7358]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.28batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 198.87batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12480/12480 [02:21<00:00, 88.11batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12536/12536 [02:19<00:00, 90.06batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.22batches/s]\n",
      "Training Set: [Epoch 35, Loss1: 1.750366, Loss2: 1.775663]\n",
      "DivideMix Training: [Epoch 35, Loss1: 0.001211, Loss2: 0.001257]\n",
      "Evaluation Set: [Epoch 35, Loss: 3.278522, Acc: 0.6903, AUC: 0.7339]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.69batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 199.56batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12564/12564 [02:22<00:00, 87.92batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12443/12443 [02:19<00:00, 89.48batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 85.17batches/s]\n",
      "Training Set: [Epoch 36, Loss1: 1.680732, Loss2: 1.697086]\n",
      "DivideMix Training: [Epoch 36, Loss1: 0.001220, Loss2: 0.001248]\n",
      "Evaluation Set: [Epoch 36, Loss: 3.288430, Acc: 0.6907, AUC: 0.7305]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 191.17batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.07batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12510/12510 [02:21<00:00, 88.41batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12558/12558 [02:19<00:00, 89.99batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 84.08batches/s]\n",
      "Training Set: [Epoch 37, Loss1: 1.651429, Loss2: 1.719263]\n",
      "DivideMix Training: [Epoch 37, Loss1: 0.001214, Loss2: 0.001262]\n",
      "Evaluation Set: [Epoch 37, Loss: 3.324276, Acc: 0.6906, AUC: 0.7347]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 190.05batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 198.94batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12591/12591 [02:19<00:00, 90.09batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12498/12498 [02:18<00:00, 90.41batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 86.63batches/s]\n",
      "Training Set: [Epoch 38, Loss1: 1.667673, Loss2: 1.723144]\n",
      "DivideMix Training: [Epoch 38, Loss1: 0.001216, Loss2: 0.001250]\n",
      "Evaluation Set: [Epoch 38, Loss: 3.292112, Acc: 0.6903, AUC: 0.7315]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 195.04batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 203.35batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12535/12535 [02:19<00:00, 90.04batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12558/12558 [02:18<00:00, 90.49batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 86.26batches/s]\n",
      "Training Set: [Epoch 39, Loss1: 1.631948, Loss2: 1.718663]\n",
      "DivideMix Training: [Epoch 39, Loss1: 0.001207, Loss2: 0.001254]\n",
      "Evaluation Set: [Epoch 39, Loss: 3.433929, Acc: 0.6907, AUC: 0.7389]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 194.50batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 204.10batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12618/12618 [02:17<00:00, 91.48batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12503/12503 [02:13<00:00, 93.57batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 91.07batches/s]\n",
      "Training Set: [Epoch 40, Loss1: 1.744741, Loss2: 1.738202]\n",
      "DivideMix Training: [Epoch 40, Loss1: 0.001215, Loss2: 0.001242]\n",
      "Evaluation Set: [Epoch 40, Loss: 3.521146, Acc: 0.6909, AUC: 0.7402]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 205.94batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 215.38batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12579/12579 [02:15<00:00, 93.07batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12607/12607 [02:16<00:00, 92.27batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 86.02batches/s]\n",
      "Training Set: [Epoch 41, Loss1: 1.791242, Loss2: 1.767173]\n",
      "DivideMix Training: [Epoch 41, Loss1: 0.001218, Loss2: 0.001257]\n",
      "Evaluation Set: [Epoch 41, Loss: 3.446869, Acc: 0.6918, AUC: 0.7373]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 195.41batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 205.63batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12615/12615 [02:18<00:00, 91.23batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12581/12581 [02:16<00:00, 92.18batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 86.28batches/s]\n",
      "Training Set: [Epoch 42, Loss1: 1.742351, Loss2: 1.742527]\n",
      "DivideMix Training: [Epoch 42, Loss1: 0.001217, Loss2: 0.001256]\n",
      "Evaluation Set: [Epoch 42, Loss: 3.303297, Acc: 0.6915, AUC: 0.7294]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 195.63batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 203.83batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12611/12611 [02:20<00:00, 90.02batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12590/12590 [02:17<00:00, 91.41batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 86.97batches/s]\n",
      "Training Set: [Epoch 43, Loss1: 1.696705, Loss2: 1.643793]\n",
      "DivideMix Training: [Epoch 43, Loss1: 0.001216, Loss2: 0.001262]\n",
      "Evaluation Set: [Epoch 43, Loss: 3.416463, Acc: 0.6915, AUC: 0.7334]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 196.26batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 205.98batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12620/12620 [02:20<00:00, 89.75batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12575/12575 [02:17<00:00, 91.21batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 85.96batches/s]\n",
      "Training Set: [Epoch 44, Loss1: 1.736331, Loss2: 1.711528]\n",
      "DivideMix Training: [Epoch 44, Loss1: 0.001221, Loss2: 0.001251]\n",
      "Evaluation Set: [Epoch 44, Loss: 3.357100, Acc: 0.6913, AUC: 0.7314]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 196.44batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 203.82batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12602/12602 [02:20<00:00, 89.45batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12634/12634 [02:17<00:00, 92.21batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.11batches/s]\n",
      "Training Set: [Epoch 45, Loss1: 1.707288, Loss2: 1.673887]\n",
      "DivideMix Training: [Epoch 45, Loss1: 0.001208, Loss2: 0.001256]\n",
      "Evaluation Set: [Epoch 45, Loss: 3.331771, Acc: 0.6916, AUC: 0.7270]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.37batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 210.58batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12647/12647 [02:17<00:00, 92.27batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12579/12579 [02:14<00:00, 93.64batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.31batches/s]\n",
      "Training Set: [Epoch 46, Loss1: 1.725616, Loss2: 1.624473]\n",
      "DivideMix Training: [Epoch 46, Loss1: 0.001219, Loss2: 0.001250]\n",
      "Evaluation Set: [Epoch 46, Loss: 3.422273, Acc: 0.6913, AUC: 0.7376]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.28batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 209.23batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12611/12611 [02:17<00:00, 91.62batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12604/12604 [02:14<00:00, 93.74batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.21batches/s]\n",
      "Training Set: [Epoch 47, Loss1: 1.723244, Loss2: 1.716572]\n",
      "DivideMix Training: [Epoch 47, Loss1: 0.001212, Loss2: 0.001249]\n",
      "Evaluation Set: [Epoch 47, Loss: 3.371383, Acc: 0.6916, AUC: 0.7322]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.39batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 210.31batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12673/12673 [02:18<00:00, 91.38batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12596/12596 [02:14<00:00, 93.70batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 88.79batches/s]\n",
      "Training Set: [Epoch 48, Loss1: 1.700972, Loss2: 1.685622]\n",
      "DivideMix Training: [Epoch 48, Loss1: 0.001221, Loss2: 0.001249]\n",
      "Evaluation Set: [Epoch 48, Loss: 3.382716, Acc: 0.6915, AUC: 0.7358]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 200.08batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 208.63batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12625/12625 [02:17<00:00, 91.52batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12617/12617 [02:14<00:00, 93.46batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 88.87batches/s]\n",
      "Training Set: [Epoch 49, Loss1: 1.661309, Loss2: 1.734893]\n",
      "DivideMix Training: [Epoch 49, Loss1: 0.001210, Loss2: 0.001248]\n",
      "Evaluation Set: [Epoch 49, Loss: 3.313598, Acc: 0.6915, AUC: 0.7337]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.83batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 210.24batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12614/12614 [02:17<00:00, 91.62batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12645/12645 [02:14<00:00, 94.34batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.79batches/s]\n",
      "Training Set: [Epoch 50, Loss1: 1.661618, Loss2: 1.663846]\n",
      "DivideMix Training: [Epoch 50, Loss1: 0.001206, Loss2: 0.001256]\n",
      "Evaluation Set: [Epoch 50, Loss: 3.411925, Acc: 0.6922, AUC: 0.7321]\n",
      "Finished Training.\n",
      "Training took 205.487351389726 minute.\n"
     ]
    }
   ],
   "source": [
    "warmup_loader = loader.load(terminus=\"n\", mode=\"warmup\")\n",
    "train_gmm_loader = loader.load(terminus=\"n\", mode=\"divide_by_GMM\")\n",
    "eval_loader = loader.load(terminus=\"n\", mode=\"evaluate\")\n",
    "\n",
    "start = time()\n",
    "highest_val_auc = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if epoch < NUM_WARM_UP_EPOCHS + 1:\n",
    "        # run warm up model 1 and 2 while adding penalty for confident predictions\n",
    "        warmup_loss1, warmup_acc1, warmup_auc1 = warmup(\n",
    "            epoch, model1, optimizer1, warmup_loader\n",
    "        )\n",
    "        warmup_loss2, warmup_acc2, warmup_auc2 = warmup(\n",
    "            epoch, model2, optimizer2, warmup_loader\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Warm-Up Model1: [Epoch {epoch:2d}, Loss: {warmup_loss1:8.6f}, Acc: {warmup_acc1:.4f}, AUC: {warmup_auc1:.4f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Warm-Up Model2: [Epoch {epoch:2d}, Loss: {warmup_loss2:8.6f}, Acc: {warmup_acc2:.4f}, AUC: {warmup_auc2:.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # evaluate on dev set\n",
    "        val_acc, val_auc, val_loss = evaluate(model1, model2, eval_loader)\n",
    "        print(\n",
    "            f\"Evaluation Set: [Epoch {epoch:2d}, Loss: {val_loss:.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        prob1, train_loss1 = process_gmm(model1, train_gmm_loader)\n",
    "        prob2, train_loss2 = process_gmm(model2, train_gmm_loader)\n",
    "\n",
    "        pred1 = prob1 > P_THRESHOLD\n",
    "        pred2 = prob2 > P_THRESHOLD\n",
    "\n",
    "        # train both models\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.load(\n",
    "            terminus=\"n\", mode=\"train\", pred=pred2, prob=prob2\n",
    "        )\n",
    "        divmix_loss1 = train(\n",
    "            epoch,\n",
    "            model1,\n",
    "            model2,\n",
    "            optimizer1,\n",
    "            labeled_trainloader,\n",
    "            unlabeled_trainloader,\n",
    "            \"model1\"\n",
    "        )\n",
    "\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.load(\n",
    "            terminus=\"n\", mode=\"train\", pred=pred1, prob=prob1\n",
    "        )\n",
    "        divmix_loss2 = train(\n",
    "            epoch,\n",
    "            model2,\n",
    "            model1,\n",
    "            optimizer2,\n",
    "            labeled_trainloader,\n",
    "            unlabeled_trainloader,\n",
    "            \"model2\"\n",
    "        )\n",
    "\n",
    "        # evaluate on dev set\n",
    "        val_acc, val_auc, val_loss = evaluate(model1, model2, eval_loader)\n",
    "\n",
    "        # if val_auc > highest_val_auc:\n",
    "        #     highest_val_auc = val_auc\n",
    "        #     path1 = f\"../params/n_term/BiLSTM/auc{val_auc:.4f}_epoch{epoch}_model1.pt\"\n",
    "        #     path2 = f\"../params/n_term/BiLSTM/auc{val_auc:.4f}_epoch{epoch}_model2.pt\"\n",
    "        #     torch.save(model1.state_dict(), path1)\n",
    "        #     torch.save(model2.state_dict(), path2)\n",
    "\n",
    "        print(\n",
    "            f\"Training Set: [Epoch {epoch:2d}, Loss1: {train_loss1:.6f}, Loss2: {train_loss2:.6f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"DivideMix Training: [Epoch {epoch:2d}, Loss1: {divmix_loss1:.6f}, Loss2: {divmix_loss2:.6f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Evaluation Set: [Epoch {epoch:2d}, Loss: {val_loss:.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1610c-9c28-4e0e-874f-3ddd6b92f6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
