{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e884ac-3cdd-467c-a96b-1417bf3eac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and print the lengths of predicted clean and predicted unclean\n",
    "# how can dataloader get to 12k twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f97cd-fdb5-4150-84a9-54aae3c67ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check individual losses, Lx, Lu, what is driving the increase in loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efc203-1a26-4514-8c9b-861173d350a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create eval dataset with only 1s\n",
    "# is the model able to predict the 1s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0036b-4c27-4cad-aa8f-67f994069794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase LAMBDA_U to 150 or so, linear rampup only to 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0ffb3-1414-4112-8d94-59b7bf0a556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rampup len to semiloss/criterion call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b376cff-a0bb-42ab-8e39-4a964c417dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../denoise/')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "\n",
    "from divide_mix.data_handling import CleavageLoader\n",
    "from divide_mix.train_utils import (\n",
    "    NegEntropy,\n",
    "    SemiLoss,\n",
    "    warmup,\n",
    "    train,\n",
    "    evaluate,\n",
    "    process_gmm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1fd9d5-ed2f-49d4-99ae-bccc7ddcee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa607d1-bf4e-459d-b577-b3d10235a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trainable parameters in model\n",
    "def get_total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228ec4a9-8484-45d5-8af5-b8459f876d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size1, rnn_size2, hidden_size, dropout1, dropout2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        self.dropout1=nn.Dropout(dropout1)\n",
    "        self.dropout2=nn.Dropout(dropout2)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2 * rnn_size1,\n",
    "            hidden_size=rnn_size2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = spectral_norm(nn.Linear(rnn_size2 * 2, hidden_size))\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    \n",
    "    def no_embed_fw(self, embedded):\n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm1(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*rnn_size1)\n",
    "        out, _ = self.lstm2(out)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, 2*hidden_size)\n",
    "        out = self.dropout1(gelu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size, 2)\n",
    "        return self.fc2(out)\n",
    "    \n",
    "    def forward(self, seq, seq2=None, lam=None, interpolate=False):\n",
    "        if interpolate:\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            embedded1 = self.embedding(seq)\n",
    "            embedded2 = self.embedding(seq2)\n",
    "            embedded_mixed = lam * embedded1 + (1 - lam) * embedded2\n",
    "            return self.no_embed_fw(self.dropout2(embedded_mixed))\n",
    "        else:\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            embedded = self.dropout1(self.embedding(seq))\n",
    "            return self.no_embed_fw(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1574351f-3461-4821-baee-c2eb4da25a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 5e-5\n",
    "ALPHA = 0.5\n",
    "LAMBDA_U = 0\n",
    "P_THRESHOLD = 0.5\n",
    "TEMPERATURE = 0.5\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WARM_UP_EPOCHS = 1\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "vocab = torch.load('../data/vocab.pt')\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "beta_dist = torch.distributions.beta.Beta(ALPHA, ALPHA)\n",
    "loader = CleavageLoader(batch_size=BATCH_SIZE, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8270e3c-2991-4df1-8371-c5ef80139f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=200,\n",
    "    rnn_size1=256,\n",
    "    rnn_size2=512,\n",
    "    hidden_size=128,\n",
    "    dropout1=0.5,\n",
    "    dropout2=0.\n",
    ").to(device)\n",
    "\n",
    "model2 = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=20,\n",
    "    rnn_size1=256,\n",
    "    rnn_size2=512,\n",
    "    hidden_size=128,\n",
    "    dropout1=0.5,\n",
    "    dropout2=0.\n",
    ").to(device)\n",
    "\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=LR)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=LR)\n",
    "\n",
    "criterion = SemiLoss()\n",
    "conf_penalty = NegEntropy()\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "CE = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59aeee47-a9fa-4c77-9058-f619df3886a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:20<00:00, 108.54batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2236/2236 [00:19<00:00, 116.34batches/s]\n",
      "Warm-Up Model1: [Epoch  1, Loss: -0.000036, Acc: 0.6033, AUC: 0.6193]\n",
      "Warm-Up Model2: [Epoch  1, Loss: -0.000026, Acc: 0.5907, AUC: 0.5887]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.96batches/s]\n",
      "Evaluation Set: [Epoch  1, Loss: 0.589756, Acc: 0.6788, AUC: 0.7463]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 205.79batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 213.23batches/s]\n",
      "train model1: 100%|█████████████████████████████████████████████| 5692/5692 [01:01<00:00, 92.24batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 10635/10635 [01:53<00:00, 93.52batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.86batches/s]\n",
      "Training Set: [Epoch  2, Loss1: 0.610691, Loss2: 0.638930]\n",
      "DivideMix Training: [Epoch  2, Loss1: 0.000999, Loss2: 0.001405]\n",
      "Evaluation Set: [Epoch  2, Loss: 2.029217, Acc: 0.6713, AUC: 0.7367]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.88batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 211.63batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12131/12131 [02:11<00:00, 92.20batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 11750/11750 [02:05<00:00, 93.37batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.28batches/s]\n",
      "Training Set: [Epoch  3, Loss1: 1.460160, Loss2: 0.983086]\n",
      "DivideMix Training: [Epoch  3, Loss1: 0.001335, Loss2: 0.001355]\n",
      "Evaluation Set: [Epoch  3, Loss: 2.267922, Acc: 0.6812, AUC: 0.7317]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.55batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 212.40batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 11851/11851 [02:08<00:00, 92.28batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12221/12221 [02:10<00:00, 93.48batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.70batches/s]\n",
      "Training Set: [Epoch  4, Loss1: 1.345515, Loss2: 1.307319]\n",
      "DivideMix Training: [Epoch  4, Loss1: 0.001287, Loss2: 0.001352]\n",
      "Evaluation Set: [Epoch  4, Loss: 2.278784, Acc: 0.6799, AUC: 0.7302]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.71batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 211.50batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12274/12274 [02:12<00:00, 92.60batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 11954/11954 [02:07<00:00, 94.08batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.13batches/s]\n",
      "Training Set: [Epoch  5, Loss1: 1.416849, Loss2: 1.267082]\n",
      "DivideMix Training: [Epoch  5, Loss1: 0.001315, Loss2: 0.001305]\n",
      "Evaluation Set: [Epoch  5, Loss: 2.391390, Acc: 0.6850, AUC: 0.7337]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.73batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 211.59batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12046/12046 [02:11<00:00, 91.65batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12273/12273 [02:10<00:00, 94.32batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.67batches/s]\n",
      "Training Set: [Epoch  6, Loss1: 1.386949, Loss2: 1.406326]\n",
      "DivideMix Training: [Epoch  6, Loss1: 0.001288, Loss2: 0.001336]\n",
      "Evaluation Set: [Epoch  6, Loss: 2.583530, Acc: 0.6827, AUC: 0.7343]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.82batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 212.22batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12328/12328 [02:13<00:00, 92.52batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12108/12108 [02:09<00:00, 93.72batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.95batches/s]\n",
      "Training Set: [Epoch  7, Loss1: 1.585714, Loss2: 1.388867]\n",
      "DivideMix Training: [Epoch  7, Loss1: 0.001296, Loss2: 0.001303]\n",
      "Evaluation Set: [Epoch  7, Loss: 2.703663, Acc: 0.6860, AUC: 0.7381]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.85batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 213.15batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12142/12142 [02:11<00:00, 92.20batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12335/12335 [02:11<00:00, 93.54batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 81.16batches/s]\n",
      "Training Set: [Epoch  8, Loss1: 1.574125, Loss2: 1.486544]\n",
      "DivideMix Training: [Epoch  8, Loss1: 0.001268, Loss2: 0.001323]\n",
      "Evaluation Set: [Epoch  8, Loss: 2.732342, Acc: 0.6841, AUC: 0.7358]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 202.18batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 210.11batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12338/12338 [02:14<00:00, 91.45batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12185/12185 [02:09<00:00, 94.09batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 89.72batches/s]\n",
      "Training Set: [Epoch  9, Loss1: 1.693872, Loss2: 1.407679]\n",
      "DivideMix Training: [Epoch  9, Loss1: 0.001286, Loss2: 0.001294]\n",
      "Evaluation Set: [Epoch  9, Loss: 2.721961, Acc: 0.6881, AUC: 0.7385]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 201.90batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1118/1118 [00:05<00:00, 211.32batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 12175/12175 [02:12<00:00, 91.70batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 12378/12378 [02:12<00:00, 93.45batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 140/140 [00:01<00:00, 90.31batches/s]\n",
      "Training Set: [Epoch 10, Loss1: 1.614656, Loss2: 1.485868]\n",
      "DivideMix Training: [Epoch 10, Loss1: 0.001249, Loss2: 0.001325]\n",
      "Evaluation Set: [Epoch 10, Loss: 2.819789, Acc: 0.6851, AUC: 0.7349]\n",
      "Finished Training.\n",
      "Training took 41.70003985961278 minute.\n"
     ]
    }
   ],
   "source": [
    "warmup_loader = loader.load(terminus=\"n\", mode=\"warmup\")\n",
    "train_gmm_loader = loader.load(terminus=\"n\", mode=\"divide_by_GMM\")\n",
    "eval_loader = loader.load(terminus=\"n\", mode=\"evaluate\")\n",
    "\n",
    "start = time()\n",
    "highest_val_auc = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if epoch < NUM_WARM_UP_EPOCHS + 1:\n",
    "        # run warm up model 1 and 2 while adding penalty for confident predictions\n",
    "        warmup_loss1, warmup_acc1, warmup_auc1 = warmup(\n",
    "            model=model1,\n",
    "            optimizer=optimizer1,\n",
    "            loss_func=CEloss,\n",
    "            conf_penalty=conf_penalty,\n",
    "            dataloader=warmup_loader\n",
    "        )\n",
    "        warmup_loss2, warmup_acc2, warmup_auc2 = warmup(\n",
    "            model=model2,\n",
    "            optimizer=optimizer2,\n",
    "            loss_func=CEloss,\n",
    "            conf_penalty=conf_penalty,\n",
    "            dataloader=warmup_loader\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Warm-Up Model1: [Epoch {epoch:2d}, Loss: {warmup_loss1:8.6f}, Acc: {warmup_acc1:.4f}, AUC: {warmup_auc1:.4f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Warm-Up Model2: [Epoch {epoch:2d}, Loss: {warmup_loss2:8.6f}, Acc: {warmup_acc2:.4f}, AUC: {warmup_auc2:.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # evaluate on dev set\n",
    "        val_acc, val_auc, val_loss = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loss_func=CE,\n",
    "            dataloader=eval_loader\n",
    "        )\n",
    "        print(\n",
    "            f\"Evaluation Set: [Epoch {epoch:2d}, Loss: {val_loss:.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        prob1, train_loss1 = process_gmm(model1, train_gmm_loader, CE)\n",
    "        prob2, train_loss2 = process_gmm(model2, train_gmm_loader, CE)\n",
    "\n",
    "        pred1 = prob1 > P_THRESHOLD\n",
    "        pred2 = prob2 > P_THRESHOLD\n",
    "\n",
    "        # train both models\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.load(\n",
    "            terminus=\"n\", mode=\"train\", pred=pred2, prob=prob2\n",
    "        )\n",
    "        divmix_loss1 = train(\n",
    "            epoch=epoch,\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            optimizer=optimizer1,\n",
    "            loss_func=criterion,\n",
    "            num_warm_up_epochs=NUM_WARM_UP_EPOCHS,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            lambda_u=LAMBDA_U,\n",
    "            temp=TEMPERATURE,\n",
    "            beta_dist=beta_dist,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            labeled_loader=labeled_trainloader,\n",
    "            unlabeled_loader=unlabeled_trainloader,\n",
    "            named_model=\"model1\"\n",
    "        )\n",
    "\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.load(\n",
    "            terminus=\"n\", mode=\"train\", pred=pred1, prob=prob1\n",
    "        )\n",
    "        divmix_loss2 = train(\n",
    "            epoch=epoch,\n",
    "            model1=model2,\n",
    "            model2=model1,\n",
    "            optimizer=optimizer2,\n",
    "            loss_func=criterion,\n",
    "            num_warm_up_epochs=NUM_WARM_UP_EPOCHS,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            lambda_u=LAMBDA_U,\n",
    "            temp=TEMPERATURE,\n",
    "            beta_dist=beta_dist,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            labeled_loader=labeled_trainloader,\n",
    "            unlabeled_loader=unlabeled_trainloader,\n",
    "            named_model=\"model2\"\n",
    "        )\n",
    "\n",
    "        # evaluate on dev set\n",
    "        val_acc, val_auc, val_loss = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loss_func=CE,\n",
    "            dataloader=eval_loader\n",
    "        )\n",
    "\n",
    "        # if val_auc > highest_val_auc:\n",
    "        #     highest_val_auc = val_auc\n",
    "        #     path1 = f\"../params/n_term/BiLSTM/auc{val_auc:.4f}_epoch{epoch}_model1.pt\"\n",
    "        #     path2 = f\"../params/n_term/BiLSTM/auc{val_auc:.4f}_epoch{epoch}_model2.pt\"\n",
    "        #     torch.save(model1.state_dict(), path1)\n",
    "        #     torch.save(model2.state_dict(), path2)\n",
    "\n",
    "        print(\n",
    "            f\"Training Set: [Epoch {epoch:2d}, Loss1: {train_loss1:.6f}, Loss2: {train_loss2:.6f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"DivideMix Training: [Epoch {epoch:2d}, Loss1: {divmix_loss1:.6f}, Loss2: {divmix_loss2:.6f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Evaluation Set: [Epoch {epoch:2d}, Loss: {val_loss:.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4685bb-ae2f-4d99-bd0c-886f9474b470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
