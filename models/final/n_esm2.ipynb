{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 3 (i.e. <unk>) at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m) + (dist == m) * 3\n",
    "\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "\n",
    "def save_metrics(*args, path):\n",
    "    if not os.path.isfile(path):\n",
    "        with open(path, \"w\", newline=\"\\n\") as f:\n",
    "            f.write(\n",
    "                \",\".join(\n",
    "                    [\n",
    "                        \"fold\",\n",
    "                        \"epoch\",\n",
    "                        \"train_loss\",\n",
    "                        \"train_acc\",\n",
    "                        \"train_auc\",\n",
    "                        \"val_loss\",\n",
    "                        \"val_acc\",\n",
    "                        \"val_auc\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "    if args:\n",
    "        with open(path, \"a\", newline=\"\\n\") as f:\n",
    "            f.write(\",\".join([str(arg) for arg in args]))\n",
    "            f.write(\"\\n\") \n",
    "            \n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lbl[idx], self.seq[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "\n",
    "\n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        lbls, _, seq = tokenizer(batch)\n",
    "        self.seq = apply_random_masking(seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in lbls], dtype=torch.float)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "\n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        lbls, _, self.seq = tokenizer(batch)\n",
    "        self.lbl = torch.tensor([int(l) for l in lbls], dtype=torch.float)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2Finetune(nn.Module):\n",
    "    def __init__(self, pretrained_model, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm2 = pretrained_model\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(33, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # input shape: (batch_size, seq_len=10+2 (cls, eos))\n",
    "        result = self.esm2(seq)['logits'][:, 1:10+1, :] # remove cls, eos token position\n",
    "        result = self.dropout(result)\n",
    "        \n",
    "        # in: (batch_size, seq_len, vocab_size=33)\n",
    "        result, _ = result.max(dim=1)\n",
    "\n",
    "        # input shape: (batch_size, 33)\n",
    "        # out shape: (batch_size)\n",
    "        return self.fc(result).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    preds, lbls = [], []\n",
    "    \n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += ((scores > 0) == lbl).sum().item()\n",
    "        total += len(seq)\n",
    "        preds.extend(scores.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47db27a3-072e-47fe-b817-d5ebd54e6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ingo/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# load train+dev set, mix it back as one\n",
    "train_path = '../../data/n_train.csv'\n",
    "dev_path = '../../data/n_val.csv'\n",
    "test_path = '../../data/n_test.csv'\n",
    "\n",
    "# combine previously split train and dev set\n",
    "train_seqs, train_lbls = read_data(train_path)\n",
    "dev_seqs, dev_lbls = read_data(dev_path)\n",
    "total_seqs, total_lbls = np.array(train_seqs + dev_seqs), np.array(train_lbls + dev_lbls)\n",
    "\n",
    "assert len(train_seqs) + len(dev_seqs) == len(total_seqs)\n",
    "assert len(train_lbls) + len(dev_lbls) == len(total_lbls)\n",
    "\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=64, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load pre-trained esm2 model and vocab\n",
    "_, vocab = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "tokenizer = vocab.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "logging_path = \"../../params/n_term/esm2finetune/results.csv\"\n",
    "\n",
    "start = time()\n",
    "print(\"Starting Cross-Validation.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "# get a new split\n",
    "for fold, (train_idx, dev_idx) in enumerate(kf.split(total_seqs), 1):\n",
    "    X_tr = total_seqs[train_idx]\n",
    "    y_tr = total_lbls[train_idx]\n",
    "    X_dev = total_seqs[dev_idx]\n",
    "    y_dev = total_lbls[dev_idx]\n",
    "\n",
    "    # create datasets and loads with current split\n",
    "    train_data = CleavageDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_wrapper,\n",
    "        pin_memory=True,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "    dev_data = CleavageDataset(X_dev, y_dev)\n",
    "    dev_loader = DataLoader(\n",
    "        dev_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=eval_wrapper,\n",
    "        pin_memory=True,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "    # reset model weights with each new fold\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "        print('deleted model')\n",
    "    if 'esm2' in globals():\n",
    "        del esm2\n",
    "        print('deleted esm2')\n",
    "    esm2, _ = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "    model = ESM2Finetune(esm2, dropout=0.5).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # normal training loop\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss, train_acc, train_auc = process(\n",
    "            model, train_loader, criterion, optimizer\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "\n",
    "        # save metrics\n",
    "        save_metrics(\n",
    "            fold,\n",
    "            epoch,\n",
    "            train_loss,\n",
    "            train_acc,\n",
    "            train_auc,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "            val_auc,\n",
    "            path=logging_path,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Training:   [Fold {fold:2d}, Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Evaluation: [Fold {fold:2d}, Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        reg_auc = regularized_auc(train_auc, val_auc, threshold=0)\n",
    "        if reg_auc > highest_val_auc:\n",
    "            highest_val_auc = reg_auc\n",
    "            path = f\"../../params/n_term/esm2finetune/auc{reg_auc:.4f}_fold{fold}_epoch{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "print(\"Finished Cross-Validation.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Cross-Validation took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fb8e7d-4abb-4e41-bb99-7f35c5db81cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ingo/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.7871_fold3_epoch3.pt\n",
      "Eval: 100%|█████████████████████████████████████████████████████| 2243/2243 [00:50<00:00, 44.37batches/s]\n",
      "Test Set Performance: Loss: 0.006317, Acc: 0.8263, AUC: 0.7891\n",
      "Total model params: 148140188, trainable model params: 148140188\n"
     ]
    }
   ],
   "source": [
    "esm2, _ = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "model = ESM2Finetune(esm2, dropout=0.5).to('cpu')\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/esm2finetune/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "checkpoint = torch.load('../../params/n_term/esm2finetune/' + best_model, map_location='cpu')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc, test_auc = process(model, test_loader, criterion)\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
