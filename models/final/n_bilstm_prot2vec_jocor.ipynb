{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9d8cd4-0fbf-43ef-a391-5915dfac8927",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* BiLSTM model architecture based on [Ozols et. al., 2021](https://www.mdpi.com/1422-0067/22/6/3071/htm)\n",
    "* Prot2Vec embeddings based on [Asgari and Mofrad, 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287), available on [Github](https://github.com/ehsanasgari/Deep-Proteomics)\n",
    "* JoCoR loss function and training process adaptations are based on [Wei et al., 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Wei_Combating_Noisy_Labels_by_Agreement_A_Joint_Training_Method_with_CVPR_2020_paper.html), and official implementation on [Github](https://github.com/hongxin001/JoCoR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        seqs, lbls = [], []\n",
    "        for l in f.readlines()[1:]:\n",
    "            seq, lbl = l.strip().split('\\t')\n",
    "            seqs.append(seq)\n",
    "            lbls.append(lbl)\n",
    "    return seqs, lbls\n",
    "\n",
    "def read_embeddings(path):\n",
    "    with open(path, 'r') as f:\n",
    "        seq, vec = [], []\n",
    "        for line in f.readlines()[2:]: # skip first special chars\n",
    "            lst = line.split()\n",
    "            seq.append(lst[0].upper())\n",
    "            vec.append([float(i) for i in lst[1:]])\n",
    "        vocab = {s: i for i, s in enumerate(seq)}\n",
    "        prot2vec = torch.tensor(vec, dtype=torch.float)\n",
    "    return vocab, prot2vec\n",
    "\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 0 at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m)\n",
    "\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.seq = apply_random_masking(seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "\n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMProt2Vec(nn.Module):\n",
    "    def __init__(self, pretrained_embeds, rnn_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        embeding_dim = pretrained_embeds.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=pretrained_embeds,\n",
    "            freeze=True\n",
    "        )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embeding_dim,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.dropout(self.embedding(seq))\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, 2*hidden_size)\n",
    "        out = self.dropout(gelu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size)\n",
    "        return self.fc2(out).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261bb1c4-99fa-41b1-b9b6-2e0e3e990046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss_compute(pred, soft_targets):\n",
    "    # adjusted for binary case\n",
    "    kl = F.kl_div(F.logsigmoid(pred), torch.sigmoid(soft_targets), reduction='none')\n",
    "    return torch.sum(kl)\n",
    "\n",
    "\n",
    "class JoCoRLoss:\n",
    "    def __call__(self, y1, y2, lbls, forget_rate, loss_func, kl_loss, co_lambda=0.1):\n",
    "        loss_pick_1 = loss_func(y1, lbls) * (1 - co_lambda)\n",
    "        loss_pick_2 = loss_func(y2, lbls) * (1 - co_lambda)\n",
    "        loss_pick = (\n",
    "            loss_pick_1\n",
    "            + loss_pick_2\n",
    "            + co_lambda * kl_loss_compute(y1, y2)\n",
    "            + co_lambda * kl_loss_compute(y2, y1)\n",
    "        ).cpu()\n",
    "\n",
    "        ind_sorted = np.argsort(loss_pick.data)\n",
    "        loss_sorted = loss_pick[ind_sorted]\n",
    "\n",
    "        remember_rate = 1 - forget_rate\n",
    "        num_remember = int(remember_rate * len(loss_sorted))\n",
    "\n",
    "        ind_update = ind_sorted[:num_remember]\n",
    "\n",
    "        loss = torch.mean(loss_pick[ind_update])\n",
    "\n",
    "        return loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86820b15-3216-4c56-a9ff-91aa162898fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model1, model2, optim, loss_func, loader, forget_rate, BCEWLL, kl_loss):\n",
    "    epoch_loss1, num_correct1, total = 0, 0, 0\n",
    "    epoch_loss2, num_correct2 = 0, 0\n",
    "    preds1, preds2, lbls = [], [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores1 = model1(seq)\n",
    "        scores2 = model2(seq)\n",
    "\n",
    "        # JoCoR loss\n",
    "        loss1, loss2 = loss_func(scores1, scores2, lbl, forget_rate, BCEWLL, kl_loss)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss1.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss1 += loss1.item()\n",
    "        epoch_loss2 += loss2.item()\n",
    "        num_correct1 += ((scores1 > 0) == lbl).sum().item()\n",
    "        num_correct2 += ((scores2 > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds1.extend(scores1.detach().tolist())\n",
    "        preds2.extend(scores2.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss1 / total,\n",
    "        epoch_loss2 / total,\n",
    "        num_correct1 / total,\n",
    "        num_correct2 / total,\n",
    "        roc_auc_score(lbls, preds1),\n",
    "        roc_auc_score(lbls, preds2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61478de2-cfe5-4081-b7ce-1a8222cef824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model1, model2, loader):\n",
    "    num_correct1, num_correct2, total = 0, 0, 0\n",
    "    preds1, preds2, lbls = [], [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores1 = model1(seq)\n",
    "        scores2 = model2(seq)\n",
    "        \n",
    "        num_correct1 += ((scores1 > 0) == lbl).sum().item()\n",
    "        num_correct2 += ((scores2 > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds1.extend(scores1.detach().tolist())\n",
    "        preds2.extend(scores2.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        num_correct1 / total,\n",
    "        num_correct2 / total,\n",
    "        roc_auc_score(lbls, preds1),\n",
    "        roc_auc_score(lbls, preds2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd6e8ef-424b-4c4f-b460-ca3f49b6f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    num_correct, total = 0, 0\n",
    "    preds, lbls = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        \n",
    "        num_correct += ((scores > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(scores.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return num_correct / total, roc_auc_score(lbls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/n_train_3mer.tsv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/n_val_3mer.tsv')\n",
    "\n",
    "# create vocab from train seqs\n",
    "# load vocab and embeddings\n",
    "vocab, prot2vec = read_embeddings('../../params/uniref_3M/uniref_3M.vec')\n",
    "\n",
    "# encodes kmer sequence\n",
    "encode_text = lambda seq: [vocab.get(s, 0) for s in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 512\n",
    "VOCAB_SIZE = len(vocab)\n",
    "RNN_SIZE = 527\n",
    "HIDDEN_SIZE = 139\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "NUM_GRADUAL = 10 # how many epochs for linear drop rate\n",
    "NOISY_RATE = 0.2 # assumed\n",
    "FORGET_RATE = NOISY_RATE / 2 # recommendation when facing asymmetric loss\n",
    "EXPONENT = 1\n",
    "\n",
    "# define drop rate schedule\n",
    "rate_schedule = np.ones(NUM_EPOCHS)*FORGET_RATE\n",
    "rate_schedule[:NUM_GRADUAL] = np.linspace(0, FORGET_RATE**EXPONENT, NUM_GRADUAL)\n",
    "\n",
    "params = {\n",
    "    \"pretrained_embeds\": prot2vec,\n",
    "    \"rnn_size\": RNN_SIZE,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"dropout\": DROPOUT\n",
    "}\n",
    "\n",
    "model1 = BiLSTMProt2Vec(**params).to(device)\n",
    "model2 = BiLSTMProt2Vec(**params).to(device)\n",
    "\n",
    "BCEWLL = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion = JoCoRLoss()\n",
    "optimizer = optim.Adam(list(model1.parameters()) + list(model2.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=train_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.32batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 171.65batches/s]\n",
      "Model 1 Training:   [Epoch  1, Loss: 0.001928, Acc: 0.8173, AUC: 0.5127]\n",
      "Model 1 Evaluation: [Epoch  1, Acc: 0.8179, AUC: 0.6017]\n",
      "Model 2 Training:   [Epoch  1, Loss: 0.001928, Acc: 0.8175, AUC: 0.5117]\n",
      "Model 2 Evaluation: [Epoch  1, Acc: 0.8179, AUC: 0.6012]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.12batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 169.51batches/s]\n",
      "Model 1 Training:   [Epoch  2, Loss: 0.001792, Acc: 0.8183, AUC: 0.5362]\n",
      "Model 1 Evaluation: [Epoch  2, Acc: 0.8179, AUC: 0.6058]\n",
      "Model 2 Training:   [Epoch  2, Loss: 0.001792, Acc: 0.8183, AUC: 0.5337]\n",
      "Model 2 Evaluation: [Epoch  2, Acc: 0.8179, AUC: 0.6086]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.21batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 170.40batches/s]\n",
      "Model 1 Training:   [Epoch  3, Loss: 0.001670, Acc: 0.8183, AUC: 0.5602]\n",
      "Model 1 Evaluation: [Epoch  3, Acc: 0.8179, AUC: 0.5985]\n",
      "Model 2 Training:   [Epoch  3, Loss: 0.001670, Acc: 0.8183, AUC: 0.5586]\n",
      "Model 2 Evaluation: [Epoch  3, Acc: 0.8179, AUC: 0.5961]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 56.97batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 171.20batches/s]\n",
      "Model 1 Training:   [Epoch  4, Loss: 0.001554, Acc: 0.8183, AUC: 0.5726]\n",
      "Model 1 Evaluation: [Epoch  4, Acc: 0.8179, AUC: 0.5925]\n",
      "Model 2 Training:   [Epoch  4, Loss: 0.001554, Acc: 0.8183, AUC: 0.5715]\n",
      "Model 2 Evaluation: [Epoch  4, Acc: 0.8179, AUC: 0.5904]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.18batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 170.09batches/s]\n",
      "Model 1 Training:   [Epoch  5, Loss: 0.001452, Acc: 0.8183, AUC: 0.5826]\n",
      "Model 1 Evaluation: [Epoch  5, Acc: 0.8179, AUC: 0.6012]\n",
      "Model 2 Training:   [Epoch  5, Loss: 0.001452, Acc: 0.8183, AUC: 0.5827]\n",
      "Model 2 Evaluation: [Epoch  5, Acc: 0.8179, AUC: 0.6001]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.37batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 170.28batches/s]\n",
      "Model 1 Training:   [Epoch  6, Loss: 0.001337, Acc: 0.8183, AUC: 0.5902]\n",
      "Model 1 Evaluation: [Epoch  6, Acc: 0.8179, AUC: 0.5992]\n",
      "Model 2 Training:   [Epoch  6, Loss: 0.001337, Acc: 0.8183, AUC: 0.5900]\n",
      "Model 2 Evaluation: [Epoch  6, Acc: 0.8179, AUC: 0.6005]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:38<00:00, 57.57batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 169.80batches/s]\n",
      "Model 1 Training:   [Epoch  7, Loss: 0.001239, Acc: 0.8183, AUC: 0.5888]\n",
      "Model 1 Evaluation: [Epoch  7, Acc: 0.8179, AUC: 0.6016]\n",
      "Model 2 Training:   [Epoch  7, Loss: 0.001239, Acc: 0.8183, AUC: 0.5884]\n",
      "Model 2 Evaluation: [Epoch  7, Acc: 0.8179, AUC: 0.5992]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 56.94batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 169.16batches/s]\n",
      "Model 1 Training:   [Epoch  8, Loss: 0.001161, Acc: 0.8183, AUC: 0.5763]\n",
      "Model 1 Evaluation: [Epoch  8, Acc: 0.8179, AUC: 0.5987]\n",
      "Model 2 Training:   [Epoch  8, Loss: 0.001161, Acc: 0.8183, AUC: 0.5761]\n",
      "Model 2 Evaluation: [Epoch  8, Acc: 0.8179, AUC: 0.5955]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.04batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 169.90batches/s]\n",
      "Model 1 Training:   [Epoch  9, Loss: 0.001032, Acc: 0.8183, AUC: 0.5400]\n",
      "Model 1 Evaluation: [Epoch  9, Acc: 0.8179, AUC: 0.5893]\n",
      "Model 2 Training:   [Epoch  9, Loss: 0.001032, Acc: 0.8183, AUC: 0.5401]\n",
      "Model 2 Evaluation: [Epoch  9, Acc: 0.8179, AUC: 0.5919]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 56.98batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 166.56batches/s]\n",
      "Model 1 Training:   [Epoch 10, Loss: 0.000870, Acc: 0.8183, AUC: 0.5357]\n",
      "Model 1 Evaluation: [Epoch 10, Acc: 0.8179, AUC: 0.5883]\n",
      "Model 2 Training:   [Epoch 10, Loss: 0.000870, Acc: 0.8183, AUC: 0.5358]\n",
      "Model 2 Evaluation: [Epoch 10, Acc: 0.8179, AUC: 0.5940]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.36batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 169.36batches/s]\n",
      "Model 1 Training:   [Epoch 11, Loss: 0.000869, Acc: 0.8183, AUC: 0.5363]\n",
      "Model 1 Evaluation: [Epoch 11, Acc: 0.8179, AUC: 0.5942]\n",
      "Model 2 Training:   [Epoch 11, Loss: 0.000869, Acc: 0.8183, AUC: 0.5364]\n",
      "Model 2 Evaluation: [Epoch 11, Acc: 0.8179, AUC: 0.5987]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.48batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 167.65batches/s]\n",
      "Model 1 Training:   [Epoch 12, Loss: 0.000864, Acc: 0.8183, AUC: 0.5382]\n",
      "Model 1 Evaluation: [Epoch 12, Acc: 0.8179, AUC: 0.5962]\n",
      "Model 2 Training:   [Epoch 12, Loss: 0.000864, Acc: 0.8183, AUC: 0.5383]\n",
      "Model 2 Evaluation: [Epoch 12, Acc: 0.8179, AUC: 0.6027]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 56.96batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 170.83batches/s]\n",
      "Model 1 Training:   [Epoch 13, Loss: 0.000865, Acc: 0.8183, AUC: 0.5403]\n",
      "Model 1 Evaluation: [Epoch 13, Acc: 0.8179, AUC: 0.6123]\n",
      "Model 2 Training:   [Epoch 13, Loss: 0.000865, Acc: 0.8183, AUC: 0.5403]\n",
      "Model 2 Evaluation: [Epoch 13, Acc: 0.8179, AUC: 0.6105]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.03batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 170.03batches/s]\n",
      "Model 1 Training:   [Epoch 14, Loss: 0.000862, Acc: 0.8183, AUC: 0.5414]\n",
      "Model 1 Evaluation: [Epoch 14, Acc: 0.8180, AUC: 0.6146]\n",
      "Model 2 Training:   [Epoch 14, Loss: 0.000862, Acc: 0.8183, AUC: 0.5416]\n",
      "Model 2 Evaluation: [Epoch 14, Acc: 0.8180, AUC: 0.6153]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [00:39<00:00, 57.36batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:01<00:00, 167.42batches/s]\n",
      "Model 1 Training:   [Epoch 15, Loss: 0.000861, Acc: 0.8183, AUC: 0.5446]\n",
      "Model 1 Evaluation: [Epoch 15, Acc: 0.8183, AUC: 0.6160]\n",
      "Model 2 Training:   [Epoch 15, Loss: 0.000861, Acc: 0.8183, AUC: 0.5447]\n",
      "Model 2 Evaluation: [Epoch 15, Acc: 0.8183, AUC: 0.6171]\n",
      "saved model2\n",
      "Finished Training.\n",
      "Training took 10.466051693757375 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss1, train_loss2, train_acc1, train_acc2, train_auc1, train_auc2 = train(\n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "        optim=optimizer,\n",
    "        loss_func=criterion, # JoCoRLoss\n",
    "        loader=train_loader,\n",
    "        forget_rate=rate_schedule[epoch-1],\n",
    "        BCEWLL=BCEWLL, # nn.BCEWithLogitsLoss with reduction=none\n",
    "        kl_loss=kl_loss_compute\n",
    "    )\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_acc1, val_acc2, val_auc1, val_auc2 = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loader=dev_loader,\n",
    "        )\n",
    "    \n",
    "    print(\n",
    "        f\"Model 1 Training:   [Epoch {epoch:2d}, Loss: {train_loss1:8.6f}, Acc: {train_acc1:.4f}, AUC: {train_auc1:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 1 Evaluation: [Epoch {epoch:2d}, Acc: {val_acc1:.4f}, AUC: {val_auc1:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 2 Training:   [Epoch {epoch:2d}, Loss: {train_loss2:8.6f}, Acc: {train_acc2:.4f}, AUC: {train_auc2:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 2 Evaluation: [Epoch {epoch:2d}, Acc: {val_acc2:.4f}, AUC: {val_auc2:.4f}]\"\n",
    "    )\n",
    "\n",
    "    if val_auc1 > val_auc2:\n",
    "        reg_auc = regularized_auc(train_auc1, val_auc1, threshold=0)\n",
    "        model = model1.state_dict()\n",
    "        print('saved model1')\n",
    "    else:\n",
    "        reg_auc = regularized_auc(train_auc2, val_auc2, threshold=0)\n",
    "        model = model2.state_dict()\n",
    "        print('saved model2')\n",
    "    if reg_auc > highest_val_auc:\n",
    "        highest_val_auc = reg_auc\n",
    "        path = f\"../../params/n_term/BiLSTM_prot2vec_jocor/auc{reg_auc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model, path)\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.6171_epoch15.pt\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:00<00:00, 284.66batches/s]\n",
      "Test Set Performance: Acc: 0.8191, AUC: 0.6162\n",
      "Total model params: 16772049, trainable model params: 6593049\n"
     ]
    }
   ],
   "source": [
    "test_path = '../../data/n_test_3mer.tsv'\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/BiLSTM_prot2vec_jocor/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model1.load_state_dict(torch.load('../../params/n_term/BiLSTM_prot2vec_jocor/' + best_model))\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    test_acc, test_auc = test(model1, test_loader)\n",
    "print(\n",
    "    f\"Test Set Performance: Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model1)}, trainable model params: {trainable_model_params(model1)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15bfe2-6557-42a6-81c1-a6c2960923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
