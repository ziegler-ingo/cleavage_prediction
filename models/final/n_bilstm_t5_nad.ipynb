{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70f8adf-c8ea-4bee-9a17-b7f0f59f3d44",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* BiLSTM model architecture based on [Ozols et. al., 2021](https://www.mdpi.com/1422-0067/22/6/3071/htm)\n",
    "* T5 Encoder taken from [Elnagger et al., 2020](https://ieeexplore.ieee.org/document/9477085), [Github](https://github.com/agemagician/ProtTrans), Model on [Huggingface Hub](https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc)\n",
    "* Noise adaptation layer implementation is based on [Goldberger and Ben-Reuven, 2017](https://openreview.net/references/pdf?id=Sk5qglwSl), and unofficial implementation on [Github](https://github.com/Billy1900/Noise-Adaption-Layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "def apply_random_masking(seq, att, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 0 at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m), att * (dist < m)\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def trainable_model_params(model):\n",
    "    return sum(\n",
    "        p[1].numel()\n",
    "        for p in model.named_parameters()\n",
    "        if p[1].requires_grad and not p[0].startswith(\"t5\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "    \n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            [seq.replace(\"\", \" \").strip() for seq in ordered_batch[0]]\n",
    "        )\n",
    "        seq = torch.tensor(encoded[\"input_ids\"], dtype=torch.int64)\n",
    "        att = torch.tensor(encoded[\"attention_mask\"], dtype=torch.int64)\n",
    "        self.seq, self.att = apply_random_masking(seq, att, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.long)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.att = self.att.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "    \n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            [seq.replace(\"\", \" \").strip() for seq in ordered_batch[0]]\n",
    "        )\n",
    "        self.seq = torch.tensor(encoded[\"input_ids\"], dtype=torch.int64)\n",
    "        self.att = torch.tensor(encoded[\"attention_mask\"], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.long)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.att = self.att.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5_BiLSTM(nn.Module):\n",
    "    def __init__(self, rnn_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.t5_encoder = T5EncoderModel.from_pretrained(\n",
    "            \"Rostlab/prot_t5_xl_half_uniref50-enc\", torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.t5_encoder.config.to_dict()['d_model'], # 1024\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(rnn_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, seq, att):\n",
    "        with torch.no_grad():\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            # out: (batch_size, seq_len+1, embedding_dim=1024)\n",
    "            embedded = self.dropout(self.t5_encoder(seq, att).last_hidden_state)\n",
    "\n",
    "        # input shape: (batch_size, seq_len+1, embedding_dim)\n",
    "        out, _ = self.lstm(embedded)\n",
    "\n",
    "        # input shape: (batch_size, seq_len=1, 2*rnn_size)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "\n",
    "        # input shape: (batch_size, 2*rnn_size)\n",
    "        out = self.dropout(gelu(self.fc1(pooled)))\n",
    "\n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size, 2)\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72cabe9-f647-47ba-96ed-37c52355b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseAdaptation(nn.Module):\n",
    "    def __init__(self, theta, k):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Linear(k, k, bias=False)\n",
    "        self.theta.weight.data = theta\n",
    "        self.eye = torch.eye(k).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        theta = self.theta(self.eye)\n",
    "        theta = torch.softmax(theta, dim=0)\n",
    "        out = x @ theta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None, conf=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    pos_preds, preds, lbls = [], [], []\n",
    "    \n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        seq, att, lbl = batch.seq, batch.att, batch.lbl\n",
    "        seq, att, lbl = seq.to(device), att.to(device), lbl.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            scores = model(seq, att)\n",
    "            loss = criterion(scores, lbl)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "        \n",
    "        pred = scores.argmax(dim=1)\n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += (pred == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(pred.detach().tolist())\n",
    "        pos_preds.extend(scores[:, 1].detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    if conf is not None:   \n",
    "        return confusion_matrix(lbls, preds)\n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, pos_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5534faf-85ed-4db7-9c09-944e36f6e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid(\n",
    "    model, noisemodel, optimizer, noise_optimizer, criterion, beta, loader\n",
    "):\n",
    "    epoch_loss, model_loss, noise_loss, num_correct, total = 0, 0, 0, 0, 0\n",
    "    pos_preds, preds, lbls = [], [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Hybrid Train: \", file=sys.stdout, unit=\"batches\"):\n",
    "\n",
    "        seq, att, lbl = batch.seq, batch.att, batch.lbl\n",
    "        seq, att, lbl = seq.to(device), att.to(device), lbl.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            scores = model(seq, att)\n",
    "            noise_scores = noisemodel(scores)\n",
    "\n",
    "            model_loss = criterion(scores, lbl)\n",
    "            noise_loss = criterion(noise_scores, lbl)\n",
    "\n",
    "            loss = beta * noise_loss + (1 - beta) * model_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        noise_optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.step(noise_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        model_loss += model_loss.item()\n",
    "        noise_loss += noise_loss.item()\n",
    "        num_correct += (noise_scores.argmax(dim=1)  == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(noise_scores[:, 1].detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss / total,\n",
    "        model_loss / total,\n",
    "        noise_loss / total,\n",
    "        num_correct / total,\n",
    "        roc_auc_score(lbls, preds),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/n_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/n_val.csv')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    \"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "NUM_WARMUP = 1\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 512\n",
    "BETA=0.8\n",
    "RNN_SIZE = 512\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "\n",
    "model = T5_BiLSTM(\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=train_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|████████████████████████████████████████████████████| 2243/2243 [11:38<00:00,  3.21batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:21<00:00,  3.43batches/s]\n",
      "Warmup Training:   [Epoch  1, Loss: 0.000839, Acc: 0.8214, AUC: 0.7043]\n",
      "Warmup Evaluation: [Epoch  1, Loss: 0.000784, Acc: 0.8287, AUC: 0.7713]\n",
      "Eval: 100%|█████████████████████████████████████████████████████| 2243/2243 [10:57<00:00,  3.41batches/s]\n",
      "Created NoiseModel in epoch 1\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:44<00:00,  3.18batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.41batches/s]\n",
      "Hy-Training: [Epoch  2, Hy-Loss: 0.000901,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8248, AUC: 0.2826]\n",
      "Evaluation:  [Epoch  2, Loss: 0.000909, Acc: 0.8316, AUC: 0.7796]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:43<00:00,  3.19batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.41batches/s]\n",
      "Hy-Training: [Epoch  3, Hy-Loss: 0.000823,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8264, AUC: 0.4945]\n",
      "Evaluation:  [Epoch  3, Loss: 0.000801, Acc: 0.8318, AUC: 0.7843]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:44<00:00,  3.18batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.42batches/s]\n",
      "Hy-Training: [Epoch  4, Hy-Loss: 0.000799,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8275, AUC: 0.7203]\n",
      "Evaluation:  [Epoch  4, Loss: 0.000781, Acc: 0.8333, AUC: 0.7872]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:36<00:00,  3.22batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:20<00:00,  3.51batches/s]\n",
      "Hy-Training: [Epoch  5, Hy-Loss: 0.000790,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8281, AUC: 0.7512]\n",
      "Evaluation:  [Epoch  5, Loss: 0.000770, Acc: 0.8333, AUC: 0.7893]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:23<00:00,  3.28batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:19<00:00,  3.51batches/s]\n",
      "Hy-Training: [Epoch  6, Hy-Loss: 0.000785,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8286, AUC: 0.7548]\n",
      "Evaluation:  [Epoch  6, Loss: 0.000764, Acc: 0.8344, AUC: 0.7905]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:23<00:00,  3.28batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:20<00:00,  3.51batches/s]\n",
      "Hy-Training: [Epoch  7, Hy-Loss: 0.000781,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8293, AUC: 0.7609]\n",
      "Evaluation:  [Epoch  7, Loss: 0.000759, Acc: 0.8342, AUC: 0.7913]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:43<00:00,  3.19batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.41batches/s]\n",
      "Hy-Training: [Epoch  8, Hy-Loss: 0.000779,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8295, AUC: 0.7633]\n",
      "Evaluation:  [Epoch  8, Loss: 0.000759, Acc: 0.8343, AUC: 0.7921]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:46<00:00,  3.18batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.39batches/s]\n",
      "Hy-Training: [Epoch  9, Hy-Loss: 0.000777,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8302, AUC: 0.7653]\n",
      "Evaluation:  [Epoch  9, Loss: 0.000757, Acc: 0.8345, AUC: 0.7926]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2243/2243 [11:44<00:00,  3.18batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.41batches/s]\n",
      "Hy-Training: [Epoch 10, Hy-Loss: 0.000776,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8304, AUC: 0.7653]\n",
      "Evaluation:  [Epoch 10, Loss: 0.000755, Acc: 0.8354, AUC: 0.7927]\n",
      "Finished Training.\n",
      "Training took 142.3155985514323 minutes.\n"
     ]
    }
   ],
   "source": [
    "# scale everything to fp16\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if epoch < NUM_WARMUP + 1:\n",
    "        model.train()\n",
    "        train_loss, train_acc, train_auc = process(model, train_loader, criterion, optimizer)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "            \n",
    "        print(\n",
    "        f\"Warmup Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Warmup Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        if epoch == NUM_WARMUP:\n",
    "            # get conf matrix based on predictions on train data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                conf = process(model, train_loader, criterion, conf=True)\n",
    "            theta = conf / conf.sum(axis=1, keepdims=True)\n",
    "            theta = torch.from_numpy(np.log(theta + 1e-8)).to(torch.float) # avoid zeros with +1e-8\n",
    "\n",
    "            # create noisemodel\n",
    "            noisemodel = NoiseAdaptation(theta=theta, k=NUM_CLASSES).to(device)\n",
    "            noise_optimizer = optim.Adam(noisemodel.parameters(), lr=LEARNING_RATE)\n",
    "            print(f'Created NoiseModel in epoch {epoch}')\n",
    "\n",
    "    else:\n",
    "        # hybrid training\n",
    "        model.train()\n",
    "        noisemodel.train()\n",
    "        hy_tr_loss, model_loss, noise_loss, hy_tr_acc, hy_tr_auc = train_hybrid(\n",
    "            model=model,\n",
    "            noisemodel=noisemodel,\n",
    "            optimizer=optimizer,\n",
    "            noise_optimizer=noise_optimizer,\n",
    "            criterion=criterion,\n",
    "            beta=BETA,\n",
    "            loader=train_loader\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "\n",
    "        print(\n",
    "            f\"Hy-Training: [Epoch {epoch:2d}, Hy-Loss: {hy_tr_loss:.6f},\\\n",
    "            Model-Loss: {model_loss:.6f}, Noise-Loss: {noise_loss:.6f},\\\n",
    "            Acc: {hy_tr_acc:.4f}, AUC: {hy_tr_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Evaluation:  [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        reg_auc = regularized_auc(hy_tr_auc, val_auc, threshold=0)\n",
    "        if reg_auc > highest_val_auc:\n",
    "            highest_val_auc = reg_auc\n",
    "            path = f\"../../params/n_term/T5_BiLSTM_noise_layer/auc{reg_auc:.4f}_epoch{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted model\n",
      "Loaded model:  auc0.7927_epoch10.pt\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 281/281 [01:22<00:00,  3.42batches/s]\n",
      "Test Set Performance: Loss: 0.000751, Acc: 0.8348, AUC: 0.7954\n",
      "Total model params: 1214572930, trainable model params: 6431106\n"
     ]
    }
   ],
   "source": [
    "if 'model' in globals():\n",
    "    del model\n",
    "    print('deleted model')\n",
    "    \n",
    "model = T5_BiLSTM(\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to('cpu')\n",
    "\n",
    "test_path = '../../data/n_test.csv'\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/T5_BiLSTM_noise_layer/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model.load_state_dict(torch.load('../../params/n_term/T5_BiLSTM_noise_layer/' + best_model))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc, test_auc = process(model, test_loader, criterion)\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15bfe2-6557-42a6-81c1-a6c2960923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
