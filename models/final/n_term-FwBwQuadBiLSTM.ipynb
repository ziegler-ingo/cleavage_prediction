{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3670e-2187-42ce-8fda-25a8ba1c80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: use ProtTrans pipeline as feature extraction (instead of embeddings) and build model on top of it\n",
    "# check whether input format makes sense\n",
    "# https://github.com/agemagician/ProtTrans/blob/master/Embedding/PyTorch/Basic/ProtBert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_data(path):\n",
    "#     with open(path, 'r') as csvfile:\n",
    "#         train_data = list(csv.reader(csvfile))[1:] # skip col name\n",
    "#         sents, lbls = [], []\n",
    "#         for i in range(0, len(train_data), 16):\n",
    "#             s, l = zip(*train_data[i:i+16])\n",
    "#             sents.append(s)\n",
    "#             lbls.append(l)\n",
    "#     return sents, lbls\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path, 'r') as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:] # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "# number of trainable parameters in model\n",
    "def get_total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "class CleavageBatch:\n",
    "    def __init__(self, batch: List[Tuple[str, str]]):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.bw_seq = torch.tensor([encode_text(seq)[::-1] for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.fw_seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.bw_seq = self.bw_seq.pin_memory()\n",
    "        self.fw_seq = self.fw_seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def collate_wrapper(batch):\n",
    "    return CleavageBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fw_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        self.bw_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fw_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, hidden_size=rnn_size, batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.bw_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, hidden_size=rnn_size, batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, bw_seq, fw_seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        fw_embeddings = self.dropout(self.fw_embedding(fw_seq))\n",
    "        bw_embeddings = self.dropout(self.bw_embedding(bw_seq))\n",
    "    \n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        fw_out, _ = self.fw_lstm(fw_embeddings)\n",
    "        bw_out, _ = self.bw_lstm(bw_embeddings)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, rnn_size)\n",
    "        # only get representation at last t\n",
    "        fw_out = self.dropout(fw_out[:, -1, :])\n",
    "        bw_out = self.dropout(bw_out[:, -1, :])\n",
    "        \n",
    "        # input shape: (batch_size, rnn_size)\n",
    "        # out shape: (batch_size, 2*rnn_size)\n",
    "        return torch.cat([fw_out, bw_out], dim=1) \n",
    "        \n",
    "    \n",
    "\n",
    "class FwBwQuadBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_enc_emb_dim, seq_enc_rnn_size, rnn_size1, rnn_size2, rnn_size3, rnn_size4, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # sequence encoder replaces embedding representations\n",
    "        self.seq_encoder = SeqEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=seq_enc_emb_dim,\n",
    "            rnn_size=seq_enc_rnn_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=seq_enc_rnn_size * 2,\n",
    "            hidden_size=rnn_size1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2*rnn_size1,\n",
    "            hidden_size=rnn_size2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=2*rnn_size2,\n",
    "            hidden_size=rnn_size3,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=2*rnn_size3,\n",
    "            hidden_size=rnn_size4,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size4 * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, bw_seq, fw_seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.dropout(self.seq_encoder(bw_seq, fw_seq))\n",
    "        \n",
    "        # input shape: (batch_size, seq_enc_rnn_size * 2)\n",
    "        out, _ = self.lstm1(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, 2*rnn_size)\n",
    "        out, _ = self.lstm2(out)\n",
    "        \n",
    "        # input shape: (batch_size, 2*rnn_size)\n",
    "        out, _ = self.lstm3(out)\n",
    "        \n",
    "        # input shape: (batch_size, 2*rnn_size)\n",
    "        out, _ = self.lstm4(out)\n",
    "        \n",
    "        # input shape; (batch_size, 2*rnn_size)\n",
    "        out = self.dropout(F.relu(self.fc1(out)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size)\n",
    "        return self.fc2(out).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        bw_seq, fw_seq, lbl = batch.bw_seq, batch.fw_seq, batch.lbl\n",
    "        bw_seq, fw_seq, lbl = bw_seq.to(device), fw_seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(bw_seq, fw_seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += ((scores > 0) == lbl).sum()\n",
    "        total += len(lbl)\n",
    "    return epoch_loss / total, num_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/n_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/n_val.csv')\n",
    "\n",
    "# create vocab from train seqs\n",
    "vocab = build_vocab_from_iterator(train_seqs, specials=['<UNK>'])\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "encode_text = lambda x: vocab(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "VOCAB_SIZE = len(vocab)\n",
    "SEQ_ENC_EMB_DIM = 100\n",
    "SEQ_ENC_RNN_SIZE = 200\n",
    "RNN_SIZE1 = 128\n",
    "RNN_SIZE2 = 512\n",
    "RNN_SIZE3 = 256\n",
    "RNN_SIZE4 = 128\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = FwBwQuadBiLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    seq_enc_emb_dim=SEQ_ENC_EMB_DIM,\n",
    "    seq_enc_rnn_size=SEQ_ENC_RNN_SIZE,\n",
    "    rnn_size1=RNN_SIZE1,\n",
    "    rnn_size2=RNN_SIZE2,\n",
    "    rnn_size3=RNN_SIZE3,\n",
    "    rnn_size4=RNN_SIZE4,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "print(f\"Total trainable model parameters: {get_total_model_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_acc = 0\n",
    "train_losses, train_accuracies= [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss, train_acc = process(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process(model, dev_loader, criterion)\n",
    "        \n",
    "    # save current acc, loss\n",
    "    train_losses.append((epoch, train_loss))\n",
    "    train_accuracies.append((epoch, train_acc))\n",
    "    val_losses.append((epoch, val_loss))\n",
    "    val_accuracies.append((epoch, val_acc))\n",
    "    \n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        path = f\"../../params/n_term/quadBiLSTM/fwbw_acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        \n",
    "    print(\n",
    "        f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.4f}, Acc: {train_acc:.4f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.4f}, Acc: {val_acc:.4f}]\")\n",
    "    \n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training stats\n",
    "lsts = [train_losses, train_accuracies, val_losses, val_accuracies, train_time]\n",
    "names = [\n",
    "    \"train_losses\",\n",
    "    \"train_accuracies\",\n",
    "    \"val_losses\",\n",
    "    \"val_accuracies\",\n",
    "    \"train_time\",\n",
    "]\n",
    "to_save = dict()\n",
    "for name, lst in zip(names, lsts):\n",
    "    to_save[name] = lst\n",
    "\n",
    "with open(f\"../params/n_term/quadBiLSTM/metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(to_save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Finished Saving Details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
