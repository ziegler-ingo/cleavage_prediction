{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2c7a34-7558-4e52-9e81-5fc93af6eab5",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* ESM2 taken from [Lin et al., 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1), [Github](https://github.com/facebookresearch/esm)\n",
    "* Co-teaching+ loss function and training process adaptations are based on [Yu et al., 2019](https://arxiv.org/abs/1901.04215), and official implementation on [Github](https://github.com/xingruiyu/coteaching_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 3 (i.e. <unk>) at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m) + (dist == m) * 3\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "def total_model_params(model):\n",
    "    sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lbl[idx], self.seq[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "\n",
    "\n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        lbls, _, seq = tokenizer(batch)\n",
    "        self.seq = apply_random_masking(seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in lbls], dtype=torch.float)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "\n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        lbls, _, self.seq = tokenizer(batch)\n",
    "        self.lbl = torch.tensor([int(l) for l in lbls], dtype=torch.float)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2Finetune(nn.Module):\n",
    "    def __init__(self, pretrained_model, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm2 = pretrained_model\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(33, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # input shape: (batch_size, seq_len=10+2 (cls, eos))\n",
    "        result = self.esm2(seq)['logits'][:, 1:10+1, :] # remove cls, eos token position\n",
    "        result = self.dropout(result)\n",
    "        \n",
    "        # in: (batch_size, seq_len, vocab_size=33)\n",
    "        result, _ = result.max(dim=1)\n",
    "\n",
    "        # input shape: (batch_size, 33)\n",
    "        # out shape: (batch_size)\n",
    "        return self.fc(result).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a6aee8-1817-4ca0-a033-b4a09f40f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_coteaching(y_1, y_2, t, forget_rate):\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    loss_1 = criterion(y_1, t)\n",
    "    ind_1_sorted = np.argsort(loss_1.data.cpu())\n",
    "    loss_1_sorted = loss_1[ind_1_sorted]\n",
    "\n",
    "    loss_2 = criterion(y_2, t)\n",
    "    ind_2_sorted = np.argsort(loss_2.data.cpu())\n",
    "    loss_2_sorted = loss_2[ind_2_sorted]\n",
    "\n",
    "    remember_rate = 1 - forget_rate\n",
    "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
    "\n",
    "    ind_1_update = ind_1_sorted[:num_remember]\n",
    "    ind_2_update = ind_2_sorted[:num_remember]\n",
    "    \n",
    "    # exchange\n",
    "    loss_1_update = criterion(y_1[ind_2_update], t[ind_2_update])\n",
    "    loss_2_update = criterion(y_2[ind_1_update], t[ind_1_update])\n",
    "\n",
    "    return torch.sum(loss_1_update)/num_remember, torch.sum(loss_2_update)/num_remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3e56b0-7148-431d-b353-021e86c7e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model1, optim1, model2, optim2, loader, forget_rate):\n",
    "    epoch_loss1, num_correct1, total = 0, 0, 0\n",
    "    epoch_loss2, num_correct2 = 0, 0\n",
    "    preds1, preds2, lbls = [], [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Train: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "\n",
    "        scores1 = model1(seq)\n",
    "        pred1 = scores1 > 0\n",
    "        scores2 = model2(seq)\n",
    "        pred2 = scores2 > 0\n",
    "\n",
    "        inds = torch.where(pred1 != pred2)\n",
    "        if len(inds[0]) * (1 - forget_rate) < 1:\n",
    "            loss1 = criterion(scores1, lbl)\n",
    "            loss2 = criterion(scores2, lbl)\n",
    "        else:\n",
    "            loss1, loss2 = loss_coteaching(scores1, scores2, lbl, forget_rate)\n",
    "\n",
    "        optim1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optim1.step()\n",
    "        \n",
    "        optim2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optim2.step()\n",
    "\n",
    "        epoch_loss1 += loss1.item()\n",
    "        epoch_loss2 += loss2.item()\n",
    "        num_correct1 += (pred1 == lbl).sum().item()\n",
    "        num_correct2 += (pred2 == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds1.extend(scores1.detach().tolist())\n",
    "        preds2.extend(scores2.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss1 / total,\n",
    "        epoch_loss2 / total,\n",
    "        num_correct1 / total,\n",
    "        num_correct2 / total,\n",
    "        roc_auc_score(lbls, preds1),\n",
    "        roc_auc_score(lbls, preds2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb135684-fa8e-4430-82af-784397658cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model1, model2, loader, criterion):\n",
    "    epoch_loss1, num_correct1, total = 0, 0, 0\n",
    "    epoch_loss2, num_correct2 = 0, 0\n",
    "    preds1, preds2, lbls = [], [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores1 = model1(seq)\n",
    "        scores2 = model2(seq)\n",
    "\n",
    "        loss1 = criterion(scores1, lbl)\n",
    "        loss2 = criterion(scores2, lbl)\n",
    "        \n",
    "        epoch_loss1 += loss1.item()\n",
    "        epoch_loss2 += loss2.item()\n",
    "        num_correct1 += ((scores1 > 0) == lbl).sum().item()\n",
    "        num_correct2 += ((scores2 > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds1.extend(scores1.detach().tolist())\n",
    "        preds2.extend(scores2.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss1 / total,\n",
    "        epoch_loss2 / total,\n",
    "        num_correct1 / total,\n",
    "        num_correct2 / total,\n",
    "        roc_auc_score(lbls, preds1),\n",
    "        roc_auc_score(lbls, preds2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5cfea1-9b4e-4e09-b882-e7e05792934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, criterion):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    preds, lbls = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += ((scores > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(scores.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ingo/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/n_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/n_val.csv')\n",
    "\n",
    "# load pre-trained esm2 model and vocab\n",
    "esm2, vocab = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "esm2c = copy.deepcopy(esm2) # needed when training two models, otherwise unexpected behavior\n",
    "tokenizer = vocab.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 256\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "NUM_GRADUAL = 1 # how many epochs for linear drop rate\n",
    "NOISY_RATE = 0.2\n",
    "FORGET_RATE = 0.2\n",
    "EXPONENT = 1\n",
    "\n",
    "# define drop rate schedule\n",
    "rate_schedule = np.ones(NUM_EPOCHS)*FORGET_RATE\n",
    "rate_schedule[:NUM_GRADUAL] = np.linspace(0, FORGET_RATE**EXPONENT, NUM_GRADUAL)\n",
    "\n",
    "model1 = ESM2Finetune(\n",
    "    pretrained_model=esm2,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "model2 = ESM2Finetune(\n",
    "    pretrained_model=esm2c, # deepcopy esm2\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer1 = optim.AdamW(model1.parameters(), lr=LEARNING_RATE)\n",
    "optimizer2 = optim.AdamW(model2.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=train_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|████████████████████████████████████████████████████| 4485/4485 [34:29<00:00,  2.17batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 561/561 [01:16<00:00,  7.30batches/s]\n",
      "Model 1 Training:   [Epoch  1, Loss: 0.001601, Acc: 0.8254, AUC: 0.7547]\n",
      "Model 1 Evaluation: [Epoch  1, Loss: 0.002005, Acc: 0.8034, AUC: 0.7825]\n",
      "Model 2 Training:   [Epoch  1, Loss: 0.001606, Acc: 0.8247, AUC: 0.7539]\n",
      "Model 2 Evaluation: [Epoch  1, Loss: 0.001839, Acc: 0.8207, AUC: 0.7824]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 4485/4485 [34:35<00:00,  2.16batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 561/561 [01:20<00:00,  6.99batches/s]\n",
      "Model 1 Training:   [Epoch  2, Loss: 0.000545, Acc: 0.8290, AUC: 0.7517]\n",
      "Model 1 Evaluation: [Epoch  2, Loss: 0.001673, Acc: 0.8322, AUC: 0.7741]\n",
      "Model 2 Training:   [Epoch  2, Loss: 0.000545, Acc: 0.8289, AUC: 0.7519]\n",
      "Model 2 Evaluation: [Epoch  2, Loss: 0.001604, Acc: 0.8331, AUC: 0.7793]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 4485/4485 [35:01<00:00,  2.13batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 561/561 [01:17<00:00,  7.28batches/s]\n",
      "Model 1 Training:   [Epoch  3, Loss: 0.000617, Acc: 0.8307, AUC: 0.7650]\n",
      "Model 1 Evaluation: [Epoch  3, Loss: 0.001671, Acc: 0.8313, AUC: 0.7769]\n",
      "Model 2 Training:   [Epoch  3, Loss: 0.000616, Acc: 0.8308, AUC: 0.7657]\n",
      "Model 2 Evaluation: [Epoch  3, Loss: 0.001627, Acc: 0.8329, AUC: 0.7816]\n",
      "saved model2\n",
      "Finished Training.\n",
      "Training took 108.08450101613998 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss1, train_loss2, train_acc1, train_acc2, train_auc1, train_auc2 = train(\n",
    "        model1=model1,\n",
    "        optim1=optimizer1,\n",
    "        model2=model2,\n",
    "        optim2=optimizer2,\n",
    "        loader=train_loader,\n",
    "        forget_rate=rate_schedule[epoch-1],\n",
    "    )\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss1, val_loss2, val_acc1, val_acc2, val_auc1, val_auc2 = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loader=dev_loader,\n",
    "            criterion=criterion\n",
    "        )\n",
    "    \n",
    "    print(\n",
    "        f\"Model 1 Training:   [Epoch {epoch:2d}, Loss: {train_loss1:8.6f}, Acc: {train_acc1:.4f}, AUC: {train_auc1:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 1 Evaluation: [Epoch {epoch:2d}, Loss: {val_loss1:8.6f}, Acc: {val_acc1:.4f}, AUC: {val_auc1:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 2 Training:   [Epoch {epoch:2d}, Loss: {train_loss2:8.6f}, Acc: {train_acc2:.4f}, AUC: {train_auc2:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 2 Evaluation: [Epoch {epoch:2d}, Loss: {val_loss2:8.6f}, Acc: {val_acc2:.4f}, AUC: {val_auc2:.4f}]\"\n",
    "    )\n",
    "\n",
    "    if val_auc1 > val_auc2:\n",
    "        reg_auc = regularized_auc(train_auc1, val_auc1, threshold=0)\n",
    "        model = model1.state_dict()\n",
    "        print('saved model1')\n",
    "    else:\n",
    "        reg_auc = regularized_auc(train_auc2, val_auc2, threshold=0)\n",
    "        model = model2.state_dict()\n",
    "        print('saved model2')\n",
    "    if reg_auc > highest_val_auc:\n",
    "        highest_val_auc = reg_auc\n",
    "        path = f\"../../params/n_term/esm2finetune_coteaching_plus/auc{reg_auc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model, path)\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted model1\n",
      "deleted model2\n",
      "deleted esm2\n",
      "deleted esm2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ingo/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.7825_epoch1.pt\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 561/561 [00:39<00:00, 14.34batches/s]\n",
      "Test Set Performance: Loss: 0.002008, Acc: 0.8054, AUC: 0.7853\n",
      "Total model params: None, trainable model params: 148140188\n"
     ]
    }
   ],
   "source": [
    "if 'model1' in globals():\n",
    "    del model1\n",
    "    print('deleted model1')\n",
    "if 'model2' in globals():\n",
    "    del model2\n",
    "    print('deleted model2')\n",
    "if 'esm2' in globals():\n",
    "    del esm2\n",
    "    print('deleted esm2')\n",
    "if 'esm2c' in globals():\n",
    "    del esm2c\n",
    "    print('deleted esm2c')\n",
    "    \n",
    "esm2, _ = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "model = ESM2Finetune(\n",
    "    pretrained_model=esm2,\n",
    "    dropout=DROPOUT\n",
    ").to('cpu')\n",
    "\n",
    "test_path = '../../data/n_test.csv'\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/esm2finetune_coteaching_plus/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model.load_state_dict(torch.load('../../params/n_term/esm2finetune_coteaching_plus/' + best_model))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc, test_auc = test(model, test_loader, criterion)\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15bfe2-6557-42a6-81c1-a6c2960923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
