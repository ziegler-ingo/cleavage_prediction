{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b3d106-8ca5-434e-926e-d0050fcd1aaa",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* BiLSTM model architecture based on [Ozols et. al., 2021](https://www.mdpi.com/1422-0067/22/6/3071/htm)\n",
    "* ESM2 taken from [Lin et al., 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1), [Github](https://github.com/facebookresearch/esm)\n",
    "* JoCoR loss function and training process adaptations are based on [Wei et al., 2020](https://openaccess.thecvf.com/content_CVPR_2020/html/Wei_Combating_Noisy_Labels_by_Agreement_A_Joint_Training_Method_with_CVPR_2020_paper.html), and official implementation on [Github](https://github.com/hongxin001/JoCoR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 3 (i.e. <unk>) at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m) + (dist == m) * 3\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "def trainable_model_params(model):\n",
    "    return sum(\n",
    "        p[1].numel()\n",
    "        for p in model.named_parameters()\n",
    "        if p[1].requires_grad and not p[0].startswith(\"esm\")\n",
    "    )\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lbl[idx], self.seq[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "\n",
    "\n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        lbls, _, seq = tokenizer(batch)\n",
    "        self.seq = apply_random_masking(seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in lbls], dtype=torch.float)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "\n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        lbls, _, self.seq = tokenizer(batch)\n",
    "        self.lbl = torch.tensor([int(l) for l in lbls], dtype=torch.float)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2BiLSTM(nn.Module):\n",
    "    def __init__(self, esm2, rnn_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm2_encoder = esm2\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=640,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(rnn_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        with torch.no_grad():\n",
    "            # input shape: (batch_size, seq_len=10+2(cls, eos))\n",
    "            # out: (batch_size, seq_len, embedding_dim=1280)\n",
    "            result = self.esm2_encoder(seq, repr_layers=[30])\n",
    "        \n",
    "        embedded = self.dropout(result['representations'][30][:, 1:10+1, :])\n",
    "\n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm(embedded)\n",
    "\n",
    "        # input shape: (batch_size, seq_len=1, 2*rnn_size)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "\n",
    "        # input shape: (batch_size, 2*rnn_size)\n",
    "        out = self.dropout(gelu(self.fc1(pooled)))\n",
    "\n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size)\n",
    "        return self.fc2(out).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913fb459-edec-4e84-9741-9cf96da4bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss_compute(pred, soft_targets):\n",
    "    # adjusted for binary case\n",
    "    kl = F.kl_div(F.logsigmoid(pred), torch.sigmoid(soft_targets), reduction='none')\n",
    "    return torch.sum(kl)\n",
    "\n",
    "\n",
    "class JoCoRLoss:\n",
    "    def __call__(self, y1, y2, lbls, forget_rate, loss_func, kl_loss, co_lambda=0.1):\n",
    "        loss_pick_1 = loss_func(y1, lbls) * (1 - co_lambda)\n",
    "        loss_pick_2 = loss_func(y2, lbls) * (1 - co_lambda)\n",
    "        loss_pick = (\n",
    "            loss_pick_1\n",
    "            + loss_pick_2\n",
    "            + co_lambda * kl_loss_compute(y1, y2)\n",
    "            + co_lambda * kl_loss_compute(y2, y1)\n",
    "        ).cpu()\n",
    "\n",
    "        ind_sorted = np.argsort(loss_pick.data)\n",
    "        loss_sorted = loss_pick[ind_sorted]\n",
    "\n",
    "        remember_rate = 1 - forget_rate\n",
    "        num_remember = int(remember_rate * len(loss_sorted))\n",
    "\n",
    "        ind_update = ind_sorted[:num_remember]\n",
    "\n",
    "        loss = torch.mean(loss_pick[ind_update])\n",
    "\n",
    "        return loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf60c752-391d-453a-9d9d-0fad3c35d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model1, model2, optim, loss_func, loader, forget_rate, BCEWLL, kl_loss):\n",
    "    epoch_loss1, num_correct1, total = 0, 0, 0\n",
    "    epoch_loss2, num_correct2 = 0, 0\n",
    "    preds1, preds2, lbls = [], [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores1 = model1(seq)\n",
    "        scores2 = model2(seq)\n",
    "\n",
    "        # JoCoR loss\n",
    "        loss1, loss2 = loss_func(scores1, scores2, lbl, forget_rate, BCEWLL, kl_loss)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss1.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss1 += loss1.item()\n",
    "        epoch_loss2 += loss2.item()\n",
    "        num_correct1 += ((scores1 > 0) == lbl).sum().item()\n",
    "        num_correct2 += ((scores2 > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds1.extend(scores1.detach().tolist())\n",
    "        preds2.extend(scores2.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss1 / total,\n",
    "        epoch_loss2 / total,\n",
    "        num_correct1 / total,\n",
    "        num_correct2 / total,\n",
    "        roc_auc_score(lbls, preds1),\n",
    "        roc_auc_score(lbls, preds2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed5977bd-4d82-42ef-8a6d-1423dfd3e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model1, model2, loader):\n",
    "    num_correct1, num_correct2, total = 0, 0, 0\n",
    "    preds1, preds2, lbls = [], [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores1 = model1(seq)\n",
    "        scores2 = model2(seq)\n",
    "        \n",
    "        num_correct1 += ((scores1 > 0) == lbl).sum().item()\n",
    "        num_correct2 += ((scores2 > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds1.extend(scores1.detach().tolist())\n",
    "        preds2.extend(scores2.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        num_correct1 / total,\n",
    "        num_correct2 / total,\n",
    "        roc_auc_score(lbls, preds1),\n",
    "        roc_auc_score(lbls, preds2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a14c35-6cc9-4990-a3fc-78c5b5f3d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    num_correct, total = 0, 0\n",
    "    preds, lbls = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval: \", file=sys.stdout, unit=\"batches\"):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        \n",
    "        num_correct += ((scores > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(scores.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return num_correct / total, roc_auc_score(lbls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ingo/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/c_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/c_val.csv')\n",
    "\n",
    "# load pre-trained esm2 model and vocab\n",
    "esm2, vocab = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "esm2c = copy.deepcopy(esm2) # needed when training two models, otherwise unexpected behavior\n",
    "tokenizer = vocab.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "RNN_SIZE = 512\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "NUM_GRADUAL = 10 # how many epochs for linear drop rate\n",
    "NOISY_RATE = 0.2 # assumed\n",
    "FORGET_RATE = NOISY_RATE / 2 # recommendation when facing asymmetric loss\n",
    "EXPONENT = 1\n",
    "\n",
    "# define drop rate schedule\n",
    "rate_schedule = np.ones(NUM_EPOCHS)*FORGET_RATE\n",
    "rate_schedule[:NUM_GRADUAL] = np.linspace(0, FORGET_RATE**EXPONENT, NUM_GRADUAL)\n",
    "\n",
    "model1 = ESM2BiLSTM(\n",
    "    esm2=esm2,\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "model2 = ESM2BiLSTM(\n",
    "    esm2=esm2c, # deepcopy esm2\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "BCEWLL = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion = JoCoRLoss()\n",
    "optimizer = optim.Adam(list(model1.parameters()) + list(model2.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=train_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [10:03<00:00,  3.68batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.84batches/s]\n",
      "Model 1 Training:   [Epoch  1, Loss: 0.001927, Acc: 0.8223, AUC: 0.5505]\n",
      "Model 1 Evaluation: [Epoch  1, Acc: 0.8215, AUC: 0.6723]\n",
      "Model 2 Training:   [Epoch  1, Loss: 0.001927, Acc: 0.8218, AUC: 0.5481]\n",
      "Model 2 Evaluation: [Epoch  1, Acc: 0.8215, AUC: 0.6789]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [10:00<00:00,  3.69batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.82batches/s]\n",
      "Model 1 Training:   [Epoch  2, Loss: 0.001757, Acc: 0.8231, AUC: 0.6928]\n",
      "Model 1 Evaluation: [Epoch  2, Acc: 0.8230, AUC: 0.8238]\n",
      "Model 2 Training:   [Epoch  2, Loss: 0.001757, Acc: 0.8231, AUC: 0.6903]\n",
      "Model 2 Evaluation: [Epoch  2, Acc: 0.8237, AUC: 0.8243]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [09:58<00:00,  3.70batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.85batches/s]\n",
      "Model 1 Training:   [Epoch  3, Loss: 0.001468, Acc: 0.8329, AUC: 0.7901]\n",
      "Model 1 Evaluation: [Epoch  3, Acc: 0.8427, AUC: 0.8304]\n",
      "Model 2 Training:   [Epoch  3, Loss: 0.001468, Acc: 0.8330, AUC: 0.7899]\n",
      "Model 2 Evaluation: [Epoch  3, Acc: 0.8431, AUC: 0.8309]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [09:56<00:00,  3.72batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:11<00:00,  3.87batches/s]\n",
      "Model 1 Training:   [Epoch  4, Loss: 0.001178, Acc: 0.8408, AUC: 0.7925]\n",
      "Model 1 Evaluation: [Epoch  4, Acc: 0.8471, AUC: 0.8433]\n",
      "Model 2 Training:   [Epoch  4, Loss: 0.001178, Acc: 0.8408, AUC: 0.7927]\n",
      "Model 2 Evaluation: [Epoch  4, Acc: 0.8480, AUC: 0.8429]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [09:59<00:00,  3.70batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:13<00:00,  3.80batches/s]\n",
      "Model 1 Training:   [Epoch  5, Loss: 0.000917, Acc: 0.8422, AUC: 0.8087]\n",
      "Model 1 Evaluation: [Epoch  5, Acc: 0.8485, AUC: 0.8457]\n",
      "Model 2 Training:   [Epoch  5, Loss: 0.000917, Acc: 0.8423, AUC: 0.8093]\n",
      "Model 2 Evaluation: [Epoch  5, Acc: 0.8484, AUC: 0.8457]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [09:59<00:00,  3.70batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.85batches/s]\n",
      "Model 1 Training:   [Epoch  6, Loss: 0.000819, Acc: 0.8456, AUC: 0.8041]\n",
      "Model 1 Evaluation: [Epoch  6, Acc: 0.8501, AUC: 0.8400]\n",
      "Model 2 Training:   [Epoch  6, Loss: 0.000819, Acc: 0.8453, AUC: 0.8040]\n",
      "Model 2 Evaluation: [Epoch  6, Acc: 0.8506, AUC: 0.8393]\n",
      "saved model1\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [09:56<00:00,  3.72batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.85batches/s]\n",
      "Model 1 Training:   [Epoch  7, Loss: 0.000702, Acc: 0.8469, AUC: 0.8053]\n",
      "Model 1 Evaluation: [Epoch  7, Acc: 0.8520, AUC: 0.8391]\n",
      "Model 2 Training:   [Epoch  7, Loss: 0.000702, Acc: 0.8468, AUC: 0.8047]\n",
      "Model 2 Evaluation: [Epoch  7, Acc: 0.8519, AUC: 0.8409]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [10:11<00:00,  3.63batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:16<00:00,  3.62batches/s]\n",
      "Model 1 Training:   [Epoch  8, Loss: 0.000623, Acc: 0.8478, AUC: 0.8003]\n",
      "Model 1 Evaluation: [Epoch  8, Acc: 0.8526, AUC: 0.8383]\n",
      "Model 2 Training:   [Epoch  8, Loss: 0.000623, Acc: 0.8480, AUC: 0.8012]\n",
      "Model 2 Evaluation: [Epoch  8, Acc: 0.8526, AUC: 0.8397]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [10:23<00:00,  3.55batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.83batches/s]\n",
      "Model 1 Training:   [Epoch  9, Loss: 0.000539, Acc: 0.8492, AUC: 0.7932]\n",
      "Model 1 Evaluation: [Epoch  9, Acc: 0.8535, AUC: 0.8297]\n",
      "Model 2 Training:   [Epoch  9, Loss: 0.000539, Acc: 0.8491, AUC: 0.7973]\n",
      "Model 2 Evaluation: [Epoch  9, Acc: 0.8528, AUC: 0.8331]\n",
      "saved model2\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [09:59<00:00,  3.70batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [01:12<00:00,  3.84batches/s]\n",
      "Model 1 Training:   [Epoch 10, Loss: 0.000460, Acc: 0.8492, AUC: 0.7868]\n",
      "Model 1 Evaluation: [Epoch 10, Acc: 0.8535, AUC: 0.8210]\n",
      "Model 2 Training:   [Epoch 10, Loss: 0.000460, Acc: 0.8492, AUC: 0.7847]\n",
      "Model 2 Evaluation: [Epoch 10, Acc: 0.8536, AUC: 0.8099]\n",
      "saved model1\n",
      "Finished Training.\n",
      "Training took 112.9463880777359 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "# normal training loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss1, train_loss2, train_acc1, train_acc2, train_auc1, train_auc2 = train(\n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "        optim=optimizer,\n",
    "        loss_func=criterion, # JoCoRLoss\n",
    "        loader=train_loader,\n",
    "        forget_rate=rate_schedule[epoch-1],\n",
    "        BCEWLL=BCEWLL, # nn.BCEWithLogitsLoss with reduction=none\n",
    "        kl_loss=kl_loss_compute\n",
    "    )\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_acc1, val_acc2, val_auc1, val_auc2 = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loader=dev_loader,\n",
    "        )\n",
    "    \n",
    "    print(\n",
    "        f\"Model 1 Training:   [Epoch {epoch:2d}, Loss: {train_loss1:8.6f}, Acc: {train_acc1:.4f}, AUC: {train_auc1:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 1 Evaluation: [Epoch {epoch:2d}, Acc: {val_acc1:.4f}, AUC: {val_auc1:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 2 Training:   [Epoch {epoch:2d}, Loss: {train_loss2:8.6f}, Acc: {train_acc2:.4f}, AUC: {train_auc2:.4f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Model 2 Evaluation: [Epoch {epoch:2d}, Acc: {val_acc2:.4f}, AUC: {val_auc2:.4f}]\"\n",
    "    )\n",
    "\n",
    "    if val_auc1 > val_auc2:\n",
    "        reg_auc = regularized_auc(train_auc1, val_auc1, threshold=0)\n",
    "        model = model1.state_dict()\n",
    "        print('saved model1')\n",
    "    else:\n",
    "        reg_auc = regularized_auc(train_auc2, val_auc2, threshold=0)\n",
    "        model = model2.state_dict()\n",
    "        print('saved model2')\n",
    "    if reg_auc > highest_val_auc:\n",
    "        highest_val_auc = reg_auc\n",
    "        path = f\"../../params/c_term/ESM2BiLSTM_jocor/auc{reg_auc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model, path)\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted model1\n",
      "deleted model2\n",
      "deleted esm2\n",
      "deleted esm2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ingo/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.8457_epoch5.pt\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:38<00:00,  7.16batches/s]\n",
      "Test Set Performance: Acc: 0.8493, AUC: 0.8452\n",
      "Total model params: 152998267, trainable model params: 4858113\n"
     ]
    }
   ],
   "source": [
    "if 'model1' in globals():\n",
    "    del model1\n",
    "    print('deleted model1')\n",
    "if 'model2' in globals():\n",
    "    del model2\n",
    "    print('deleted model2')\n",
    "if 'esm2' in globals():\n",
    "    del esm2\n",
    "    print('deleted esm2')\n",
    "if 'esm2c' in globals():\n",
    "    del esm2c\n",
    "    print('deleted esm2c')\n",
    "    \n",
    "esm2, _ = torch.hub.load('facebookresearch/esm:main', 'esm2_t30_150M_UR50D')\n",
    "model = ESM2BiLSTM(\n",
    "    esm2=esm2,\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to('cpu')\n",
    "\n",
    "test_path = '../../data/c_test.csv'\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/c_term/ESM2BiLSTM_jocor/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model.load_state_dict(torch.load('../../params/c_term/ESM2BiLSTM_jocor/' + best_model))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_acc, test_auc = test(model, test_loader)\n",
    "print(\n",
    "    f\"Test Set Performance: Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15bfe2-6557-42a6-81c1-a6c2960923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
