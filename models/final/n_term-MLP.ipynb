{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1380137a-3d14-4fd2-92d2-c558cb7ac777",
   "metadata": {},
   "source": [
    "### Model architecture is based on https://www.biorxiv.org/content/10.1101/710699v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_data(path):\n",
    "#     with open(path, 'r') as csvfile:\n",
    "#         train_data = list(csv.reader(csvfile))[1:] # skip col name\n",
    "#         sents, lbls = [], []\n",
    "#         for i in range(0, len(train_data), 16):\n",
    "#             s, l = zip(*train_data[i:i+16])\n",
    "#             sents.append(s)\n",
    "#             lbls.append(l)\n",
    "#     return sents, lbls\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path, 'r') as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:] # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "# number of trainable parameters in model\n",
    "def get_total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "class CleavageBatch:\n",
    "    def __init__(self, batch: List[Tuple[str, str]]):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def collate_wrapper(batch):\n",
    "    return CleavageBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_size1, hidden_size2, hidden_size3, dropout):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "#         self.fc1 = nn.Linear(embedding_dim, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "#         self.fc4 = nn.Linear(hidden_size3, 1)\n",
    "        \n",
    "#     def forward(self, seq):\n",
    "#         # input shape: (batch_size, seq_len=10)\n",
    "#         embedded = self.dropout(self.embedding(seq))\n",
    "        \n",
    "#         # input shape: (batch_size, seq_len, embedding_dim)\n",
    "#         pooled = embedded.mean(dim=1)\n",
    "        \n",
    "#         # input shape: (batch_size, embedding_dim)\n",
    "#         out = self.dropout(F.leaky_relu(self.fc1(pooled)))\n",
    "        \n",
    "#         # input shape: (batch_size, hidden_size1)\n",
    "#         out = self.dropout(F.leaky_relu(self.fc2(out)))\n",
    "        \n",
    "#         # input shape: (batch_size, hidden_size2)\n",
    "#         out = self.dropout(F.leaky_relu(self.fc3(out)))\n",
    "        \n",
    "#         # input shape: (batch_size, hidden_size3)\n",
    "#         # output shape: (batch_size)\n",
    "#         return self.fc4(out).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff220e30-1534-407f-9701-af94d7c2ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.embedding(seq)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        \n",
    "        return self.fc(pooled).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += ((scores > 0) == lbl).sum()\n",
    "        total += len(seq)\n",
    "    return epoch_loss / total, num_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/n_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/n_val.csv')\n",
    "\n",
    "# create vocab from train seqs\n",
    "vocab = build_vocab_from_iterator(train_seqs, specials=['<UNK>'])\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "encode_text = lambda x: vocab(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable model parameters: 4,401\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 512\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_SIZE1 = 128\n",
    "HIDDEN_SIZE2 = 32\n",
    "HIDDEN_SIZE3 = 8\n",
    "DROPOUT = 0.\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = MLP(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ").to(device)\n",
    "\n",
    "# model = MLP(\n",
    "#     vocab_size=VOCAB_SIZE,\n",
    "#     embedding_dim=EMBEDDING_DIM,\n",
    "#     hidden_size1=HIDDEN_SIZE1,\n",
    "#     hidden_size2=HIDDEN_SIZE2,\n",
    "#     hidden_size3=HIDDEN_SIZE3,\n",
    "#     dropout=DROPOUT\n",
    "# ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "print(f\"Total trainable model parameters: {get_total_model_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|███████████████████████████████████████████████████| 2236/2236 [00:03<00:00, 722.86batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 280/280 [00:00<00:00, 725.14batches/s]\n",
      "Training:   [Epoch  1, Loss:   0.0013, Acc: 0.5796]\n",
      "Evaluation: [Epoch  1, Loss:   0.0013, Acc: 0.5808]\n",
      "Train: 100%|███████████████████████████████████████████████████| 2236/2236 [00:02<00:00, 769.71batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 280/280 [00:00<00:00, 705.89batches/s]\n",
      "Training:   [Epoch  2, Loss:   0.0013, Acc: 0.5806]\n",
      "Evaluation: [Epoch  2, Loss:   0.0013, Acc: 0.5811]\n",
      "Finished Training.\n",
      "Training took 0.11317599614461263 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_acc = 0\n",
    "train_losses, train_accuracies= [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss, train_acc = process(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process(model, dev_loader, criterion)\n",
    "        \n",
    "    # save current acc, loss\n",
    "    train_losses.append((epoch, train_loss))\n",
    "    train_accuracies.append((epoch, train_acc))\n",
    "    val_losses.append((epoch, val_loss))\n",
    "    val_accuracies.append((epoch, val_acc))\n",
    "    \n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        path = f\"../../params/n_term/quadBiLSTM/acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        \n",
    "    print(\n",
    "        f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.4f}, Acc: {train_acc:.4f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.4f}, Acc: {val_acc:.4f}]\")\n",
    "    \n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training stats\n",
    "lsts = [train_losses, train_accuracies, val_losses, val_accuracies, train_time]\n",
    "names = [\n",
    "    \"train_losses\",\n",
    "    \"train_accuracies\",\n",
    "    \"val_losses\",\n",
    "    \"val_accuracies\",\n",
    "    \"train_time\",\n",
    "]\n",
    "to_save = dict()\n",
    "for name, lst in zip(names, lsts):\n",
    "    to_save[name] = lst\n",
    "\n",
    "with open(f\"../params/n_term/quadBiLSTM/metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(to_save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Finished Saving Details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
