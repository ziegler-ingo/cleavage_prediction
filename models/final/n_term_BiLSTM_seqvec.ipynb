{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "CKH0qtixaNo3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "CKH0qtixaNo3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "395abda0-2c29-4d54-d528-8e349cf9903a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting bio-embeddings[seqvec]\n",
      "  Downloading bio_embeddings-0.2.2-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 7.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.18.3 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (1.21.6)\n",
      "Collecting scikit-learn<0.25.0,>=0.24.0\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 85.8 MB/s \n",
      "\u001b[?25hCollecting python-slugify<6.0.0,>=5.0.2\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Collecting umap-learn<0.6.0,>=0.5.1\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 9.4 MB/s \n",
      "\u001b[?25hCollecting biopython<2.0,>=1.79\n",
      "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 46.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata<5.0.0,>=4.6.1 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (4.12.0)\n",
      "Collecting torch<=1.10.0,>=1.8.0\n",
      "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:40tcmalloc: large alloc 1147494400 bytes == 0x39c04000 @  0x7f46be57c615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n",
      "\u001b[?25hCollecting gensim<4.0.0,>=3.8.2\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 1.2 MB/s \n",
      "\u001b[?25hCollecting h5py<4.0.0,>=3.2.1\n",
      "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 49.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.45.0 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (4.64.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (1.4.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (1.7.3)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (5.5.0)\n",
      "Collecting ruamel.yaml<0.18.0,>=0.17.10\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 70.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: atomicwrites<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (1.4.1)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (3.2.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings[seqvec]) (1.3.5)\n",
      "Collecting humanize<4.0.0,>=3.2.0\n",
      "  Downloading humanize-3.14.0-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 10.2 MB/s \n",
      "\u001b[?25hCollecting lock<2019.0.0,>=2018.3.25\n",
      "  Downloading lock-2018.3.25.2110.tar.gz (3.0 kB)\n",
      "Collecting bio-embeddings-allennlp==0.9.2\n",
      "  Downloading bio_embeddings_allennlp-0.9.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 15.7 MB/s \n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.24.54-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 74.2 MB/s \n",
      "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 52.2 MB/s \n",
      "\u001b[?25hCollecting conllu==4.4\n",
      "  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pytorch-transformers==1.1.0\n",
      "  Downloading pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 72.7 MB/s \n",
      "\u001b[?25hCollecting responses>=0.7\n",
      "  Downloading responses-0.21.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.6 MB/s \n",
      "\u001b[?25hCollecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
      "\u001b[K     |████████████████████████████████| 592 kB 60.1 MB/s \n",
      "\u001b[?25hCollecting gevent>=1.3.6\n",
      "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 56.6 MB/s \n",
      "\u001b[?25hCollecting unidecode\n",
      "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
      "\u001b[K     |████████████████████████████████| 235 kB 78.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (3.6.4)\n",
      "Collecting flask-cors>=3.0.7\n",
      "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting flaky\n",
      "  Downloading flaky-3.7.0-py2.py3-none-any.whl (22 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.23.0)\n",
      "Collecting numpydoc>=0.8.0\n",
      "  Downloading numpydoc-1.4.0-py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 840 kB/s \n",
      "\u001b[?25hCollecting jsonpickle\n",
      "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting overrides<5,>=4\n",
      "  Downloading overrides-4.1.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.4.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (3.7)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 76.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.5.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2022.2)\n",
      "Collecting word2number>=1.1\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "Collecting spacy<3.0,>=2.1.0\n",
      "  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 34.9 MB/s \n",
      "\u001b[?25hCollecting parsimonious>=0.8.0\n",
      "  Downloading parsimonious-0.9.0.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 7.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.1.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2022.6.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 58.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.1.0)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.0.1)\n",
      "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.7->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0,>=3.8.2->bio-embeddings[seqvec]) (5.2.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (57.4.0)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
      "\u001b[K     |████████████████████████████████| 251 kB 70.0 MB/s \n",
      "\u001b[?25hCollecting zope.event\n",
      "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata<5.0.0,>=4.6.1->bio-embeddings[seqvec]) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata<5.0.0,>=4.6.1->bio-embeddings[seqvec]) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.1->bio-embeddings[seqvec]) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.1->bio-embeddings[seqvec]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.1->bio-embeddings[seqvec]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.1->bio-embeddings[seqvec]) (3.0.9)\n",
      "Collecting sphinx>=3.0\n",
      "  Downloading Sphinx-5.1.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 50.0 MB/s \n",
      "\u001b[?25hCollecting typing-utils>=0.0.3\n",
      "  Downloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly<6.0.0,>=5.1.0->bio-embeddings[seqvec]) (8.0.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify<6.0.0,>=5.0.2->bio-embeddings[seqvec]) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.24.3)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 70.1 MB/s \n",
      "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
      "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
      "\u001b[K     |████████████████████████████████| 546 kB 64.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0->bio-embeddings[seqvec]) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0->bio-embeddings[seqvec]) (1.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.1.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (3.0.6)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 65.4 MB/s \n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.1.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.0.7)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
      "\u001b[K     |████████████████████████████████| 184 kB 63.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.1.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.1.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.10.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.1.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.7.8)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.1.5)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 12.3 MB/s \n",
      "\u001b[?25hCollecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (21.3)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.2.0)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.20,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.17.1)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.4.1)\n",
      "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (2.10.3)\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 78.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.7.12)\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 12.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (3.17.3)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn<0.6.0,>=0.5.1->bio-embeddings[seqvec]) (0.56.0)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 25.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn<0.6.0,>=0.5.1->bio-embeddings[seqvec]) (0.39.0)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 10.8 MB/s \n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.28.0,>=1.27.54\n",
      "  Downloading botocore-1.27.54-py3-none-any.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 42.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.2.5)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (0.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (22.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (8.14.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bio-embeddings-allennlp==0.9.2->bio-embeddings[seqvec]) (1.11.0)\n",
      "Building wheels for collected packages: jsonnet, lock, parsimonious, umap-learn, pynndescent, word2number\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994696 sha256=54b499c85f1b95199fca40788ef0ee2a900d40e5ec2da6d7f9318c305c8fca24\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n",
      "  Building wheel for lock (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lock: filename=lock-2018.3.25.2110-py3-none-any.whl size=3318 sha256=c1ac4bb28b849e1d4238f780da4e9a3202fa9a0826c21444068c33ec66aba7e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/70/80/f2d0dbe94130ae6eee956436fb20d3920649588cc39c892206\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for parsimonious: filename=parsimonious-0.9.0-py3-none-any.whl size=44314 sha256=25a4d6a1ffdd626e87e9c33b4b15b4ca8e1bb912a12824ea5f4eac44481239a9\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/54/88/c1ee7de0eabd1fb817cbf35824e4c2cba664d5816ddc64efb1\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=c3933fa814717841a20cabb224f2137358caa56d8ced8239bdeec1d7e64268af\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=a3f99575977cf9739a10bdebb1e4e730059299f7fe3a33886b03ee7fc47c3f20\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=ed60a46b23ec7f5ca4f25e7e8d9722b3afac0c7e812ed0770ff50393a4c65867\n",
      "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
      "Successfully built jsonnet lock parsimonious umap-learn pynndescent word2number\n",
      "Installing collected packages: urllib3, jmespath, botocore, srsly, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, scikit-learn, s3transfer, plac, catalogue, zope.interface, zope.event, typing-utils, torch, thinc, sphinx, sentencepiece, ruamel.yaml.clib, pynndescent, boto3, word2number, unidecode, umap-learn, tensorboardX, spacy, ruamel.yaml, responses, pytorch-transformers, pytorch-pretrained-bert, python-slugify, parsimonious, overrides, numpydoc, lock, jsonpickle, jsonnet, humanize, h5py, gevent, gensim, ftfy, flask-cors, flaky, conllu, biopython, bio-embeddings-allennlp, bio-embeddings\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 2.4.4\n",
      "    Uninstalling srsly-2.4.4:\n",
      "      Successfully uninstalled srsly-2.4.4\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 2.0.8\n",
      "    Uninstalling catalogue-2.0.8:\n",
      "      Successfully uninstalled catalogue-2.0.8\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu113\n",
      "    Uninstalling torch-1.12.1+cu113:\n",
      "      Successfully uninstalled torch-1.12.1+cu113\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.0\n",
      "    Uninstalling thinc-8.1.0:\n",
      "      Successfully uninstalled thinc-8.1.0\n",
      "  Attempting uninstall: sphinx\n",
      "    Found existing installation: Sphinx 1.8.6\n",
      "    Uninstalling Sphinx-1.8.6:\n",
      "      Successfully uninstalled Sphinx-1.8.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n",
      "  Attempting uninstall: python-slugify\n",
      "    Found existing installation: python-slugify 6.1.2\n",
      "    Uninstalling python-slugify-6.1.2:\n",
      "      Successfully uninstalled python-slugify-6.1.2\n",
      "  Attempting uninstall: humanize\n",
      "    Found existing installation: humanize 0.5.1\n",
      "    Uninstalling humanize-0.5.1:\n",
      "      Successfully uninstalled humanize-0.5.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n",
      "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n",
      "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed bio-embeddings-0.2.2 bio-embeddings-allennlp-0.9.2 biopython-1.79 boto3-1.24.54 botocore-1.27.54 catalogue-1.0.0 conllu-4.4 flaky-3.7.0 flask-cors-3.0.10 ftfy-6.1.1 gensim-3.8.3 gevent-21.12.0 h5py-3.7.0 humanize-3.14.0 jmespath-1.0.1 jsonnet-0.18.0 jsonpickle-2.2.0 lock-2018.3.25.2110 numpydoc-1.4.0 overrides-4.1.2 parsimonious-0.9.0 plac-1.1.3 pynndescent-0.5.7 python-slugify-5.0.2 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.21.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 s3transfer-0.6.0 scikit-learn-0.24.2 sentencepiece-0.1.97 spacy-2.3.7 sphinx-5.1.1 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 srsly-1.0.5 tensorboardX-2.5.1 thinc-7.4.5 torch-1.10.0 typing-utils-0.1.0 umap-learn-0.5.3 unidecode-1.3.4 urllib3-1.25.11 word2number-1.1 zope.event-4.5.0 zope.interface-5.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "sphinxcontrib",
         "torch"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install bio-embeddings[seqvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "FXDMuR9Oe2ay",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXDMuR9Oe2ay",
    "outputId": "8e4f1100-b1c0-42e2-ab1d-4a13e5ac2f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {
    "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from allennlp.commands.elmo import ElmoEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {
    "id": "2bbbe232-2c39-49c6-9187-a80c6812d813"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {
    "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r') as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:] # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "# number of trainable parameters in model\n",
    "def get_total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {
    "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8"
   },
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)    \n",
    "    \n",
    "def collate_batch(batch):\n",
    "    ordered_batch = list(zip(*batch))\n",
    "    seq = [list(s) for s in ordered_batch[0]] # is still a string\n",
    "    lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "    return seq, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {
    "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, rnn_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        # input is already embedded by ELMo\n",
    "        # input shape: (batch_size, seq_len=10, embedding_dim)\n",
    "        embedded = self.dropout(seq)\n",
    "\n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled = torch.mean(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, 2*hidden_size)\n",
    "        out = self.dropout(F.relu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size)\n",
    "        out = self.fc2(out).squeeze()\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {
    "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99"
   },
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    \n",
    "    for seq, lbl in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        seq, _ = embedder.batch_to_embeddings(seq) # is already on GPU\n",
    "        seq = seq.sum(dim=1)\n",
    "        lbl = lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += ((scores > 0) == lbl).sum()\n",
    "        total += len(seq)\n",
    "    return epoch_loss / total, num_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "z8KrBUMem0CO",
   "metadata": {
    "id": "z8KrBUMem0CO"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "embedder = ElmoEmbedder(\n",
    "    options_file='./drive/MyDrive/data/seqvec/options.json',\n",
    "    weight_file='./drive/MyDrive/data/seqvec/weights.hdf5',\n",
    "    cuda_device=0 # use colab gpu\n",
    ")\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('./drive/MyDrive/data/n_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('./drive/MyDrive/data/n_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
    "outputId": "089718d3-f10f-46a3-e5a8-cdcaa2273298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable model parameters: 6,430,977\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "EMBEDDING_DIM = 1024 # given by ELMo\n",
    "RNN_SIZE = 512\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = BiLSTM(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_size=RNN_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_batch, num_workers=2)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_batch, num_workers=2)\n",
    "\n",
    "print(f\"Total trainable model parameters: {get_total_model_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
    "outputId": "649b2f67-4623-4cf3-d3e9-0f8fee24a94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|██████████| 2236/2236 [27:51<00:00,  1.34batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:20<00:00,  1.39batches/s]\n",
      "Training:   [Epoch  1, Loss:   0.0012, Acc: 0.6467]\n",
      "Evaluation: [Epoch  1, Loss:   0.0011, Acc: 0.6797]\n",
      "Train: 100%|██████████| 2236/2236 [27:51<00:00,  1.34batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:20<00:00,  1.40batches/s]\n",
      "Training:   [Epoch  2, Loss:   0.0011, Acc: 0.6844]\n",
      "Evaluation: [Epoch  2, Loss:   0.0011, Acc: 0.6894]\n",
      "Train: 100%|██████████| 2236/2236 [27:49<00:00,  1.34batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:19<00:00,  1.40batches/s]\n",
      "Training:   [Epoch  3, Loss:   0.0011, Acc: 0.6895]\n",
      "Evaluation: [Epoch  3, Loss:   0.0011, Acc: 0.6933]\n",
      "Train: 100%|██████████| 2236/2236 [27:51<00:00,  1.34batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:21<00:00,  1.39batches/s]\n",
      "Training:   [Epoch  4, Loss:   0.0011, Acc: 0.6915]\n",
      "Evaluation: [Epoch  4, Loss:   0.0011, Acc: 0.6941]\n",
      "Train: 100%|██████████| 2236/2236 [27:57<00:00,  1.33batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:21<00:00,  1.39batches/s]\n",
      "Training:   [Epoch  5, Loss:   0.0011, Acc: 0.6935]\n",
      "Evaluation: [Epoch  5, Loss:   0.0011, Acc: 0.6956]\n",
      "Train: 100%|██████████| 2236/2236 [27:58<00:00,  1.33batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:21<00:00,  1.39batches/s]\n",
      "Training:   [Epoch  6, Loss:   0.0011, Acc: 0.6952]\n",
      "Evaluation: [Epoch  6, Loss:   0.0011, Acc: 0.6961]\n",
      "Train: 100%|██████████| 2236/2236 [28:00<00:00,  1.33batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:21<00:00,  1.39batches/s]\n",
      "Training:   [Epoch  7, Loss:   0.0011, Acc: 0.6967]\n",
      "Evaluation: [Epoch  7, Loss:   0.0011, Acc: 0.6964]\n",
      "Train: 100%|██████████| 2236/2236 [28:03<00:00,  1.33batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:22<00:00,  1.38batches/s]\n",
      "Training:   [Epoch  8, Loss:   0.0011, Acc: 0.6980]\n",
      "Evaluation: [Epoch  8, Loss:   0.0011, Acc: 0.6973]\n",
      "Train: 100%|██████████| 2236/2236 [28:03<00:00,  1.33batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:22<00:00,  1.38batches/s]\n",
      "Training:   [Epoch  9, Loss:   0.0011, Acc: 0.6990]\n",
      "Evaluation: [Epoch  9, Loss:   0.0011, Acc: 0.6980]\n",
      "Train: 100%|██████████| 2236/2236 [28:03<00:00,  1.33batches/s]\n",
      "Eval: 100%|██████████| 280/280 [03:21<00:00,  1.39batches/s]\n",
      "Training:   [Epoch 10, Loss:   0.0011, Acc: 0.7000]\n",
      "Evaluation: [Epoch 10, Loss:   0.0011, Acc: 0.6973]\n",
      "Finished Training.\n",
      "Training took 313.10402743816377 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_acc = 0\n",
    "train_losses, train_accuracies= [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss, train_acc = process(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = process(model, dev_loader, criterion)\n",
    "        \n",
    "    # save current acc, loss\n",
    "    train_losses.append((epoch, train_loss))\n",
    "    train_accuracies.append((epoch, train_acc))\n",
    "    val_losses.append((epoch, val_loss))\n",
    "    val_accuracies.append((epoch, val_acc))\n",
    "    \n",
    "    if val_acc > highest_val_acc:\n",
    "        highest_val_acc = val_acc\n",
    "        path = f\"./drive/MyDrive/data/n_term/seqvecBiLSTM/acc{val_acc:.4f}_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        \n",
    "    print(\n",
    "        f\"Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.4f}, Acc: {train_acc:.4f}]\"\n",
    "    )\n",
    "    print(f\"Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.4f}, Acc: {val_acc:.4f}]\")\n",
    "    \n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {
    "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22"
   },
   "outputs": [],
   "source": [
    "# save training stats\n",
    "lsts = [train_losses, train_accuracies, val_losses, val_accuracies, train_time]\n",
    "names = [\n",
    "    \"train_losses\",\n",
    "    \"train_accuracies\",\n",
    "    \"val_losses\",\n",
    "    \"val_accuracies\",\n",
    "    \"train_time\",\n",
    "]\n",
    "to_save = dict()\n",
    "for name, lst in zip(names, lsts):\n",
    "    to_save[name] = lst\n",
    "\n",
    "with open(f\"../params/n_term/quadBiLSTM/metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(to_save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Finished Saving Details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7",
   "metadata": {
    "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "n_term-BiLSTM-seqvec.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
