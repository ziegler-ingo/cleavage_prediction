{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f2c00a-8657-495d-a255-caffe9e86e78",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* BiLSTM model architecture based on [Ozols et. al., 2021](https://www.mdpi.com/1422-0067/22/6/3071/htm)\n",
    "* Noise adaptation layer implementation is based on [Goldberger and Ben-Reuven, 2017](https://openreview.net/references/pdf?id=Sk5qglwSl), and unofficial implementation on [Github](https://github.com/Billy1900/Noise-Adaption-Layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence,\n",
    "    pad_packed_sequence\n",
    ")\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    Sampler,\n",
    "    BatchSampler\n",
    ")\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 1 (i.e. [UNK]) at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m) + (dist == m) * 1\n",
    "\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "    def __init__(self, seqs, batch_size):\n",
    "\n",
    "        # pair each sequence with their *tokenized* length\n",
    "        indices = [(idx, len(tokenizer.encode(s).ids)) for idx, s in enumerate(seqs)]\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        idx_pools = []\n",
    "        # generate pseudo-random batches of (arbitrary) size batch_size * 100\n",
    "        # each batch of size batch_size * 100 is sorted in itself by seq length\n",
    "        for i in range(0, len(indices), batch_size * 100):\n",
    "            idx_pools.extend(\n",
    "                sorted(indices[i : i + batch_size * 100], key=lambda x: x[1])\n",
    "            )\n",
    "\n",
    "        # filter only indices\n",
    "        self.idx_pools = [x[0] for x in idx_pools]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.idx_pools)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_pools)\n",
    "\n",
    "\n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        seq = torch.tensor(\n",
    "            [s.ids for s in tokenizer.encode_batch(ordered_batch[0])], dtype=torch.int64\n",
    "        )\n",
    "        self.seq = apply_random_masking(seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.long)\n",
    "        self.lengths = torch.tensor([self.seq.shape[1]] * self.seq.shape[0], dtype=torch.int64)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "\n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.seq = torch.tensor(\n",
    "            [s.ids for s in tokenizer.encode_batch(ordered_batch[0])], dtype=torch.int64\n",
    "        )\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.long)\n",
    "        self.lengths = torch.tensor([self.seq.shape[1]] * self.seq.shape[0], dtype=torch.int64)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "\n",
    "\n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        rnn_size1,\n",
    "        rnn_size2,\n",
    "        hidden_size,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=1\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2 * rnn_size1,\n",
    "            hidden_size=rnn_size2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(rnn_size2 * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, seq, lengths):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.dropout(self.embedding(seq))\n",
    "\n",
    "        packed_embeddings = pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm1(packed_embeddings)\n",
    "\n",
    "        # input shape: (batch_size, seq_len, 2*rnn_size1)\n",
    "        out, _ = self.lstm2(out)\n",
    "\n",
    "        unpacked_output, _ = pad_packed_sequence(out, batch_first=True, padding_value=1)\n",
    "\n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled, _ = torch.max(unpacked_output, dim=1)\n",
    "\n",
    "        # input shape; (batch_size, 2*hidden_size)\n",
    "        out = self.dropout(gelu(self.fc1(pooled)))\n",
    "\n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size, 2)\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72cabe9-f647-47ba-96ed-37c52355b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseAdaptation(nn.Module):\n",
    "    def __init__(self, theta, k):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Linear(k, k, bias=False)\n",
    "        self.theta.weight.data = theta\n",
    "        self.eye = torch.eye(k).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        theta = self.theta(self.eye)\n",
    "        theta = torch.softmax(theta, dim=0)\n",
    "        out = x @ theta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None, conf=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    pos_preds, preds, lbls = [], [], []\n",
    "\n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\",\n",
    "    ):\n",
    "        seq, lbl, lengths = batch.seq, batch.lbl, batch.lengths\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "\n",
    "        scores = model(seq, lengths)\n",
    "        loss = criterion(scores, lbl)\n",
    "\n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        pred = scores.argmax(dim=1)\n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += (pred == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(pred.detach().tolist())\n",
    "        pos_preds.extend(scores[:, 1].detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    if conf is not None:   \n",
    "        return confusion_matrix(lbls, preds)\n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, pos_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5534faf-85ed-4db7-9c09-944e36f6e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid(\n",
    "    model, noisemodel, optimizer, noise_optimizer, criterion, beta, loader\n",
    "):\n",
    "    epoch_loss, model_loss, noise_loss, num_correct, total = 0, 0, 0, 0, 0\n",
    "    preds, lbls = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Hybrid Train: \", file=sys.stdout, unit=\"batches\"):\n",
    "\n",
    "        seq, lbl, lengths = batch.seq, batch.lbl, batch.lengths\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "\n",
    "        scores = model(seq, lengths)\n",
    "        noise_scores = noisemodel(scores)\n",
    "\n",
    "        model_loss = criterion(scores, lbl)\n",
    "        noise_loss = criterion(noise_scores, lbl)\n",
    "\n",
    "        loss = beta * noise_loss + (1 - beta) * model_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        noise_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        noise_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        model_loss += model_loss.item()\n",
    "        noise_loss += noise_loss.item()\n",
    "        num_correct += (noise_scores.argmax(dim=1)  == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(noise_scores[:, 1].detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss / total,\n",
    "        model_loss / total,\n",
    "        noise_loss / total,\n",
    "        num_correct / total,\n",
    "        roc_auc_score(lbls, preds),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_file = \"../../params/c_term/bbpe_params/50k/vocab.json\"\n",
    "merge_file = \"../../params/c_term/bbpe_params/50k/merges.txt\"\n",
    "\n",
    "# tokenizer serves as vocab at the same time\n",
    "tokenizer = ByteLevelBPETokenizer.from_file(vocab_file, merge_file)\n",
    "tokenizer.enable_padding(pad_token=\"<PAD>\")\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/c_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/c_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "NUM_WARMUP = 1\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 512\n",
    "BETA=0.8\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "EMBEDDING_DIM = 150\n",
    "RNN_SIZE1 = 256\n",
    "RNN_SIZE2 = 512\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = BiLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_size1=RNN_SIZE1,\n",
    "    rnn_size2=RNN_SIZE2,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=train_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|███████████████████████████████████████████████████| 2218/2218 [00:14<00:00, 150.63batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 289.13batches/s]\n",
      "Warmup Training:   [Epoch  1, Loss: 0.000918, Acc: 0.8231, AUC: 0.5010]\n",
      "Warmup Evaluation: [Epoch  1, Loss: 0.000917, Acc: 0.8215, AUC: 0.5291]\n",
      "Eval: 100%|████████████████████████████████████████████████████| 2218/2218 [00:05<00:00, 428.21batches/s]\n",
      "Created NoiseModel in epoch 1\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.94batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 288.96batches/s]\n",
      "Hy-Training: [Epoch  2, Hy-Loss: 0.001266,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4919]\n",
      "Evaluation:  [Epoch  2, Loss: 0.000916, Acc: 0.8215, AUC: 0.5428]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.87batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 292.01batches/s]\n",
      "Hy-Training: [Epoch  3, Hy-Loss: 0.001265,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4845]\n",
      "Evaluation:  [Epoch  3, Loss: 0.000913, Acc: 0.8215, AUC: 0.5604]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 146.88batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 285.85batches/s]\n",
      "Hy-Training: [Epoch  4, Hy-Loss: 0.001265,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4686]\n",
      "Evaluation:  [Epoch  4, Loss: 0.000907, Acc: 0.8217, AUC: 0.5764]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 144.14batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 291.45batches/s]\n",
      "Hy-Training: [Epoch  5, Hy-Loss: 0.001264,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4537]\n",
      "Evaluation:  [Epoch  5, Loss: 0.000904, Acc: 0.8220, AUC: 0.5888]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 144.01batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 302.03batches/s]\n",
      "Hy-Training: [Epoch  6, Hy-Loss: 0.001263,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4414]\n",
      "Evaluation:  [Epoch  6, Loss: 0.000899, Acc: 0.8221, AUC: 0.6006]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 143.87batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 293.90batches/s]\n",
      "Hy-Training: [Epoch  7, Hy-Loss: 0.001263,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4309]\n",
      "Evaluation:  [Epoch  7, Loss: 0.000894, Acc: 0.8223, AUC: 0.6135]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.19batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 286.30batches/s]\n",
      "Hy-Training: [Epoch  8, Hy-Loss: 0.001262,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4206]\n",
      "Evaluation:  [Epoch  8, Loss: 0.000889, Acc: 0.8224, AUC: 0.6240]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 144.85batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 286.27batches/s]\n",
      "Hy-Training: [Epoch  9, Hy-Loss: 0.001262,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8231, AUC: 0.4090]\n",
      "Evaluation:  [Epoch  9, Loss: 0.000884, Acc: 0.8225, AUC: 0.6342]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.47batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 284.30batches/s]\n",
      "Hy-Training: [Epoch 10, Hy-Loss: 0.001261,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8232, AUC: 0.3998]\n",
      "Evaluation:  [Epoch 10, Loss: 0.000881, Acc: 0.8226, AUC: 0.6408]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.78batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 294.30batches/s]\n",
      "Hy-Training: [Epoch 11, Hy-Loss: 0.001260,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8232, AUC: 0.3922]\n",
      "Evaluation:  [Epoch 11, Loss: 0.000876, Acc: 0.8227, AUC: 0.6496]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.78batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 289.69batches/s]\n",
      "Hy-Training: [Epoch 12, Hy-Loss: 0.001260,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8232, AUC: 0.3852]\n",
      "Evaluation:  [Epoch 12, Loss: 0.000874, Acc: 0.8228, AUC: 0.6544]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 144.89batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 286.52batches/s]\n",
      "Hy-Training: [Epoch 13, Hy-Loss: 0.001259,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8233, AUC: 0.3790]\n",
      "Evaluation:  [Epoch 13, Loss: 0.000871, Acc: 0.8229, AUC: 0.6600]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 144.06batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 287.04batches/s]\n",
      "Hy-Training: [Epoch 14, Hy-Loss: 0.001259,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8234, AUC: 0.3738]\n",
      "Evaluation:  [Epoch 14, Loss: 0.000867, Acc: 0.8232, AUC: 0.6646]\n",
      "Hybrid Train: 100%|████████████████████████████████████████████| 2218/2218 [00:15<00:00, 145.52batches/s]\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 287.59batches/s]\n",
      "Hy-Training: [Epoch 15, Hy-Loss: 0.001258,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8236, AUC: 0.3696]\n",
      "Evaluation:  [Epoch 15, Loss: 0.000865, Acc: 0.8231, AUC: 0.6687]\n",
      "Finished Training.\n",
      "Training took 4.281923135121663 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if epoch < NUM_WARMUP + 1:\n",
    "        model.train()\n",
    "        train_loss, train_acc, train_auc = process(model, train_loader, criterion, optimizer)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "            \n",
    "        print(\n",
    "        f\"Warmup Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Warmup Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        if epoch == NUM_WARMUP:\n",
    "            # get conf matrix based on predictions on train data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                conf = process(model, train_loader, criterion, conf=True)\n",
    "            theta = conf / conf.sum(axis=1, keepdims=True)\n",
    "            theta = torch.from_numpy(np.log(theta + 1e-8)).to(torch.float) # avoid zeros with +1e-8\n",
    "\n",
    "            # create noisemodel\n",
    "            noisemodel = NoiseAdaptation(theta=theta, k=NUM_CLASSES).to(device)\n",
    "            noise_optimizer = optim.Adam(noisemodel.parameters(), lr=LEARNING_RATE)\n",
    "            print(f'Created NoiseModel in epoch {epoch}')\n",
    "\n",
    "    else:\n",
    "        # hybrid training\n",
    "        model.train()\n",
    "        noisemodel.train()\n",
    "        hy_tr_loss, model_loss, noise_loss, hy_tr_acc, hy_tr_auc = train_hybrid(\n",
    "            model=model,\n",
    "            noisemodel=noisemodel,\n",
    "            optimizer=optimizer,\n",
    "            noise_optimizer=noise_optimizer,\n",
    "            criterion=criterion,\n",
    "            beta=BETA,\n",
    "            loader=train_loader\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "\n",
    "        print(\n",
    "            f\"Hy-Training: [Epoch {epoch:2d}, Hy-Loss: {hy_tr_loss:.6f},\\\n",
    "            Model-Loss: {model_loss:.6f}, Noise-Loss: {noise_loss:.6f},\\\n",
    "            Acc: {hy_tr_acc:.4f}, AUC: {hy_tr_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Evaluation:  [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        reg_auc = regularized_auc(hy_tr_auc, val_auc, threshold=0)\n",
    "        if reg_auc > highest_val_auc:\n",
    "            highest_val_auc = reg_auc\n",
    "            path = f\"../../params/c_term/bbpe50k_BiLSTM_noise_layer/auc{reg_auc:.4f}_epoch{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.6687_epoch15.pt\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 278/278 [00:00<00:00, 311.28batches/s]\n",
      "Test Set Performance: Loss: 0.000861, Acc: 0.8238, AUC: 0.6742\n",
      "Total model params: 12669538, trainable model params: 12669538\n"
     ]
    }
   ],
   "source": [
    "test_path = '../../data/c_test.csv'\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/c_term/bbpe50k_BiLSTM_noise_layer/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model.load_state_dict(torch.load('../../params/c_term/bbpe50k_BiLSTM_noise_layer/' + best_model))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc, test_auc = process(model, test_loader, criterion)\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15bfe2-6557-42a6-81c1-a6c2960923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
