{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../denoise/')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from divide_mix.data_handling import CleavageLoader\n",
    "from divide_mix.train_utils import (\n",
    "    NegEntropy,\n",
    "    SemiLoss,\n",
    "    warmup,\n",
    "    train,\n",
    "    evaluate,\n",
    "    process_gmm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "            \n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size1, rnn_size2, hidden_size, dropout1, dropout2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        self.dropout1=nn.Dropout(dropout1)\n",
    "        self.dropout2=nn.Dropout(dropout2)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2 * rnn_size1,\n",
    "            hidden_size=rnn_size2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size2 * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    \n",
    "    def no_embed_fw(self, embedded):\n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out, _ = self.lstm1(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*rnn_size1)\n",
    "        out, _ = self.lstm2(out)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*hidden_size)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, 2*hidden_size)\n",
    "        out = self.dropout1(gelu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size, 2)\n",
    "        return self.fc2(out)\n",
    "    \n",
    "    def forward(self, seq, seq2=None, lam=None, interpolate=False):\n",
    "        if interpolate:\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            embedded1 = self.embedding(seq)\n",
    "            embedded2 = self.embedding(seq2)\n",
    "            embedded_mixed = lam * embedded1 + (1 - lam) * embedded2\n",
    "            return self.no_embed_fw(self.dropout2(embedded_mixed))\n",
    "        else:\n",
    "            # input shape: (batch_size, seq_len=10)\n",
    "            embedded = self.dropout1(self.embedding(seq))\n",
    "            return self.no_embed_fw(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83845c28-9cb4-48ed-a6c1-87a9f0908dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 3e-4\n",
    "ALPHA = 0.5\n",
    "LAMBDA_U = 0\n",
    "P_THRESHOLD = 0.5\n",
    "TEMPERATURE = 0.5\n",
    "NUM_EPOCHS = 15\n",
    "NUM_WARM_UP_EPOCHS = 1\n",
    "RAMPUP_LEN = 5\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "vocab = torch.load('../../data/vocab.pt')\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "beta_dist = torch.distributions.beta.Beta(ALPHA, ALPHA)\n",
    "loader = CleavageLoader(batch_size=BATCH_SIZE, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=76,\n",
    "    rnn_size1=252,\n",
    "    rnn_size2=518,\n",
    "    hidden_size=179,\n",
    "    dropout1=0.5,\n",
    "    dropout2=0.\n",
    ").to(device)\n",
    "\n",
    "model2 = BiLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=76,\n",
    "    rnn_size1=252,\n",
    "    rnn_size2=518,\n",
    "    hidden_size=179,\n",
    "    dropout1=0.5,\n",
    "    dropout2=0.\n",
    ").to(device)\n",
    "\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=LR)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=LR)\n",
    "\n",
    "criterion = SemiLoss()\n",
    "conf_penalty = NegEntropy()\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "CE = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup: 100%|██████████████████████████████████████████████████| 2243/2243 [00:21<00:00, 102.15batches/s]\n",
      "warmup: 100%|██████████████████████████████████████████████████| 2243/2243 [00:21<00:00, 102.15batches/s]\n",
      "Warm-Up Model1: [Epoch  1, Loss: -0.000239, Acc: 0.8222, AUC: 0.6990]\n",
      "Warm-Up Model2: [Epoch  1, Loss: -0.000238, Acc: 0.8218, AUC: 0.7023]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 87.26batches/s]\n",
      "Evaluation Set: [Epoch  1, Loss: 0.403189, Acc: 0.8315, AUC: 0.7769]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 197.38batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 196.56batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14003/14003 [02:35<00:00, 90.14batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 13758/13758 [02:32<00:00, 90.23batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 86.56batches/s]\n",
      "Training Set: [Epoch  2, Loss1: 0.476903, Loss2: 0.479681]\n",
      "DivideMix Training: [Epoch  2, Loss1: 0.001231, Loss2: 0.001203]\n",
      "Evaluation Set: [Epoch  2, Loss: 1.221427, Acc: 0.8184, AUC: 0.7685]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 196.90batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 196.90batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14742/14742 [02:43<00:00, 90.06batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14674/14674 [02:43<00:00, 89.57batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 86.06batches/s]\n",
      "Training Set: [Epoch  3, Loss1: 0.611745, Loss2: 0.718266]\n",
      "DivideMix Training: [Epoch  3, Loss1: 0.001279, Loss2: 0.001270]\n",
      "Evaluation Set: [Epoch  3, Loss: 1.303379, Acc: 0.8093, AUC: 0.7740]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 195.66batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 194.19batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14721/14721 [02:43<00:00, 89.88batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14683/14683 [02:43<00:00, 89.54batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 86.16batches/s]\n",
      "Training Set: [Epoch  4, Loss1: 0.667750, Loss2: 0.747271]\n",
      "DivideMix Training: [Epoch  4, Loss1: 0.001245, Loss2: 0.001248]\n",
      "Evaluation Set: [Epoch  4, Loss: 1.458051, Acc: 0.8049, AUC: 0.7726]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 193.83batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 191.74batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14579/14579 [02:43<00:00, 89.36batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14511/14511 [02:41<00:00, 89.97batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 85.40batches/s]\n",
      "Training Set: [Epoch  5, Loss1: 0.805825, Loss2: 0.748033]\n",
      "DivideMix Training: [Epoch  5, Loss1: 0.001210, Loss2: 0.001198]\n",
      "Evaluation Set: [Epoch  5, Loss: 1.496941, Acc: 0.7991, AUC: 0.7674]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 194.49batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 194.18batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14462/14462 [02:41<00:00, 89.77batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14482/14482 [02:41<00:00, 89.66batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 86.49batches/s]\n",
      "Training Set: [Epoch  6, Loss1: 0.798756, Loss2: 0.790007]\n",
      "DivideMix Training: [Epoch  6, Loss1: 0.001188, Loss2: 0.001188]\n",
      "Evaluation Set: [Epoch  6, Loss: 1.698192, Acc: 0.7975, AUC: 0.7638]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 193.69batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 194.02batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14539/14539 [02:39<00:00, 90.94batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14388/14388 [02:36<00:00, 91.86batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 88.06batches/s]\n",
      "Training Set: [Epoch  7, Loss1: 0.874703, Loss2: 0.900120]\n",
      "DivideMix Training: [Epoch  7, Loss1: 0.001196, Loss2: 0.001166]\n",
      "Evaluation Set: [Epoch  7, Loss: 1.851774, Acc: 0.7970, AUC: 0.7624]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.59batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.47batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14469/14469 [02:37<00:00, 91.87batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14454/14454 [02:36<00:00, 92.18batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 87.96batches/s]\n",
      "Training Set: [Epoch  8, Loss1: 0.932489, Loss2: 0.986957]\n",
      "DivideMix Training: [Epoch  8, Loss1: 0.001171, Loss2: 0.001176]\n",
      "Evaluation Set: [Epoch  8, Loss: 1.858657, Acc: 0.7957, AUC: 0.7666]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.56batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.10batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14480/14480 [02:38<00:00, 91.28batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14413/14413 [02:37<00:00, 91.60batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 88.26batches/s]\n",
      "Training Set: [Epoch  9, Loss1: 0.978437, Loss2: 0.948430]\n",
      "DivideMix Training: [Epoch  9, Loss1: 0.001176, Loss2: 0.001164]\n",
      "Evaluation Set: [Epoch  9, Loss: 1.871194, Acc: 0.7938, AUC: 0.7611]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.38batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 198.96batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14391/14391 [02:36<00:00, 92.14batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14452/14452 [02:38<00:00, 91.19batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 88.17batches/s]\n",
      "Training Set: [Epoch 10, Loss1: 0.938186, Loss2: 0.999730]\n",
      "DivideMix Training: [Epoch 10, Loss1: 0.001159, Loss2: 0.001172]\n",
      "Evaluation Set: [Epoch 10, Loss: 2.061969, Acc: 0.7935, AUC: 0.7584]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.51batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.30batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14436/14436 [02:38<00:00, 90.87batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14443/14443 [02:38<00:00, 91.35batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 88.01batches/s]\n",
      "Training Set: [Epoch 11, Loss1: 1.107720, Loss2: 1.014150]\n",
      "DivideMix Training: [Epoch 11, Loss1: 0.001171, Loss2: 0.001165]\n",
      "Evaluation Set: [Epoch 11, Loss: 1.795986, Acc: 0.7908, AUC: 0.7558]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.36batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.20batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14386/14386 [02:37<00:00, 91.48batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14374/14374 [02:36<00:00, 91.68batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 87.95batches/s]\n",
      "Training Set: [Epoch 12, Loss1: 0.936250, Loss2: 0.926486]\n",
      "DivideMix Training: [Epoch 12, Loss1: 0.001161, Loss2: 0.001162]\n",
      "Evaluation Set: [Epoch 12, Loss: 1.991312, Acc: 0.7904, AUC: 0.7587]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.10batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.61batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14376/14376 [02:35<00:00, 92.33batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14387/14387 [02:36<00:00, 91.65batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 87.58batches/s]\n",
      "Training Set: [Epoch 13, Loss1: 1.044268, Loss2: 1.002870]\n",
      "DivideMix Training: [Epoch 13, Loss1: 0.001165, Loss2: 0.001152]\n",
      "Evaluation Set: [Epoch 13, Loss: 2.032674, Acc: 0.7878, AUC: 0.7658]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.54batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 199.89batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14379/14379 [02:37<00:00, 91.58batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14343/14343 [02:35<00:00, 91.98batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 87.49batches/s]\n",
      "Training Set: [Epoch 14, Loss1: 0.995566, Loss2: 1.091547]\n",
      "DivideMix Training: [Epoch 14, Loss1: 0.001155, Loss2: 0.001153]\n",
      "Evaluation Set: [Epoch 14, Loss: 2.155942, Acc: 0.7862, AUC: 0.7641]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.25batches/s]\n",
      "GMM processing: 100%|██████████████████████████████████████████| 1122/1122 [00:05<00:00, 200.10batches/s]\n",
      "train model1: 100%|███████████████████████████████████████████| 14338/14338 [02:35<00:00, 92.10batches/s]\n",
      "train model2: 100%|███████████████████████████████████████████| 14395/14395 [02:40<00:00, 89.66batches/s]\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 84.93batches/s]\n",
      "Training Set: [Epoch 15, Loss1: 1.057156, Loss2: 1.144123]\n",
      "DivideMix Training: [Epoch 15, Loss1: 0.001161, Loss2: 0.001155]\n",
      "Evaluation Set: [Epoch 15, Loss: 1.991077, Acc: 0.7855, AUC: 0.7561]\n",
      "Finished Training.\n",
      "Training took 79.92440860668817 minute.\n"
     ]
    }
   ],
   "source": [
    "warmup_loader = loader.load(terminus=\"n\", mode=\"warmup\")\n",
    "train_gmm_loader = loader.load(terminus=\"n\", mode=\"divide_by_GMM\")\n",
    "eval_loader = loader.load(terminus=\"n\", mode=\"evaluate\")\n",
    "\n",
    "start = time()\n",
    "highest_val_auc = 0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if epoch < NUM_WARM_UP_EPOCHS + 1:\n",
    "        # run warm up model 1 and 2 while adding penalty for confident predictions\n",
    "        warmup_loss1, warmup_acc1, warmup_auc1 = warmup(\n",
    "            model=model1,\n",
    "            optimizer=optimizer1,\n",
    "            loss_func=CEloss,\n",
    "            conf_penalty=conf_penalty,\n",
    "            dataloader=warmup_loader\n",
    "        )\n",
    "        warmup_loss2, warmup_acc2, warmup_auc2 = warmup(\n",
    "            model=model2,\n",
    "            optimizer=optimizer2,\n",
    "            loss_func=CEloss,\n",
    "            conf_penalty=conf_penalty,\n",
    "            dataloader=warmup_loader\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Warm-Up Model1: [Epoch {epoch:2d}, Loss: {warmup_loss1:8.6f}, Acc: {warmup_acc1:.4f}, AUC: {warmup_auc1:.4f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Warm-Up Model2: [Epoch {epoch:2d}, Loss: {warmup_loss2:8.6f}, Acc: {warmup_acc2:.4f}, AUC: {warmup_auc2:.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # evaluate on dev set\n",
    "        val_acc, val_auc, val_loss = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loss_func=CE,\n",
    "            dataloader=eval_loader\n",
    "        )\n",
    "        print(\n",
    "            f\"Evaluation Set: [Epoch {epoch:2d}, Loss: {val_loss:.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        prob1, train_loss1, raw_losses1, norm_losses1 = process_gmm(model1, train_gmm_loader, CE)\n",
    "        prob2, train_loss2, raw_losses2, norm_losses2 = process_gmm(model2, train_gmm_loader, CE)\n",
    "\n",
    "        pred1 = prob1 > P_THRESHOLD\n",
    "        pred2 = prob2 > P_THRESHOLD\n",
    "        \n",
    "        # train both models\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.load(\n",
    "            terminus=\"n\", mode=\"train\", pred=pred2, prob=prob2\n",
    "        )\n",
    "        \n",
    "        divmix_loss1 = train(\n",
    "            epoch=epoch,\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            optimizer=optimizer1,\n",
    "            loss_func=criterion,\n",
    "            num_warm_up_epochs=NUM_WARM_UP_EPOCHS,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            lambda_u=LAMBDA_U,\n",
    "            temp=TEMPERATURE,\n",
    "            beta_dist=beta_dist,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            labeled_loader=labeled_trainloader,\n",
    "            unlabeled_loader=unlabeled_trainloader,\n",
    "            rampup_len=RAMPUP_LEN,\n",
    "            named_model=\"model1\"\n",
    "        )\n",
    "\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.load(\n",
    "            terminus=\"n\", mode=\"train\", pred=pred1, prob=prob1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        divmix_loss2 = train(\n",
    "            epoch=epoch,\n",
    "            model1=model2,\n",
    "            model2=model1,\n",
    "            optimizer=optimizer2,\n",
    "            loss_func=criterion,\n",
    "            num_warm_up_epochs=NUM_WARM_UP_EPOCHS,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            lambda_u=LAMBDA_U,\n",
    "            temp=TEMPERATURE,\n",
    "            beta_dist=beta_dist,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            labeled_loader=labeled_trainloader,\n",
    "            unlabeled_loader=unlabeled_trainloader,\n",
    "            rampup_len=RAMPUP_LEN,\n",
    "            named_model=\"model2\"\n",
    "        )\n",
    "\n",
    "        # evaluate on dev set\n",
    "        val_acc, val_auc, val_loss = evaluate(\n",
    "            model1=model1,\n",
    "            model2=model2,\n",
    "            loss_func=CE,\n",
    "            dataloader=eval_loader\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Training Set: [Epoch {epoch:2d}, Loss1: {train_loss1:.6f}, Loss2: {train_loss2:.6f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"DivideMix Training: [Epoch {epoch:2d}, Loss1: {divmix_loss1:.6f}, Loss2: {divmix_loss2:.6f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Evaluation Set: [Epoch {epoch:2d}, Loss: {val_loss:.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\"\n",
    "        )\n",
    "        \n",
    "        if val_auc > highest_val_auc:\n",
    "            highest_val_auc = val_auc\n",
    "            path1 = f\"../../params/n_term/BiLSTM_dividemix/auc{val_auc:.4f}_epoch{epoch}_model1.pt\"\n",
    "            path2 = f\"../../params/n_term/BiLSTM_dividemix/model2_epoch{epoch}.pt\"\n",
    "            torch.save(model1.state_dict(), path1)\n",
    "            torch.save(model2.state_dict(), path2)\n",
    "\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241f4a5e-aba3-4762-a0d3-745238ae3560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model1:  auc0.7740_epoch3_model1.pt\n",
      "Loaded model2:  model2_epoch3.pt\n",
      "evaluate: 100%|███████████████████████████████████████████████████| 141/141 [00:01<00:00, 86.16batches/s]\n",
      "Test Set Performance: Loss: 1.305726, Acc: 0.8098, AUC: 0.7764\n",
      "Total model params: 5096315, trainable model params: 5096315\n"
     ]
    }
   ],
   "source": [
    "# load best model, evaluate on test set\n",
    "test_loader = loader.load(terminus=\"n\", mode=\"test\")\n",
    "\n",
    "best_model1 = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/BiLSTM_dividemix/\") if f.endswith(\"model1.pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model1: \", best_model1)\n",
    "\n",
    "best_model2 = 'model2_' + best_model1.split('_')[1] + '.pt'\n",
    "print(\"Loaded model2: \", best_model2)\n",
    "\n",
    "model1.load_state_dict(torch.load('../../params/n_term/BiLSTM_dividemix/' + best_model1))\n",
    "model2.load_state_dict(torch.load('../../params/n_term/BiLSTM_dividemix/' + best_model2))\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# evaluate on dev set\n",
    "test_acc, test_auc, test_loss = evaluate(\n",
    "    model1=model1,\n",
    "    model2=model2,\n",
    "    loss_func=CE,\n",
    "    dataloader=test_loader\n",
    ")\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model1)}, trainable model params: {trainable_model_params(model1)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e055-1031-4aae-a5fb-95a1325017d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
