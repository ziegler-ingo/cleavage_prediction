{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc007dc-cd55-4de7-a113-577f096a9409",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* Model architecture based on [Liu and Gong, 2019](https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-019-3199-1.pdf), [Github](https://github.com/Jiale-Liu/LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 0 at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m)\n",
    "\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "\n",
    "def save_metrics(*args, path):\n",
    "    if not os.path.isfile(path):\n",
    "        with open(path, \"w\", newline=\"\\n\") as f:\n",
    "            f.write(\n",
    "                \",\".join(\n",
    "                    [\n",
    "                        \"fold\",\n",
    "                        \"epoch\",\n",
    "                        \"train_loss\",\n",
    "                        \"train_acc\",\n",
    "                        \"train_auc\",\n",
    "                        \"val_loss\",\n",
    "                        \"val_acc\",\n",
    "                        \"val_auc\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "    if args:\n",
    "        with open(path, \"a\", newline=\"\\n\") as f:\n",
    "            f.write(\",\".join([str(arg) for arg in args]))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))    \n",
    "            \n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.seq = apply_random_masking(seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "\n",
    "\n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.float)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.seq = self.seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size, hidden_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2*rnn_size,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=2*rnn_size,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm4 = nn.LSTM(\n",
    "            input_size=2*rnn_size,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm5 = nn.LSTM(\n",
    "            input_size=2*rnn_size,\n",
    "            hidden_size=rnn_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=rnn_size * 2,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.dropout(self.embedding(seq))\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        out1, (hn1, cn1) = self.lstm1(embedded)\n",
    "        out2, (hn2, cn2) = self.lstm2(out1, (hn1, cn1))\n",
    "        out3, (hn3, cn3) = self.lstm3(out2, (hn2, cn2))\n",
    "        out3, hn3, cn3 = [torch.add(i, j) for i, j in zip([out1, hn1, cn1], [out3, hn3, cn3])]\n",
    "        \n",
    "        out4, (hn4, cn4) = self.lstm4(out3, (hn3, cn3))\n",
    "        out5, (hn5, cn5) = self.lstm5(out4, (hn4, cn4))\n",
    "        out5 = torch.add(out3, out5)\n",
    "        \n",
    "        out, _ = self.attention(out5, out1, out5)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, 2*rnn_size4)\n",
    "        pooled, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        out = self.dropout(gelu(self.fc1(pooled)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # shape: (batch_size)\n",
    "        return self.fc2(out).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    preds, lbls = [], []\n",
    "    \n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\"\n",
    "    ):\n",
    "        seq, lbl = batch.seq, batch.lbl\n",
    "        seq, lbl = seq.to(device), lbl.to(device)\n",
    "        \n",
    "        scores = model(seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "        \n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += ((scores > 0) == lbl).sum().item()\n",
    "        total += seq.shape[0]\n",
    "        preds.extend(scores.detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# load train+dev set, mix it back as one\n",
    "train_path = '../../data/n_train.csv'\n",
    "dev_path = '../../data/n_val.csv'\n",
    "test_path = '../../data/n_test.csv'\n",
    "\n",
    "# combine previously split train and dev set\n",
    "train_seqs, train_lbls = read_data(train_path)\n",
    "dev_seqs, dev_lbls = read_data(dev_path)\n",
    "total_seqs, total_lbls = np.array(train_seqs + dev_seqs), np.array(train_lbls + dev_lbls)\n",
    "\n",
    "assert len(train_seqs) + len(dev_seqs) == len(total_seqs)\n",
    "assert len(train_lbls) + len(dev_lbls) == len(total_lbls)\n",
    "\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# create vocab from train seqs\n",
    "vocab = build_vocab_from_iterator(train_seqs, specials=['<UNK>'])\n",
    "# vocab = build_vocab_from_iterator(train_seqs)\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "encode_text = lambda x: vocab(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 216\n",
    "RNN_SIZE = 111\n",
    "HIDDEN_SIZE = 150\n",
    "NUM_HEADS = 1\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "params = {\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"rnn_size\": RNN_SIZE,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"num_heads\": NUM_HEADS,\n",
    "    \"dropout\": DROPOUT\n",
    "}\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "logging_path = \"../../params/n_term/BiLSTMAttention/results.csv\"\n",
    "\n",
    "start = time()\n",
    "print(\"Starting Cross-Validation.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "# get a new split\n",
    "for fold, (train_idx, dev_idx) in enumerate(kf.split(total_seqs), 1):\n",
    "    X_tr = total_seqs[train_idx]\n",
    "    y_tr = total_lbls[train_idx]\n",
    "    X_dev = total_seqs[dev_idx]\n",
    "    y_dev = total_lbls[dev_idx]\n",
    "\n",
    "    # create datasets and loads with current split\n",
    "    train_data = CleavageDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_wrapper,\n",
    "        pin_memory=True,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "    dev_data = CleavageDataset(X_dev, y_dev)\n",
    "    dev_loader = DataLoader(\n",
    "        dev_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=eval_wrapper,\n",
    "        pin_memory=True,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "    # reset model weights with each new fold\n",
    "    model = BiLSTMAttention(**params).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # normal training loop\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        train_loss, train_acc, train_auc = process(\n",
    "            model, train_loader, criterion, optimizer\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "\n",
    "        # save metrics\n",
    "        save_metrics(\n",
    "            fold,\n",
    "            epoch,\n",
    "            train_loss,\n",
    "            train_acc,\n",
    "            train_auc,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "            val_auc,\n",
    "            path=logging_path,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Training:   [Fold {fold:2d}, Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Evaluation: [Fold {fold:2d}, Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        reg_auc = regularized_auc(train_auc, val_auc, threshold=0)\n",
    "        if reg_auc > highest_val_auc:\n",
    "            highest_val_auc = reg_auc\n",
    "            path = f\"../../params/n_term/BiLSTMAttention/auc{reg_auc:.4f}_fold{fold}_epoch{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "print(\"Finished Cross-Validation.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Cross-Validation took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.7923_fold6_epoch19.pt\n",
      "Eval: 100%|██████████████████████████████████████████████████████| 281/281 [00:00<00:00, 311.09batches/s]\n",
      "Test Set Performance: Loss: 0.000755, Acc: 0.8336, AUC: 0.7924\n"
     ]
    }
   ],
   "source": [
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/BiLSTMAttention/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model.load_state_dict(torch.load('../../params/n_term/BiLSTMAttention/' + best_model))\n",
    "model.eval()\n",
    "test_loss, test_acc, test_auc = process(model, test_loader, criterion)\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b9f839-ffcd-4d84-b060-80bb75be99e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.7923_fold6_epoch19.pt\n",
      "Total model params: 1718233, trainable model params: 1718233\n"
     ]
    }
   ],
   "source": [
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/n_term/BiLSTMAttention/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model = BiLSTMAttention(**params)\n",
    "model.load_state_dict(torch.load('../../params/n_term/BiLSTMAttention/' + best_model))\n",
    "model.eval()\n",
    "\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa26b80-599d-410d-a176-96eab8747507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
