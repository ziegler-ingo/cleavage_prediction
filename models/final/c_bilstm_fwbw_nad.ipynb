{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3bd843-d2fe-4ab3-8cab-9e75d8703904",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* Sequence Encoder model architecture based on [Heigold et al., 2016](https://arxiv.org/abs/1606.06640)\n",
    "* BiLSTM model architecture based on [Ozols et. al., 2021](https://www.mdpi.com/1422-0067/22/6/3071/htm)\n",
    "* Noise adaptation layer implementation is based on [Goldberger and Ben-Reuven, 2017](https://openreview.net/references/pdf?id=Sk5qglwSl), and unofficial implementation on [Github](https://github.com/Billy1900/Noise-Adaption-Layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded39806-fc83-4b0c-b9ce-14e896a4675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbbe232-2c39-49c6-9187-a80c6812d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449b96d-6e11-4b72-a5e6-8f20c1b71115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, \"r\") as csvfile:\n",
    "        train_data = list(csv.reader(csvfile))[1:]  # skip col name\n",
    "        sents, lbls = [], []\n",
    "        for s, l in train_data:\n",
    "            sents.append(s)\n",
    "            lbls.append(l)\n",
    "    return sents, lbls\n",
    "\n",
    "\n",
    "def apply_random_masking(seq, num_tokens):\n",
    "    \"\"\"\n",
    "    Mask `num_tokens` as 0 at random positions per sequence.\n",
    "    \"\"\"\n",
    "    dist = torch.rand(seq.shape)\n",
    "    m, _ = torch.topk(dist, num_tokens)\n",
    "    return seq * (dist < m)\n",
    "\n",
    "\n",
    "def regularized_auc(train_auc, dev_auc, threshold=0.0025):\n",
    "    \"\"\"\n",
    "    Returns development AUC if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_auc if (train_auc - dev_auc) < threshold else 0\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Facebook Research implementation of the gelu activation function.\n",
    "    \n",
    "    For information: OpenAI GPT's gelu is slightly different\n",
    "    (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def trainable_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def total_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ffe646-fe1e-4c17-b3e5-c36ed03e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleavageDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx], self.lbl[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lbl)\n",
    "    \n",
    "class TrainBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        bw_seq = torch.tensor([encode_text(seq)[::-1] for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        fw_seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.bw_seq = apply_random_masking(bw_seq, num_tokens=1)\n",
    "        self.fw_seq = apply_random_masking(fw_seq, num_tokens=1)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.long)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.bw_seq = self.bw_seq.pin_memory()\n",
    "        self.fw_seq = self.fw_seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def train_wrapper(batch):\n",
    "    return TrainBatch(batch)\n",
    "    \n",
    "    \n",
    "class EvalBatch:\n",
    "    def __init__(self, batch):\n",
    "        ordered_batch = list(zip(*batch))\n",
    "        self.bw_seq = torch.tensor([encode_text(seq)[::-1] for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.fw_seq = torch.tensor([encode_text(seq) for seq in ordered_batch[0]], dtype=torch.int64)\n",
    "        self.lbl = torch.tensor([int(l) for l in ordered_batch[1]], dtype=torch.long)\n",
    "        \n",
    "    def pin_memory(self):\n",
    "        self.bw_seq = self.bw_seq.pin_memory()\n",
    "        self.fw_seq = self.fw_seq.pin_memory()\n",
    "        self.lbl = self.lbl.pin_memory()\n",
    "        return self\n",
    "    \n",
    "def eval_wrapper(batch):\n",
    "    return EvalBatch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cc48e7-1186-4391-b9d8-924eee98b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fw_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        self.bw_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fw_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, hidden_size=rnn_size, batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.bw_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, hidden_size=rnn_size, batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, bw_seq, fw_seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        fw_embeddings = self.dropout(self.fw_embedding(fw_seq))\n",
    "        bw_embeddings = self.dropout(self.bw_embedding(bw_seq))\n",
    "    \n",
    "        # input shape: (batch_size, seq_len, embedding_dim)\n",
    "        fw_out, _ = self.fw_lstm(fw_embeddings)\n",
    "        bw_out, _ = self.bw_lstm(bw_embeddings)\n",
    "        \n",
    "        # input shape: (batch_size, seq_len, rnn_size)\n",
    "        # only get representation at last t\n",
    "        fw_out = self.dropout(fw_out[:, -1, :])\n",
    "        bw_out = self.dropout(bw_out[:, -1, :])\n",
    "        \n",
    "        # input shape: (batch_size, rnn_size)\n",
    "        # out shape: (batch_size, 2*rnn_size)\n",
    "        return torch.cat([fw_out, bw_out], dim=1) \n",
    "        \n",
    "    \n",
    "\n",
    "class FwBwBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_enc_emb_dim, seq_enc_rnn_size, rnn_size1, rnn_size2, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # sequence encoder replaces embedding representations\n",
    "        self.seq_encoder = SeqEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=seq_enc_emb_dim,\n",
    "            rnn_size=seq_enc_rnn_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=seq_enc_rnn_size * 2,\n",
    "            hidden_size=rnn_size1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=2*rnn_size1,\n",
    "            hidden_size=rnn_size2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(rnn_size2 * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    def forward(self, bw_seq, fw_seq):\n",
    "        # input shape: (batch_size, seq_len=10)\n",
    "        embedded = self.dropout(self.seq_encoder(bw_seq, fw_seq))\n",
    "        \n",
    "        # input shape: (batch_size, seq_enc_rnn_size * 2)\n",
    "        out, _ = self.lstm1(embedded)\n",
    "        \n",
    "        # input shape: (batch_size, 2*rnn_size)\n",
    "        out, _ = self.lstm2(out)\n",
    "        \n",
    "        # input shape; (batch_size, 2*rnn_size)\n",
    "        out = self.dropout(gelu(self.fc1(out)))\n",
    "        \n",
    "        # input shape: (batch_size, hidden_size)\n",
    "        # output shape: (batch_size, 2)\n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72cabe9-f647-47ba-96ed-37c52355b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseAdaptation(nn.Module):\n",
    "    def __init__(self, theta, k):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Linear(k, k, bias=False)\n",
    "        self.theta.weight.data = theta\n",
    "        self.eye = torch.eye(k).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        theta = self.theta(self.eye)\n",
    "        theta = torch.softmax(theta, dim=0)\n",
    "        out = x @ theta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e91040-ea43-4a0f-b17a-bcbb2b38ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loader, criterion, optim=None, conf=None):\n",
    "    epoch_loss, num_correct, total = 0, 0, 0\n",
    "    pos_preds, preds, lbls = [], [], []\n",
    "\n",
    "    for batch in tqdm(\n",
    "        loader,\n",
    "        desc=\"Train: \" if optim is not None else \"Eval: \",\n",
    "        file=sys.stdout,\n",
    "        unit=\"batches\",\n",
    "    ):\n",
    "        bw_seq, fw_seq, lbl = batch.bw_seq, batch.fw_seq, batch.lbl\n",
    "        bw_seq, fw_seq, lbl = bw_seq.to(device), fw_seq.to(device), lbl.to(device)\n",
    "\n",
    "        scores = model(bw_seq, fw_seq)\n",
    "        loss = criterion(scores, lbl)\n",
    "\n",
    "        if optim is not None:\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        pred = scores.argmax(dim=1)\n",
    "        epoch_loss += loss.item()\n",
    "        num_correct += (pred == lbl).sum().item()\n",
    "        total += bw_seq.shape[0]\n",
    "        preds.extend(pred.detach().tolist())\n",
    "        pos_preds.extend(scores[:, 1].detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    if conf is not None:   \n",
    "        return confusion_matrix(lbls, preds)\n",
    "    return epoch_loss / total, num_correct / total, roc_auc_score(lbls, pos_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5534faf-85ed-4db7-9c09-944e36f6e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid(\n",
    "    model, noisemodel, optimizer, noise_optimizer, criterion, beta, loader\n",
    "):\n",
    "    epoch_loss, model_loss, noise_loss, num_correct, total = 0, 0, 0, 0, 0\n",
    "    preds, lbls = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Hybrid Train: \", file=sys.stdout, unit=\"batches\"):\n",
    "\n",
    "        bw_seq, fw_seq, lbl = batch.bw_seq, batch.fw_seq, batch.lbl\n",
    "        bw_seq, fw_seq, lbl = bw_seq.to(device), fw_seq.to(device), lbl.to(device)\n",
    "\n",
    "        scores = model(bw_seq, fw_seq)\n",
    "        noise_scores = noisemodel(scores)\n",
    "\n",
    "        model_loss = criterion(scores, lbl)\n",
    "        noise_loss = criterion(noise_scores, lbl)\n",
    "\n",
    "        loss = beta * noise_loss + (1 - beta) * model_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        noise_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        noise_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        model_loss += model_loss.item()\n",
    "        noise_loss += noise_loss.item()\n",
    "        num_correct += (noise_scores.argmax(dim=1)  == lbl).sum().item()\n",
    "        total += bw_seq.shape[0]\n",
    "        preds.extend(noise_scores[:, 1].detach().tolist())\n",
    "        lbls.extend(lbl.detach().tolist())\n",
    "        \n",
    "    return (\n",
    "        epoch_loss / total,\n",
    "        model_loss / total,\n",
    "        noise_loss / total,\n",
    "        num_correct / total,\n",
    "        roc_auc_score(lbls, preds),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42148192-6ec1-4ffa-8f86-2662575167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load train and dev data\n",
    "train_seqs, train_lbl = read_data('../../data/c_train.csv')\n",
    "dev_seqs, dev_lbl = read_data('../../data/c_val.csv')\n",
    "\n",
    "# create vocab from train seqs\n",
    "vocab = build_vocab_from_iterator(train_seqs, specials=['<UNK>'])\n",
    "vocab.set_default_index(vocab['<UNK>'])\n",
    "encode_text = lambda x: vocab(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff5c30b-2822-4871-8ab5-0ec37673f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "NUM_WARMUP = 1\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 512\n",
    "BETA=0.8\n",
    "VOCAB_SIZE = len(vocab)\n",
    "SEQ_ENC_EMB_DIM = 100\n",
    "SEQ_ENC_RNN_SIZE = 200\n",
    "RNN_SIZE1 = 128\n",
    "RNN_SIZE2 = 512\n",
    "HIDDEN_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = FwBwBiLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    seq_enc_emb_dim=SEQ_ENC_EMB_DIM,\n",
    "    seq_enc_rnn_size=SEQ_ENC_RNN_SIZE,\n",
    "    rnn_size1=RNN_SIZE1,\n",
    "    rnn_size2=RNN_SIZE2,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create train and dev loader\n",
    "train_data = CleavageDataset(train_seqs, train_lbl)\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=train_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "dev_data = CleavageDataset(dev_seqs, dev_lbl)\n",
    "dev_loader = DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle=True, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbe5697-c1a7-4f53-9826-9e91e80d0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training.\n",
      "Train: 100%|████████████████████████████████████████████████████| 2218/2218 [02:03<00:00, 18.00batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 59.42batches/s]\n",
      "Warmup Training:   [Epoch  1, Loss: 0.000884, Acc: 0.8227, AUC: 0.6100]\n",
      "Warmup Evaluation: [Epoch  1, Loss: 0.000775, Acc: 0.8329, AUC: 0.7740]\n",
      "Eval: 100%|█████████████████████████████████████████████████████| 2218/2218 [00:35<00:00, 62.05batches/s]\n",
      "Created NoiseModel in epoch 1\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:01<00:00, 18.33batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 62.68batches/s]\n",
      "Hy-Training: [Epoch  2, Hy-Loss: 0.000790,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8388, AUC: 0.2220]\n",
      "Evaluation:  [Epoch  2, Loss: 0.000821, Acc: 0.8520, AUC: 0.8591]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.06batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 59.42batches/s]\n",
      "Hy-Training: [Epoch  3, Hy-Loss: 0.000721,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8468, AUC: 0.2690]\n",
      "Evaluation:  [Epoch  3, Loss: 0.000771, Acc: 0.8518, AUC: 0.8619]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:01<00:00, 18.25batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 61.31batches/s]\n",
      "Hy-Training: [Epoch  4, Hy-Loss: 0.000694,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8497, AUC: 0.5548]\n",
      "Evaluation:  [Epoch  4, Loss: 0.000758, Acc: 0.8499, AUC: 0.8652]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:01<00:00, 18.26batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 58.67batches/s]\n",
      "Hy-Training: [Epoch  5, Hy-Loss: 0.000678,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8518, AUC: 0.7513]\n",
      "Evaluation:  [Epoch  5, Loss: 0.000726, Acc: 0.8515, AUC: 0.8668]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:01<00:00, 18.30batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 62.48batches/s]\n",
      "Hy-Training: [Epoch  6, Hy-Loss: 0.000668,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8530, AUC: 0.8158]\n",
      "Evaluation:  [Epoch  6, Loss: 0.000680, Acc: 0.8542, AUC: 0.8683]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.14batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 58.91batches/s]\n",
      "Hy-Training: [Epoch  7, Hy-Loss: 0.000661,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8542, AUC: 0.8368]\n",
      "Evaluation:  [Epoch  7, Loss: 0.000691, Acc: 0.8507, AUC: 0.8683]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.10batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 62.44batches/s]\n",
      "Hy-Training: [Epoch  8, Hy-Loss: 0.000656,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8551, AUC: 0.8452]\n",
      "Evaluation:  [Epoch  8, Loss: 0.000660, Acc: 0.8543, AUC: 0.8697]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:01<00:00, 18.23batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 56.83batches/s]\n",
      "Hy-Training: [Epoch  9, Hy-Loss: 0.000652,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8556, AUC: 0.8494]\n",
      "Evaluation:  [Epoch  9, Loss: 0.000660, Acc: 0.8547, AUC: 0.8702]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.06batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 60.36batches/s]\n",
      "Hy-Training: [Epoch 10, Hy-Loss: 0.000648,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8563, AUC: 0.8517]\n",
      "Evaluation:  [Epoch 10, Loss: 0.000656, Acc: 0.8543, AUC: 0.8710]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.07batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 58.86batches/s]\n",
      "Hy-Training: [Epoch 11, Hy-Loss: 0.000645,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8569, AUC: 0.8545]\n",
      "Evaluation:  [Epoch 11, Loss: 0.000643, Acc: 0.8570, AUC: 0.8727]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.17batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 62.16batches/s]\n",
      "Hy-Training: [Epoch 12, Hy-Loss: 0.000643,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8574, AUC: 0.8564]\n",
      "Evaluation:  [Epoch 12, Loss: 0.000635, Acc: 0.8601, AUC: 0.8734]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:02<00:00, 18.03batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 57.23batches/s]\n",
      "Hy-Training: [Epoch 13, Hy-Loss: 0.000640,            Model-Loss: 0.000000, Noise-Loss: 0.000000,            Acc: 0.8578, AUC: 0.8579]\n",
      "Evaluation:  [Epoch 13, Loss: 0.000630, Acc: 0.8608, AUC: 0.8740]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:07<00:00, 17.39batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 60.47batches/s]\n",
      "Hy-Training: [Epoch 14, Hy-Loss: 0.000638,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8584, AUC: 0.8589]\n",
      "Evaluation:  [Epoch 14, Loss: 0.000637, Acc: 0.8583, AUC: 0.8743]\n",
      "Hybrid Train: 100%|█████████████████████████████████████████████| 2218/2218 [02:06<00:00, 17.54batches/s]\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 56.63batches/s]\n",
      "Hy-Training: [Epoch 15, Hy-Loss: 0.000636,            Model-Loss: 0.000001, Noise-Loss: 0.000001,            Acc: 0.8587, AUC: 0.8603]\n",
      "Evaluation:  [Epoch 15, Loss: 0.000632, Acc: 0.8600, AUC: 0.8754]\n",
      "Finished Training.\n",
      "Training took 32.60035171508789 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(\"Starting Training.\")\n",
    "highest_val_auc = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if epoch < NUM_WARMUP + 1:\n",
    "        model.train()\n",
    "        train_loss, train_acc, train_auc = process(model, train_loader, criterion, optimizer)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "            \n",
    "        print(\n",
    "        f\"Warmup Training:   [Epoch {epoch:2d}, Loss: {train_loss:8.6f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Warmup Evaluation: [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        if epoch == NUM_WARMUP:\n",
    "            # get conf matrix based on predictions on train data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                conf = process(model, train_loader, criterion, conf=True)\n",
    "            theta = conf / conf.sum(axis=1, keepdims=True)\n",
    "            theta = torch.from_numpy(np.log(theta + 1e-8)).to(torch.float) # avoid zeros with +1e-8\n",
    "\n",
    "            # create noisemodel\n",
    "            noisemodel = NoiseAdaptation(theta=theta, k=NUM_CLASSES).to(device)\n",
    "            noise_optimizer = optim.Adam(noisemodel.parameters(), lr=LEARNING_RATE)\n",
    "            print(f'Created NoiseModel in epoch {epoch}')\n",
    "\n",
    "    else:\n",
    "        # hybrid training\n",
    "        model.train()\n",
    "        noisemodel.train()\n",
    "        hy_tr_loss, model_loss, noise_loss, hy_tr_acc, hy_tr_auc = train_hybrid(\n",
    "            model=model,\n",
    "            noisemodel=noisemodel,\n",
    "            optimizer=optimizer,\n",
    "            noise_optimizer=noise_optimizer,\n",
    "            criterion=criterion,\n",
    "            beta=BETA,\n",
    "            loader=train_loader\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc, val_auc = process(model, dev_loader, criterion)\n",
    "\n",
    "        print(\n",
    "            f\"Hy-Training: [Epoch {epoch:2d}, Hy-Loss: {hy_tr_loss:.6f},\\\n",
    "            Model-Loss: {model_loss:.6f}, Noise-Loss: {noise_loss:.6f},\\\n",
    "            Acc: {hy_tr_acc:.4f}, AUC: {hy_tr_auc:.4f}]\"\n",
    "        )\n",
    "        print(f\"Evaluation:  [Epoch {epoch:2d}, Loss: {val_loss:8.6f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}]\")\n",
    "        \n",
    "        reg_auc = regularized_auc(hy_tr_auc, val_auc, threshold=0)\n",
    "        if reg_auc > highest_val_auc:\n",
    "            highest_val_auc = reg_auc\n",
    "            path = f\"../../params/c_term/FwBwBiLSTM_noise_layer/auc{reg_auc:.4f}_epoch{epoch}.pt\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "train_time = (time() - start) / 60\n",
    "print(f\"Training took {train_time} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126fd50d-b4b5-4da7-a8e5-8ebeed450b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model:  auc0.8754_epoch15.pt\n",
      "Eval: 100%|███████████████████████████████████████████████████████| 278/278 [00:04<00:00, 57.49batches/s]\n",
      "Test Set Performance: Loss: 0.000626, Acc: 0.8609, AUC: 0.8775\n",
      "Total model params: 4315498, trainable model params: 4315498\n"
     ]
    }
   ],
   "source": [
    "test_path = '../../data/c_test.csv'\n",
    "test_seqs, test_lbls = read_data(test_path)\n",
    "\n",
    "test_data = CleavageDataset(test_seqs, test_lbls)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=eval_wrapper, pin_memory=True, num_workers=10)\n",
    "\n",
    "# load best model, evaluate on test set\n",
    "best_model = sorted(\n",
    "    [f for f in os.listdir(\"../../params/c_term/FwBwBiLSTM_noise_layer/\") if f.endswith(\".pt\")],\n",
    "    reverse=True,\n",
    ")[0]\n",
    "print(\"Loaded model: \", best_model)\n",
    "model.load_state_dict(torch.load('../../params/c_term/FwBwBiLSTM_noise_layer/' + best_model))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc, test_auc = process(model, test_loader, criterion)\n",
    "print(\n",
    "    f\"Test Set Performance: Loss: {test_loss:.6f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Total model params: {total_model_params(model)}, trainable model params: {trainable_model_params(model)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15bfe2-6557-42a6-81c1-a6c2960923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
